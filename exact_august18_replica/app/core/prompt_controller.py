"""
Modular Legal AI Architecture - Production Ready
Separated concerns with proper error handling and fallback strategies
"""
import logging
from typing import Dict, List, Optional, Any
import asyncio
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum
import re
import logging
from .strategic_templates import StrategicLanguageTemplates
# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ==================== CORE ENUMS ====================

    
class DocumentType(Enum):
    """Document types for specialized prompting"""
    EXECUTION_DISPUTE = "execution_dispute"
    FAMILY_MEMO = "family_memo"
    DEFENSE_MEMO = "defense_memo"
    LAWSUIT = "lawsuit"
    CONTRACT = "contract"
    APPEAL = "appeal"
    DEMAND_LETTER = "demand_letter"
    CONSULTATION = "consultation"


class LegalDomain(Enum):
    """Legal domains for context"""
    EXECUTION = "execution"
    FAMILY = "family"
    COMMERCIAL = "commercial"
    CRIMINAL = "criminal"
    ADMINISTRATIVE = "administrative"
    CIVIL = "civil"
    GENERAL = "general"


class UserIntentType(Enum):
    """User intent for strategic response"""
    WIN_CASE = "win_case"
    UNDERSTAND_LAW = "understand_law"
    PROTECT_INTERESTS = "protect_interests"
    TAKE_ACTION = "take_action"


class ComplexityLevel(Enum):
    """Response complexity based on user needs"""
    SIMPLE_EXPLANATION = "simple"
    STRATEGIC_DOCUMENT = "strategic"
    COMPREHENSIVE_ANALYSIS = "comprehensive"


# ==================== DATA CLASSES ====================

@dataclass
class DetectionResult:
    """Result from intent/document detection with confidence score"""
    document_type: DocumentType
    confidence: float
    user_intent: UserIntentType
    complexity_level: ComplexityLevel
    legal_domain: LegalDomain
    fallback_used: bool = False
    warnings: List[str] = None

    def __post_init__(self):
        if self.warnings is None:
            self.warnings = []


@dataclass
class LegalContext:
    """Enhanced context for generating strategic legal responses"""
    document_type: DocumentType
    legal_domain: LegalDomain
    query: str
    user_intent: UserIntentType
    complexity_level: ComplexityLevel
    retrieved_documents: List[Any] = None
    conversation_history: List[Dict[str, str]] = None
    user_position: str = "seeking_advice"
    urgency_level: str = "standard"
    confidence_score: float = 1.0
    warnings: List[str] = None
    case_strength_assessment: str = "unknown"  # weak/moderate/strong
    strategic_recommendation: str = ""  # Action advice
    previous_analysis_summary: str = ""  # Facts from prior responses
    escalation_suggestion: str = ""  # Next legal steps
    evidence_requests: List[str] = None


    def __post_init__(self):
        if self.warnings is None:
            self.warnings = []
        if self.retrieved_documents is None:
            self.retrieved_documents = []
        if self.conversation_history is None:
            self.conversation_history = []
        if self.evidence_requests is None:
            self.evidence_requests = []    


# ==================== MODULAR COMPONENTS ====================

class InputSanitizer:
    """Handles input normalization and Arabic consistency"""
    
    @staticmethod
    def sanitize_query(query: str) -> Tuple[str, List[str]]:
        """Sanitize and normalize user input"""
        warnings = []
        
        if not query or not query.strip():
            raise ValueError("Empty query provided")
        
        # Clean whitespace
        cleaned_query = query.strip()
        
        # Check for mixed languages
        has_arabic = bool(re.search(r'[\u0600-\u06FF]', cleaned_query))
        has_english = bool(re.search(r'[a-zA-Z]', cleaned_query))
        
        if has_english and has_arabic:
            warnings.append("Mixed Arabic/English detected - response will be in Arabic only")
        
        if not has_arabic:
            warnings.append("No Arabic detected - assuming Arabic legal context")
        
        # Remove problematic characters
        cleaned_query = re.sub(r'[^\u0600-\u06FF\u0020-\u007E\s]', '', cleaned_query)
        
        return cleaned_query, warnings

    @staticmethod
    def validate_conversation_history(history: List[Dict[str, str]]) -> List[str]:
        """Validate conversation history format"""
        warnings = []
        
        if not isinstance(history, list):
            raise ValueError("Conversation history must be a list")
        
        for i, msg in enumerate(history):
            if not isinstance(msg, dict):
                warnings.append(f"Message {i} is not a dictionary")
                continue
            
            if 'role' not in msg or 'content' not in msg:
                warnings.append(f"Message {i} missing required fields (role, content)")
            
            if msg.get('role') not in ['user', 'assistant']:
                warnings.append(f"Message {i} has invalid role: {msg.get('role')}")
        
        return warnings


class IntentDetector:
    """Specialized component for detecting user intent and document type"""
    
    def __init__(self):
        self.confidence_threshold = 0.7
        self._load_detection_patterns()
    
    def _load_detection_patterns(self):
        """Load detection patterns with confidence weights"""
        
        # High-confidence patterns (0.9+)
        self.high_confidence_patterns = {
            DocumentType.EXECUTION_DISPUTE: [
                'Ù…Ù†Ø§Ø²Ø¹Ø© ØªÙ†ÙÙŠØ°', 'Ø§Ø¹ØªØ±Ø§Ø¶ Ø¹Ù„Ù‰ ØªÙ†ÙÙŠØ°', 'ÙˆÙ‚Ù ØªÙ†ÙÙŠØ°', 
                'Ù…Ø­ÙƒÙ…Ø© Ø§Ù„ØªÙ†ÙÙŠØ°', 'ØªÙ†ÙÙŠØ° Ø¶Ø¯ÙŠ'
            ],
            DocumentType.DEFENSE_MEMO: [
                'Ù…Ø°ÙƒØ±Ø© Ø¯ÙØ§Ø¹', 'Ø±Ø¯ Ø¹Ù„Ù‰ Ø¯Ø¹ÙˆÙ‰', 'Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù„Ø§Ø¦Ø­Ø©',
                'Ø¯ÙØ§Ø¹ Ø¹Ù†', 'Ø¯Ø¹ÙˆÙ‰ Ø¶Ø¯ÙŠ', 'Ù…Ø±ÙÙˆØ¹ Ø¶Ø¯ÙŠ'
            ],
            DocumentType.LAWSUIT: [
                'Ù„Ø§Ø¦Ø­Ø© Ø¯Ø¹ÙˆÙ‰', 'ØµØ­ÙŠÙØ© Ø¯Ø¹ÙˆÙ‰', 'Ø±ÙØ¹ Ø¯Ø¹ÙˆÙ‰',
                'Ø£Ø±ÙŠØ¯ Ù…Ù‚Ø§Ø¶Ø§Ø©', 'Ø£Ø±ÙŠØ¯ Ø±ÙØ¹ Ø¯Ø¹ÙˆÙ‰'
            ]
        }
        
        # Medium-confidence patterns (0.6-0.8)
        self.medium_confidence_patterns = {
            DocumentType.CONTRACT: ['Ø¹Ù‚Ø¯', 'Ø§ØªÙØ§Ù‚ÙŠØ©', 'ØµÙŠØ§ØºØ© Ø¹Ù‚Ø¯'],
            DocumentType.FAMILY_MEMO: ['Ø£Ø­ÙˆØ§Ù„ Ø´Ø®ØµÙŠØ©', 'Ø·Ù„Ø§Ù‚', 'Ù†ÙÙ‚Ø©'],
            DocumentType.APPEAL: ['Ø§Ø³ØªØ¦Ù†Ø§Ù', 'Ø§Ø¹ØªØ±Ø§Ø¶', 'Ø·Ø¹Ù†'],
            DocumentType.DEMAND_LETTER: ['Ø¥Ù†Ø°Ø§Ø±', 'Ø®Ø·Ø§Ø¨ Ø¥Ù†Ø°Ø§Ø±', 'ØªØ­Ø°ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ']
        }
        
        # Intent patterns
        self.intent_patterns = {
            UserIntentType.WIN_CASE: [
                'Ø£Ø±ÙŠØ¯ Ø§Ù„ÙÙˆØ²', 'ÙƒÙŠÙ Ø£ÙƒØ³Ø¨', 'Ø£Ù‚ÙˆÙ‰ Ø¯ÙØ¹', 'Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©',
                'Ù‡Ø²ÙŠÙ…Ø© Ø§Ù„Ø®ØµÙ…', 'Ø¥Ø¨Ø·Ø§Ù„ Ø§Ù„Ø¯Ø¹ÙˆÙ‰', 'Ø¶Ø¯ÙŠ', 'ÙŠØ·Ø§Ù„Ø¨Ù†ÙŠ'
            ],
            UserIntentType.TAKE_ACTION: [
                'Ø±ÙØ¹ Ø¯Ø¹ÙˆÙ‰', 'Ù…Ù‚Ø§Ø¶Ø§Ø©', 'Ø£Ø±ÙŠØ¯ Ù…Ø·Ø§Ù„Ø¨Ø©', 'Ø§ØªØ®Ø§Ø° Ø¥Ø¬Ø±Ø§Ø¡ Ù‚Ø§Ù†ÙˆÙ†ÙŠ'
            ],
            UserIntentType.PROTECT_INTERESTS: [
                'Ø­Ù…Ø§ÙŠØ© Ø­Ù‚ÙˆÙ‚ÙŠ', 'ÙƒÙŠÙ Ø£Ø­Ù…ÙŠ Ù†ÙØ³ÙŠ', 'Ø§Ø­ØªÙŠØ§Ø·Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©', 'Ø§Ù„ÙˆÙ‚Ø§ÙŠØ©'
            ],
            UserIntentType.UNDERSTAND_LAW: [
                'Ù…Ø§ Ù‡ÙŠ', 'ÙƒÙŠÙ', 'Ø´Ø±Ø­', 'ØªÙˆØ¶ÙŠØ­', 'ÙÙ‡Ù…', 'Ø£Ø±ÙŠØ¯ Ø£Ù† Ø£Ø¹Ø±Ù'
            ]
        }
    
    def detect_document_type(self, query: str) -> Tuple[DocumentType, float]:
        """Detect document type with confidence score"""
        query_lower = query.lower()
        best_match = DocumentType.CONSULTATION
        best_confidence = 0.0
        
        # Check high-confidence patterns first
        for doc_type, patterns in self.high_confidence_patterns.items():
            for pattern in patterns:
                if pattern in query_lower:
                    confidence = 0.9 + (len(pattern) / 100)  # Longer patterns = higher confidence
                    if confidence > best_confidence:
                        best_match = doc_type
                        best_confidence = confidence
        
        # Check medium-confidence patterns if no high-confidence match
        if best_confidence < 0.8:
            for doc_type, patterns in self.medium_confidence_patterns.items():
                for pattern in patterns:
                    if pattern in query_lower:
                        confidence = 0.6 + (len(pattern) / 200)
                        if confidence > best_confidence:
                            best_match = doc_type
                            best_confidence = confidence
        
        return best_match, min(best_confidence, 1.0)
    
    def detect_user_intent(self, query: str, doc_type: DocumentType) -> UserIntentType:
        """Detect user intent based on query and document type"""
        query_lower = query.lower()
        
        # Check explicit intent patterns
        for intent, patterns in self.intent_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                return intent
        
        # Default based on document type
        intent_mapping = {
            DocumentType.DEFENSE_MEMO: UserIntentType.WIN_CASE,
            DocumentType.EXECUTION_DISPUTE: UserIntentType.WIN_CASE,
            DocumentType.APPEAL: UserIntentType.WIN_CASE,
            DocumentType.LAWSUIT: UserIntentType.TAKE_ACTION,
            DocumentType.DEMAND_LETTER: UserIntentType.TAKE_ACTION,
            DocumentType.CONTRACT: UserIntentType.PROTECT_INTERESTS,
            DocumentType.CONSULTATION: UserIntentType.UNDERSTAND_LAW
        }
        
        return intent_mapping.get(doc_type, UserIntentType.UNDERSTAND_LAW)
    
    def detect_complexity_level(self, query: str, intent: UserIntentType) -> ComplexityLevel:
        """Determine appropriate response complexity"""
        query_lower = query.lower()
        
        # Document generation indicators
        if any(word in query_lower for word in ['Ù…Ø°ÙƒØ±Ø©', 'Ù„Ø§Ø¦Ø­Ø©', 'ØµØ­ÙŠÙØ©', 'Ø¹Ù‚Ø¯', 'Ø§ÙƒØªØ¨', 'ØµÙŠØ§ØºØ©']):
            return ComplexityLevel.STRATEGIC_DOCUMENT
        
        # Simple explanation indicators
        if any(word in query_lower for word in ['Ù…Ø§ Ù‡ÙŠ', 'Ø§Ø´Ø±Ø­', 'ÙˆØ¶Ø­', 'Ø£Ø±ÙŠØ¯ ÙÙ‡Ù…']):
            return ComplexityLevel.SIMPLE_EXPLANATION
        
        # Complex analysis indicators
        if any(word in query_lower for word in ['ØªØ­Ù„ÙŠÙ„', 'Ø¯Ø±Ø§Ø³Ø©', 'Ø´Ø§Ù…Ù„', 'Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ÙƒØ§Ù…Ù„Ø©']):
            return ComplexityLevel.COMPREHENSIVE_ANALYSIS
        
        # Default based on intent
        if intent in [UserIntentType.WIN_CASE, UserIntentType.TAKE_ACTION]:
            return ComplexityLevel.STRATEGIC_DOCUMENT
        elif intent == UserIntentType.UNDERSTAND_LAW:
            return ComplexityLevel.SIMPLE_EXPLANATION
        else:
            return ComplexityLevel.COMPREHENSIVE_ANALYSIS

class LegalDomainClassifier:
    """Specialized component for legal domain classification"""
    
    @staticmethod
    def classify_domain(query: str) -> LegalDomain:
        """Classify legal domain from query"""
        query_lower = query.lower()
        
        domain_patterns = {
            LegalDomain.EXECUTION: ['ØªÙ†ÙÙŠØ°', 'Ø­Ø¬Ø²', 'Ù…Ù†Ø§Ø²Ø¹Ø© ØªÙ†ÙÙŠØ°', 'Ù…Ø­ÙƒÙ…Ø© Ø§Ù„ØªÙ†ÙÙŠØ°'],
            LegalDomain.FAMILY: ['Ø£Ø­ÙˆØ§Ù„ Ø´Ø®ØµÙŠØ©', 'Ø·Ù„Ø§Ù‚', 'Ù†ÙÙ‚Ø©', 'Ø­Ø¶Ø§Ù†Ø©', 'Ù…ÙŠØ±Ø§Ø«', 'Ø²ÙˆØ§Ø¬'],
            LegalDomain.COMMERCIAL: ['Ø´Ø±ÙƒØ©', 'ØªØ¬Ø§Ø±ÙŠ', 'Ø§Ø³ØªØ«Ù…Ø§Ø±', 'Ø¹Ù‚Ø¯ ØªØ¬Ø§Ø±ÙŠ', 'Ø´Ø±Ø§ÙƒØ©'],
            LegalDomain.CRIMINAL: ['Ø¬Ø±ÙŠÙ…Ø©', 'Ø¬Ù†Ø§Ø¦ÙŠ', 'Ø¹Ù‚ÙˆØ¨Ø©', 'Ù…Ø®Ø§Ù„ÙØ©', 'Ø­Ø¯', 'Ù‚ØµØ§Øµ'],
            LegalDomain.ADMINISTRATIVE: ['Ø¥Ø¯Ø§Ø±ÙŠ', 'Ø¯ÙŠÙˆØ§Ù† Ø§Ù„Ù…Ø¸Ø§Ù„Ù…', 'Ù‚Ø±Ø§Ø± Ø¥Ø¯Ø§Ø±ÙŠ', 'Ù…ÙˆØ¸Ù Ø­ÙƒÙˆÙ…ÙŠ'],
            LegalDomain.CIVIL: ['Ø¯Ø¹ÙˆÙ‰', 'Ù…Ø±Ø§ÙØ¹Ø©', 'Ù‚Ø¶ÙŠØ© Ù…Ø¯Ù†ÙŠØ©', 'Ø­Ù‚ÙˆÙ‚ Ù…Ø¯Ù†ÙŠØ©']
        }
        
        for domain, patterns in domain_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                return domain
        
        return LegalDomain.GENERAL


class CitationValidator:
    """Enhanced citation extraction and validation"""
    
    def __init__(self):
        self.citation_patterns = [
            r'Ø§Ù„Ù…Ø§Ø¯Ø©\s*\((\d+)\)',           # Ø§Ù„Ù…Ø§Ø¯Ø© (12)
            r'Ø§Ù„Ù…Ø§Ø¯Ø©\s*(\d+)',              # Ø§Ù„Ù…Ø§Ø¯Ø© 12
            r'Ù…Ø§Ø¯Ø©\s*\((\d+)\)',            # Ù…Ø§Ø¯Ø© (12)
            r'Ù…Ø§Ø¯Ø©\s*(\d+)',               # Ù…Ø§Ø¯Ø© 12
            r'Ø§Ù„Ù…Ø§Ø¯Ø©\s*Ø±Ù‚Ù…\s*(\d+)',        # Ø§Ù„Ù…Ø§Ø¯Ø© Ø±Ù‚Ù… 12
            r'ÙˆÙÙ‚\s*Ø§Ù„Ù…Ø§Ø¯Ø©\s*Ø±Ù‚Ù…\s*(\d+)',  # ÙˆÙÙ‚ Ø§Ù„Ù…Ø§Ø¯Ø© Ø±Ù‚Ù… 12
            r'Ø§Ø³ØªÙ†Ø§Ø¯Ø§Ù‹\s*Ù„Ù„Ù…Ø§Ø¯Ø©\s*(\d+)',   # Ø§Ø³ØªÙ†Ø§Ø¯Ø§Ù‹ Ù„Ù„Ù…Ø§Ø¯Ø© 12
        ]
    
    def extract_citations(self, text: str) -> List[str]:
        """Extract all legal citations from text with enhanced patterns"""
        citations = []
        
        for pattern in self.citation_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                # Normalize citation format
                normalized = f"Ø§Ù„Ù…Ø§Ø¯Ø© ({match})"
                if normalized not in citations:
                    citations.append(normalized)
        
        return citations
    
    def validate_citations(self, response: str, available_documents: List[Any]) -> Tuple[bool, List[str]]:
        """Validate that response only uses available citations"""
        warnings = []
        
        if not available_documents:
            response_citations = self.extract_citations(response)
            if response_citations:
                warnings.append(f"Response contains citations but no documents provided: {response_citations}")
                return False, warnings
            return True, warnings
        
        # Extract citations from response and documents
        response_citations = self.extract_citations(response)
        available_citations = []
        
        for doc in available_documents:
            try:
                if hasattr(doc, 'content') and doc.content:
                    available_citations.extend(self.extract_citations(doc.content))
            except Exception as e:
                warnings.append(f"Error processing document: {str(e)}")
        
        # Validate each citation
        invalid_citations = []
        for citation in response_citations:
            if citation not in available_citations:
                invalid_citations.append(citation)
        
        if invalid_citations:
            warnings.append(f"Invalid citations found: {invalid_citations}")
            return False, warnings
        
        return True, warnings


@dataclass
class ConversationContext:
    """Dynamic conversation context - no hardcoded patterns"""
    conversation_flow: str  # 'first_message', 'follow_up', 'topic_change', 'continuation'
    case_context: str      # AI-extracted case summary
    continuity_instruction: str  # AI-generated continuity guidance
    strategic_elements: Dict[str, Any]  # Dynamic strategic context
    confidence: float      # AI confidence in analysis


class DynamicConversationAnalyzer:
    """
    ğŸ¯ Pure AI-driven conversation analysis
    
    Replaces the hardcoded EnhancedFactExtractor with intelligent AI analysis
    that adapts to any conversation pattern, legal domain, or case type.
    
    Zero hardcoded patterns - infinite scalability
    """
    
    def __init__(self, ai_client=None):
        self.ai_client = ai_client
        self.analysis_cache = {}  # Simple in-memory cache for session
        
    async def analyze_conversation_context(
        self, 
        current_query: str, 
        conversation_history: List[Dict[str, str]]
    ) -> ConversationContext:
        """
        ğŸ§  Dynamic AI analysis of conversation context
        
        Replaces all hardcoded pattern matching with intelligent AI understanding
        """
        
        try:
            # Quick cache check for identical contexts
            cache_key = self._generate_cache_key(current_query, conversation_history)
            if cache_key in self.analysis_cache:
                logger.debug("ğŸ“‹ Using cached conversation analysis")
                return self.analysis_cache[cache_key]
            
            # Analyze with AI if client available, otherwise use lightweight fallback
            if self.ai_client:
                context = await self._ai_powered_analysis(current_query, conversation_history)
            else:
                context = self._lightweight_analysis(current_query, conversation_history)
            
            # Cache result for this session
            self.analysis_cache[cache_key] = context
            
            logger.info(f"ğŸ¯ Dynamic analysis: {context.conversation_flow} | Confidence: {context.confidence:.2f}")
            return context
            
        except Exception as e:
            logger.warning(f"Analysis failed, using fallback: {e}")
            return self._fallback_analysis(current_query, conversation_history)
    
    async def _ai_powered_analysis(
        self,
        current_query: str,
        conversation_history: List[Dict[str, str]]
    ) -> ConversationContext:
        """
        ğŸ¤– Pure AI analysis - no hardcoded patterns
        
        Uses AI to understand conversation context dynamically
        """
        
        # Build analysis prompt
        analysis_prompt = self._build_analysis_prompt(current_query, conversation_history)
        
        try:
            response = await self.ai_client.chat.completions.create(
                model="gpt-4o-mini",  # Fast model for analysis
                messages=[
                    {
                        "role": "system",
                        "content": "Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°ÙƒÙŠ. Ø­Ù„Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆÙ‚Ø¯Ù… JSON ØµØ­ÙŠØ­ ÙÙ‚Ø·."
                    },
                    {"role": "user", "content": analysis_prompt}
                ],
                max_tokens=300,  # Keep it concise
                temperature=0.1,  # Consistent analysis
                timeout=5  # Fast response
            )
            
            content = response.choices[0].message.content.strip()
            
            # Parse AI response
            import json
            analysis_data = self._parse_ai_response(content)
            
            # ğŸš€ ENHANCED: Extract detailed case context
            extracted_details = self._extract_specific_details(conversation_history)
            case_context = analysis_data.get("case_context", "")
            
            # Combine AI analysis with extracted details
            if extracted_details and extracted_details != "Ù‚Ø¶ÙŠØ© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¹Ø§Ù…Ø©":
                case_context = f"{extracted_details} | {case_context}" if case_context else extracted_details
            
            return ConversationContext(
                conversation_flow=analysis_data.get("conversation_flow", "continuation"),
                case_context=case_context,
                continuity_instruction=analysis_data.get("continuity_instruction", ""),
                strategic_elements=analysis_data.get("strategic_elements", {}),
                confidence=float(analysis_data.get("confidence", 70)) / 100
            )
            
        except Exception as e:
            logger.warning(f"AI analysis failed: {e}")
            return self._lightweight_analysis(current_query, conversation_history)
    
    

    def _build_analysis_prompt(
        self, 
        current_query: str, 
        conversation_history: List[Dict[str, str]]
    ) -> str:
        """Build ENHANCED analysis prompt with clear classification rules"""
        
        # Format conversation for AI analysis
        history_text = ""
        history_length = len(conversation_history)
        
        if conversation_history:
            recent_messages = conversation_history[-4:]  # Last 4 messages for context
            for i, msg in enumerate(recent_messages):
                role = "Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…" if msg['role'] == 'user' else "Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ"
                content = msg['content'][:150]  # More content for better analysis
                history_text += f"{role}: {content}...\n"
        
        # Determine obvious classification first
        current_lower = current_query.lower()
        follow_up_indicators = [
            "Ø§Ù„Ù…Ø°ÙƒÙˆØ±", "Ø³Ø§Ø¨Ù‚Ø§Ù‹", "ÙƒÙ…Ø§ Ù†Ø§Ù‚Ø´Ù†Ø§", "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰", "Ø§Ù„Ù…Ø¨Ù„Øº Ø§Ù„Ù…Ø°ÙƒÙˆØ±", 
            "ÙƒÙ…Ø§ Ù‚Ù„Øª", "ÙÙŠ Ø¶ÙˆØ¡ Ù…Ø§ Ø³Ø¨Ù‚", "Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø°ÙŠ", "Ø§Ù„Ù‚Ø¶ÙŠØ© Ø§Ù„ØªÙŠ",
            "Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø¹ÙˆÙ‰", "Ù…ÙˆÙ‚ÙÙŠ", "Ø­Ø§Ù„ØªÙŠ", "ÙˆØ¶Ø¹ÙŠ"
        ]
        
        has_follow_up_indicators = any(indicator in current_lower for indicator in follow_up_indicators)
        
        # Pre-determine the correct flow
        if history_length == 0:
            suggested_flow = "first_message"
        elif has_follow_up_indicators and history_length > 0:
            suggested_flow = "follow_up"
        elif history_length > 0:
            suggested_flow = "continuation"
        else:
            suggested_flow = "first_message"
        
        return f"""Ø­Ù„Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØµÙ†Ù Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø¯Ù‚Ø©:

Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ({history_length} Ø±Ø³Ø§Ø¦Ù„):
{history_text if history_text else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­Ø§Ø¯Ø«Ø© Ø³Ø§Ø¨Ù‚Ø©"}

Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ:
"{current_query}"

ğŸ¯ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠØ©:
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ù…Ø­Ø§Ø¯Ø«Ø© Ø³Ø§Ø¨Ù‚Ø© AND Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ´ÙŠØ± Ù„Ù‡Ø§ = "follow_up" 
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰: "Ø§Ù„Ù…Ø°ÙƒÙˆØ±ØŒ Ø³Ø§Ø¨Ù‚Ø§Ù‹ØŒ ÙƒÙ…Ø§ Ù†Ø§Ù‚Ø´Ù†Ø§ØŒ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰" = "follow_up"
- Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ù…Ø­Ø§Ø¯Ø«Ø© Ø³Ø§Ø¨Ù‚Ø© = "first_message"
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÙŠØ¯ Ù…Ø®ØªÙ„Ù = "topic_change"

ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ:
- Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©: {history_length}
- ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©: {"Ù†Ø¹Ù…" if has_follow_up_indicators else "Ù„Ø§"}
- Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù‚ØªØ±Ø­: {suggested_flow}

Ø£Ø±ÙŠØ¯ JSON ÙÙ‚Ø·:
{{
    "conversation_flow": "{suggested_flow}",
    "case_context": "Ù…Ù„Ø®Øµ Ù…ÙˆØ¬Ø² Ù„Ù„Ù‚Ø¶ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© (Ø§Ø°ÙƒØ± Ù…Ø¨Ø§Ù„ØºØŒ Ø£Ø±Ù‚Ø§Ù…ØŒ ØªÙØ§ØµÙŠÙ„ Ù…Ø­Ø¯Ø¯Ø©)",
    "continuity_instruction": "ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù„Ù„Ù…Ø­Ø§Ù…ÙŠ Ù„Ù„Ø±Ø¨Ø· Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚ Ù…Ø¹ Ø°ÙƒØ± Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©",
    "strategic_elements": {{
        "case_strength": "Ù‚ÙˆÙŠ Ø£Ùˆ Ù…ØªÙˆØ³Ø· Ø£Ùˆ Ø¶Ø¹ÙŠÙ Ø£Ùˆ ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
        "main_topic": "Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ",
        "user_position": "Ø§Ù„Ù…Ø³ØªÙØ³Ø± Ø£Ùˆ Ø§Ù„Ù…Ø¯Ø¹ÙŠ Ø£Ùˆ Ø§Ù„Ù…Ø¯Ø¹Ù‰ Ø¹Ù„ÙŠÙ‡ Ø£Ùˆ ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
    }},
    "confidence": "Ø±Ù‚Ù… Ù…Ù† 85 Ø¥Ù„Ù‰ 95"
}}

JSON ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø´Ø±Ø­:"""
    



    def _extract_specific_details(self, conversation_history: List[Dict[str, str]]) -> str:
        """Extract specific amounts, dates, companies, and case details"""
        
        if not conversation_history:
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙØ§ØµÙŠÙ„ Ù…Ø­Ø¯Ø¯Ø©"
        
        # Combine all conversation text
        full_text = ""
        for msg in conversation_history:
            full_text += msg.get('content', '') + " "
        
        details = []
        
        # Extract specific amounts
        import re
        amounts = re.findall(r'(\d+[\d,]*)\s*Ø±ÙŠØ§Ù„', full_text)
        if amounts:
            details.append(f"Ø§Ù„Ù…Ø¨Ù„Øº: {amounts[0]} Ø±ÙŠØ§Ù„")
        
        # Extract time periods
        years = re.findall(r'(\d+)\s*Ø³Ù†Ùˆ?Ø§Øª?', full_text)
        if years:
            details.append(f"Ù…Ø¯Ø© Ø§Ù„Ø®Ø¯Ù…Ø©: {years[0]} Ø³Ù†ÙˆØ§Øª")
        
        # Extract company types
        companies = re.findall(r'Ø´Ø±ÙƒØ©\s+(\w+)', full_text)
        if companies:
            details.append(f"Ù†ÙˆØ¹ Ø§Ù„Ø´Ø±ÙƒØ©: {companies[0]}")
        elif 'Ù…Ù‚Ø§ÙˆÙ„Ø§Øª' in full_text:
            details.append("Ù†ÙˆØ¹ Ø§Ù„Ø´Ø±ÙƒØ©: Ù…Ù‚Ø§ÙˆÙ„Ø§Øª")
        
        # Extract case strength
        strength_match = re.search(r'Ù…ÙˆÙ‚Ù.*?Ù‚ÙˆÙŠ.*?(\d+)%', full_text)
        if strength_match:
            details.append(f"Ù‚ÙˆØ© Ø§Ù„Ù…ÙˆÙ‚Ù: {strength_match.group(1)}%")
        elif 'Ù‚ÙˆÙŠ' in full_text:
            details.append("ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ÙˆÙ‚Ù: Ù‚ÙˆÙŠ")
        
        # Extract legal issues
        if 'ØªØ³Ø±ÙŠØ­' in full_text and 'ØªØ¹Ø³ÙÙŠ' in full_text:
            details.append("Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø¶ÙŠØ©: ØªØ³Ø±ÙŠØ­ ØªØ¹Ø³ÙÙŠ")
        elif 'Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø®Ø¯Ù…Ø©' in full_text:
            details.append("Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø¶ÙŠØ©: Ø­Ù‚ÙˆÙ‚ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø®Ø¯Ù…Ø©")
        
        return " | ".join(details) if details else "Ù‚Ø¶ÙŠØ© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¹Ø§Ù…Ø©"
    


    def _parse_ai_response(self, content: str) -> Dict[str, Any]:
        """Parse AI response with fallbacks"""
        
        try:
            # Clean the response
            content = content.replace("```json", "").replace("```", "").strip()
            
            # Find JSON boundaries
            start_idx = content.find('{')
            end_idx = content.rfind('}') + 1
            if start_idx != -1 and end_idx != 0:
                content = content[start_idx:end_idx]
            
            import json
            return json.loads(content)
            
        except Exception as e:
            logger.warning(f"JSON parsing failed: {e}")
            # Return safe defaults
            return {
                "conversation_flow": "follow_up",  # Default to follow_up now
                "case_context": "Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø¹ ØªÙØ§ØµÙŠÙ„ Ø³Ø§Ø¨Ù‚Ø©",
                "continuity_instruction": "Ø§Ø±Ø¨Ø· Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚ ÙˆØ§Ø°ÙƒØ± Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©",
                "strategic_elements": {
                    "case_strength": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                    "main_topic": "Ù…ÙˆØ¶ÙˆØ¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ",
                    "user_position": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
                },
                "confidence": 75
            }
        

        
    def _lightweight_analysis(
        self, 
        current_query: str, 
        conversation_history: List[Dict[str, str]]
    ) -> ConversationContext:
        """
        ğŸ’¨ Lightweight analysis when AI is not available
        
        Uses simple heuristics without hardcoded patterns
        """
        
        # Determine conversation flow based on history length
        if not conversation_history or len(conversation_history) == 0:
            flow = "first_message"
            instruction = "Ø§Ø¨Ø¯Ø£ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø´Ø§Ù…Ù„Ø© ÙˆÙˆØ§Ø¶Ø­Ø©"
        elif len(conversation_history) <= 2:
            flow = "follow_up"
            instruction = "Ø§Ø±Ø¨Ø· Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚ ÙˆØ£Ø¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©"
        else:
            flow = "continuation"
            instruction = "ØªØ§Ø¨Ø¹ Ø§Ù„Ù†Ù‚Ø§Ø´ Ù…Ø¹ Ø§Ù„ØªØ¹Ù…Ù‚ Ø£ÙƒØ«Ø±"
        
        # Extract basic case context from query length and complexity
        if len(current_query) > 200:
            case_context = "Ù‚Ø¶ÙŠØ© Ù…Ø¹Ù‚Ø¯Ø© ØªØ­ØªØ§Ø¬ ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„"
        elif len(current_query) < 50:
            case_context = "Ø§Ø³ØªÙØ³Ø§Ø± Ø³Ø±ÙŠØ¹"
        else:
            case_context = "Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¹Ø§Ù…Ø©"
        
        return ConversationContext(
            conversation_flow=flow,
            case_context=case_context,
            continuity_instruction=instruction,
            strategic_elements={
                "case_strength": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                "main_topic": "Ù…ÙˆØ¶ÙˆØ¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ",
                "user_position": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
            },
            confidence=0.6  # Moderate confidence for heuristic analysis
        )
    
    def _fallback_analysis(
        self, 
        current_query: str, 
        conversation_history: List[Dict[str, str]]
    ) -> ConversationContext:
        """Safe fallback when everything fails"""
        
        return ConversationContext(
            conversation_flow="continuation",
            case_context="Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©",
            continuity_instruction="Ù‚Ø¯Ù… Ù…Ø´ÙˆØ±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ§Ø¶Ø­Ø©",
            strategic_elements={
                "case_strength": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                "main_topic": "Ø§Ø³ØªØ´Ø§Ø±Ø© Ø¹Ø§Ù…Ø©", 
                "user_position": "Ù…Ø³ØªÙØ³Ø±"
            },
            confidence=0.5
        )
    
    def _generate_cache_key(
        self, 
        current_query: str, 
        conversation_history: List[Dict[str, str]]
    ) -> str:
        """Generate cache key for identical contexts"""
        
        # Create a hash of query + recent history for caching
        import hashlib
        
        context_string = current_query[:100]  # First 100 chars of query
        if conversation_history:
            # Add last message for context
            last_msg = conversation_history[-1].get('content', '')[:50]
            context_string += last_msg
        
        return hashlib.md5(context_string.encode()).hexdigest()[:8]




class ConversationSynthesizer:
    """Synthesizes previous analysis with new questions for conversation continuity"""
    
    def __init__(self, ai_client=None):
        """Initialize with dynamic AI analyzer"""
        self.dynamic_analyzer = DynamicConversationAnalyzer(ai_client)
        self.key_fact_patterns = self._load_fact_extraction_patterns()
        self.follow_up_patterns = self._load_follow_up_patterns()
    
    def _load_fact_extraction_patterns(self) -> Dict[str, List[str]]:
        """Patterns to extract key facts from previous responses"""
        return {
            "case_facts": [
                r"Ø§Ù„Ù…Ø¨Ù„Øº Ø§Ù„Ù…Ø­ÙˆÙ„.*?(\d+[\d,]+)",  # Amount transferred
                r"Ø¨ØªØ§Ø±ÙŠØ®.*?(\d+/\d+/\d+)",      # Dates
                r"Ù„Ø£ØºØ±Ø§Ø¶.*?([^.]+)",            # Purpose of transfer
                r"Ø§Ù„Ù…Ø¯Ø¹ÙŠ.*?([^.]+)",            # Plaintiff details
                r"Ø§Ù„Ù…Ø¯Ø¹Ù‰ Ø¹Ù„ÙŠÙ‡.*?([^.]+)",       # Defendant details
            ],
            "legal_conclusions": [
                r"ÙˆÙÙ‚Ø§Ù‹ Ù„Ù„Ù…Ø§Ø¯Ø©.*?([^.]+)",       # Legal articles cited
                r"ÙŠÙ…ÙƒÙ† Ø§Ø¹ØªØ¨Ø§Ø±.*?([^.]+)",       # Legal assessments
                r"Ø§Ù„Ø¯Ø¹ÙˆÙ‰.*?(ÙƒÙŠØ¯ÙŠØ©|Ø¶Ø¹ÙŠÙØ©|Ù‚ÙˆÙŠØ©)", # Case strength assessments
            ],
            "strategic_elements": [
                r"Ø£Ù†ØµØ­.*?([^.]+)",              # Recommendations given
                r"ÙŠØ¬Ø¨.*?([^.]+)",               # Required actions
                r"Ø§Ù„Ø¯ÙØ¹.*?([^.]+)",             # Defense strategies
            ]
        }
    
    def _load_follow_up_patterns(self) -> List[str]:
        """Patterns that indicate follow-up questions"""
        return [
            "Ù‡Ù„ ÙŠÙ…ÙƒÙ†", "ÙˆÙ…Ø§Ø°Ø§ Ø¹Ù†", "ÙƒÙŠÙ", "ÙˆÙ…Ø§ Ù‡ÙŠ", "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø§ Ø°ÙƒØ±Øª",
            "ÙƒÙ…Ø§ Ù‚Ù„Øª", "Ø§Ù„Ù…Ø°ÙƒÙˆØ± Ø³Ø§Ø¨Ù‚Ø§Ù‹", "ÙÙŠ Ø¶ÙˆØ¡ Ù…Ø§ Ø³Ø¨Ù‚", "Ø¥Ø¶Ø§ÙØ© Ù„Ù…Ø§ Ø°ÙƒØ±Øª"
        ]
    
    def synthesize_strategic_conversation_context(
        self, 
         context: LegalContext
    ) -> LegalContext:
        """ğŸ§  Pure AI-driven conversation synthesis
    
    REPLACES: All hardcoded pattern matching and fact extraction
    WITH: Dynamic AI analysis that adapts to any conversation
        """
        if not context.conversation_history:
            return context
        try:
            logger.info("ğŸ¯ Using simplified conversation synthesis")
            return self._fallback_synthesis(context)
        except Exception as e: 
            logger.warning(f"Dynamic synthesis failed, using fallback: {e}")
            return self._fallback_synthesis(context)
    
    def _build_dynamic_continuity_prompt(
        self, 
        original_query: str, 
        conversation_context: ConversationContext
    ) -> str:
        """
        ğŸ¯ Build continuity prompt based on AI analysis
        
        REPLACES: Hardcoded templates and pattern matching
        WITH: Dynamic prompt building based on AI understanding
        """
        
        flow = conversation_context.conversation_flow
        case_context = conversation_context.case_context
        continuity_instruction = conversation_context.continuity_instruction
        
        # Build dynamic prompt based on AI analysis
        if flow == "first_message":
            # First message - comprehensive analysis
            return f"""**Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©:**

**Ø§Ù„Ø³Ø¤Ø§Ù„:** {original_query}

**ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø±Ø¯:**
- Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø´Ø§Ù…Ù„Ø§Ù‹ ÙˆÙ…ÙØµÙ„Ø§Ù‹
- Ø§Ø¨Ø¯Ø£ Ø¨ØªØ±Ø­ÙŠØ¨ Ù…Ù‡Ù†ÙŠ ÙˆØ¯Ø§ÙØ¦
- Ø§Ø´Ø±Ø­ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨ÙˆØ¶ÙˆØ­
- Ù‚Ø¯Ù… ØªÙˆØµÙŠØ§Øª Ø¹Ù…Ù„ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©

Ù‡Ø°Ù‡ Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© ØªØ­ØªØ§Ø¬ ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø´Ø§Ù…Ù„Ø§Ù‹."""

        elif flow == "follow_up":
            # Follow-up question - reference previous context
            return f"""**Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**

**Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚:** {case_context}

**Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯:** {original_query}

**ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø±Ø¯:**
- Ø§Ø¨Ø¯Ø£ Ø¨Ù€ "Ø£Ø®ÙŠØŒ {continuity_instruction}"
- Ø§Ø±Ø¨Ø· Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚
- Ø£Ø¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ø¯ÙˆÙ† ØªÙƒØ±Ø§Ø±
- Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù…ØªØ³Ù‚Ø©

{continuity_instruction}"""

        elif flow == "topic_change":
            # Topic change - acknowledge transition
            return f"""**Ø§Ù†ØªÙ‚Ø§Ù„ Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÙŠØ¯:**

**Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø¬Ø¯ÙŠØ¯:** {original_query}

**ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø±Ø¯:**
- Ø§Ø¹ØªØ±Ù Ø¨ØªØºÙŠÙŠØ± Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹: "Ø§Ù†ØªÙ‚Ø§Ù„Ø§Ù‹ Ø¥Ù„Ù‰ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÙŠØ¯..."
- Ø§Ø¨Ø¯Ø£ ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø¬Ø¯ÙŠØ¯Ø§Ù‹ Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø¬Ø¯ÙŠØ¯
- Ù‚Ø¯Ù… Ø§Ø³ØªØ´Ø§Ø±Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø¬Ø¯ÙŠØ¯
- Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ù†ÙØ³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø¨Ø±Ø© ÙˆØ§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©

{continuity_instruction}"""

        else:  # continuation
            # Continuation - deepen the discussion
            return f"""**Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:**

**Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠ:** {case_context}

**Ù†Ù‚Ø·Ø© Ø§Ù„Ù†Ù‚Ø§Ø´:** {original_query}

**ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø±Ø¯:**
- ØªØ§Ø¨Ø¹ Ø§Ù„Ù†Ù‚Ø§Ø´ Ù…Ø¹ Ø§Ù„ØªØ¹Ù…Ù‚ Ø£ÙƒØ«Ø±
- Ø£Ø¶Ù Ø²ÙˆØ§ÙŠØ§ Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹
- Ù‚Ø¯Ù… ØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ© Ù…ÙÙŠØ¯Ø©
- Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ

{continuity_instruction}"""
    
    def _get_dynamic_escalation(self, conversation_context: ConversationContext) -> str:
        """Get escalation suggestion based on AI analysis"""
        
        strategic_elements = conversation_context.strategic_elements
        case_strength = strategic_elements.get("case_strength", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯")
        
        # Dynamic escalation based on AI analysis
        if case_strength == "Ù‚ÙˆÙŠ":
            return "Ø§Ù„Ù…ÙˆÙ‚Ù Ù‚ÙˆÙŠ - ÙŠÙ…ÙƒÙ† Ø§ØªØ®Ø§Ø° Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø­Ø§Ø³Ù…Ø©"
        elif case_strength == "Ù…ØªÙˆØ³Ø·":
            return "Ø§Ù„Ù…ÙˆÙ‚Ù Ù…ØªÙˆØ³Ø· - ÙŠØ­ØªØ§Ø¬ ØªÙ‚ÙˆÙŠØ© Ø¨Ø§Ù„Ø£Ø¯Ù„Ø©"
        elif case_strength == "Ø¶Ø¹ÙŠÙ":
            return "Ø§Ù„Ù…ÙˆÙ‚Ù ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ† - Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¨Ø¯Ø§Ø¦Ù„"
        else:
            return "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ù ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"
    
    def _fallback_synthesis(self, context: LegalContext) -> LegalContext:
        """Safe fallback when dynamic analysis fails"""
        
        if not context.conversation_history:
            return context
        
        # Simple fallback based on conversation length
        if len(context.conversation_history) <= 2:
            enhanced_query = f"""**Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø©:**

Ø§Ù„Ø³Ø¤Ø§Ù„: {context.query}

ØªØ¹Ù„ÙŠÙ…Ø§Øª: Ø§Ø±Ø¨Ø· Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚ ÙˆØ£Ø¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©."""
        else:
            enhanced_query = f"""**Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„Ù†Ù‚Ø§Ø´:**

Ø§Ù„Ø³Ø¤Ø§Ù„: {context.query}

ØªØ¹Ù„ÙŠÙ…Ø§Øª: ØªØ§Ø¨Ø¹ Ø§Ù„Ù†Ù‚Ø§Ø´ Ù…Ø¹ Ø§Ù„ØªØ¹Ù…Ù‚ Ø£ÙƒØ«Ø±."""
        
        context.query = enhanced_query
        context.previous_analysis_summary = "Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø³ØªÙ…Ø±Ø©"
        
        return context


class ConversationSynthesizer:
    """
    ğŸ¯ Enhanced conversation synthesis with pure AI analysis
    
    Replaces hardcoded pattern matching with dynamic AI understanding
    """
    
    def __init__(self, ai_client=None):
        """Initialize with dynamic AI analyzer"""
        self.dynamic_analyzer = DynamicConversationAnalyzer(ai_client)
        self.key_fact_patterns = self._load_fact_extraction_patterns()
        self.follow_up_patterns = self._load_follow_up_patterns()
    
    def _load_fact_extraction_patterns(self) -> Dict[str, List[str]]:
        """Patterns to extract key facts from previous responses"""
        return {
            "case_facts": [
                r"Ø§Ù„Ù…Ø¨Ù„Øº Ø§Ù„Ù…Ø­ÙˆÙ„.*?(\d+[\d,]+)",  # Amount transferred
                r"Ø¨ØªØ§Ø±ÙŠØ®.*?(\d+/\d+/\d+)",      # Dates
                r"Ù„Ø£ØºØ±Ø§Ø¶.*?([^.]+)",            # Purpose of transfer
                r"Ø§Ù„Ù…Ø¯Ø¹ÙŠ.*?([^.]+)",            # Plaintiff details
                r"Ø§Ù„Ù…Ø¯Ø¹Ù‰ Ø¹Ù„ÙŠÙ‡.*?([^.]+)",       # Defendant details
            ],
            "legal_conclusions": [
                r"ÙˆÙÙ‚Ø§Ù‹ Ù„Ù„Ù…Ø§Ø¯Ø©.*?([^.]+)",       # Legal articles cited
                r"ÙŠÙ…ÙƒÙ† Ø§Ø¹ØªØ¨Ø§Ø±.*?([^.]+)",       # Legal assessments
                r"Ø§Ù„Ø¯Ø¹ÙˆÙ‰.*?(ÙƒÙŠØ¯ÙŠØ©|Ø¶Ø¹ÙŠÙØ©|Ù‚ÙˆÙŠØ©)", # Case strength assessments
            ],
            "strategic_elements": [
                r"Ø£Ù†ØµØ­.*?([^.]+)",              # Recommendations given
                r"ÙŠØ¬Ø¨.*?([^.]+)",               # Required actions
                r"Ø§Ù„Ø¯ÙØ¹.*?([^.]+)",             # Defense strategies
            ]
        }
    
    def _load_follow_up_patterns(self) -> List[str]:
        """Patterns that indicate follow-up questions"""
        return [
            "Ù‡Ù„ ÙŠÙ…ÙƒÙ†", "ÙˆÙ…Ø§Ø°Ø§ Ø¹Ù†", "ÙƒÙŠÙ", "ÙˆÙ…Ø§ Ù‡ÙŠ", "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø§ Ø°ÙƒØ±Øª",
            "ÙƒÙ…Ø§ Ù‚Ù„Øª", "Ø§Ù„Ù…Ø°ÙƒÙˆØ± Ø³Ø§Ø¨Ù‚Ø§Ù‹", "ÙÙŠ Ø¶ÙˆØ¡ Ù…Ø§ Ø³Ø¨Ù‚", "Ø¥Ø¶Ø§ÙØ© Ù„Ù…Ø§ Ø°ÙƒØ±Øª"
        ]
    
    async def synthesize_strategic_conversation_context(self, context: LegalContext) -> LegalContext:
        """
        ğŸ§  Pure AI-driven conversation synthesis - FIXED ASYNC VERSION
        
        REPLACES: All hardcoded pattern matching and fact extraction
        WITH: Dynamic AI analysis that adapts to any conversation
        """
        
        if not context.conversation_history:
            # No conversation history - return as-is
            return context
        
        try:
            # ğŸš€ FIXED: Proper async AI-powered conversation analysis
            conversation_summary = await self._analyze_conversation_with_ai(context.conversation_history)
            
            # ğŸ¯ BUILD ENHANCED QUERY with conversation context
            enhanced_query = self._build_contextual_query(
                original_query=context.query,
                conversation_summary=conversation_summary
            )
            
            context.query = enhanced_query
            context.previous_analysis_summary = conversation_summary
            logger.info("âœ… Enhanced conversation synthesis with AI analysis")
            return context
            
        except Exception as e:
            logger.warning(f"Enhanced synthesis failed, using fallback: {e}")
            return self._fallback_synthesis(context)
    
    async def _analyze_conversation_with_ai(self, conversation_history: List[Dict[str, str]]) -> str:
        """
        ğŸ¤– Use AI to analyze conversation and extract key details
        
        FIXED: Proper async execution without event loop conflicts
        """
        
        # Get conversation text
        conversation_text = self._get_conversation_text(conversation_history)
        
        if not conversation_text:
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­Ø§Ø¯Ø«Ø© Ø³Ø§Ø¨Ù‚Ø©"
        
        try:
            # Use the dynamic analyzer for AI-powered analysis
            if hasattr(self.dynamic_analyzer, 'ai_client') and self.dynamic_analyzer.ai_client:
                # âœ… FIXED: Direct async call without event loop conflicts
                result = await self.dynamic_analyzer.analyze_conversation_context(
                    current_query="ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©", 
                    conversation_history=conversation_history
                )
                return result.case_context or "Ù…Ø­Ø§Ø¯Ø«Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©"
            else:
                # Use lightweight analysis
                result = self.dynamic_analyzer._lightweight_analysis("ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©", conversation_history)
                return result.case_context
                
        except Exception as e:
            logger.warning(f"AI conversation analysis failed: {e}")
            return self._extract_key_details_fallback(conversation_text)
    
    def _build_contextual_query(self, original_query: str, conversation_summary: str) -> str:
        """
        ğŸ¯ Build enhanced query with conversation context
        
        Creates intelligent continuity prompts based on AI analysis
        """
        
        if not conversation_summary or conversation_summary == "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­Ø§Ø¯Ø«Ø© Ø³Ø§Ø¨Ù‚Ø©":
            return original_query
        
        # Build context-aware prompt
        enhanced_query = f"""**Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**

**Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚:** {conversation_summary}

**Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ:** {original_query}

**ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù„Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø±ÙŠØ©:**
- Ø§Ø¨Ø¯Ø£ Ø¨Ù€ "Ø£Ø®ÙŠØŒ ÙƒÙ…Ø§ Ù†Ø§Ù‚Ø´Ù†Ø§ Ø³Ø§Ø¨Ù‚Ø§Ù‹..." Ø£Ùˆ "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø§ ØªØ·Ø±Ù‚Ù†Ø§ Ø¥Ù„ÙŠÙ‡..."
- Ø§Ø±Ø¨Ø· Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¨Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
- Ø§Ø°ÙƒØ± Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ù…Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© (Ù…Ø¨Ø§Ù„ØºØŒ ØªÙˆØ§Ø±ÙŠØ®ØŒ Ø£Ø³Ù…Ø§Ø¡)
- Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…ØªØ³Ù‚Ø©
- Ø£Ø¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ø¯ÙˆÙ† ØªÙƒØ±Ø§Ø±

Ù‚Ø¯Ù… Ø§Ø³ØªØ´Ø§Ø±Ø© ØªØ¨Ù†ÙŠ Ø¹Ù„Ù‰ Ù…Ø§ Ø³Ø¨Ù‚ Ù…Ø¹ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©."""

        return enhanced_query
    
    def _get_conversation_text(self, conversation_history: List[Dict[str, str]]) -> str:
        """Extract conversation text for analysis"""
        
        # Get last 4 messages for context (both user and assistant)
        messages = []
        for msg in reversed(conversation_history[-8:]):  # Last 8 messages
            content = msg.get('content', '')
            if content and len(content) > 10:  # Skip very short messages
                messages.append(content)
        
        return ' '.join(reversed(messages))
    
    def _extract_key_details_fallback(self, conversation_text: str) -> str:
        """
        ğŸ’¡ Fallback detail extraction using simple text analysis
        
        Used when AI analysis is not available
        """
        
        details = []
        
        # Extract amounts
        import re
        amounts = re.findall(r'(\d+[\d,]*)\s*Ø±ÙŠØ§Ù„', conversation_text)
        if amounts:
            details.append(f"Ø§Ù„Ù…Ø¨Ù„Øº: {amounts[0]} Ø±ÙŠØ§Ù„")
        
        # Extract case types
        if 'ØªØ³Ø±ÙŠØ­' in conversation_text or 'Ø¹Ù…Ù„' in conversation_text:
            details.append("Ù‚Ø¶ÙŠØ© Ø¹Ù…Ø§Ù„ÙŠØ©")
        elif 'Ù‚Ø±Ø¶' in conversation_text or 'Ø¯ÙŠÙ†' in conversation_text:
            details.append("Ù†Ø²Ø§Ø¹ Ù…Ø§Ù„ÙŠ")
        elif 'Ø¹Ù‚Ø¯' in conversation_text:
            details.append("Ù†Ø²Ø§Ø¹ ØªØ¹Ø§Ù‚Ø¯ÙŠ")
        
        # Extract strength assessments
        strength_match = re.search(r'Ù…ÙˆÙ‚Ù.*?(\w+)', conversation_text)
        if strength_match:
            details.append(f"ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ÙˆÙ‚Ù: {strength_match.group(1)}")
        
        percentage_match = re.search(r'(\d+)%', conversation_text)
        if percentage_match:
            details.append(f"Ù‚ÙˆØ© Ø§Ù„Ù…ÙˆÙ‚Ù: {percentage_match.group(1)}%")
        
        return " | ".join(details) if details else "Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø³ØªÙ…Ø±Ø©"
    
    def _fallback_synthesis(self, context: LegalContext) -> LegalContext:
        """Safe fallback when AI analysis fails"""
        
        if not context.conversation_history:
            return context
        
        # Simple fallback based on conversation length
        if len(context.conversation_history) <= 2:
            enhanced_query = f"""**Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø©:**

Ø§Ù„Ø³Ø¤Ø§Ù„: {context.query}

ØªØ¹Ù„ÙŠÙ…Ø§Øª: Ø§Ø±Ø¨Ø· Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚ ÙˆØ£Ø¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ø°ÙƒØ± "ÙƒÙ…Ø§ Ù†Ø§Ù‚Ø´Ù†Ø§ Ø³Ø§Ø¨Ù‚Ø§Ù‹"."""
        else:
            enhanced_query = f"""**Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„Ù†Ù‚Ø§Ø´:**

Ø§Ù„Ø³Ø¤Ø§Ù„: {context.query}

ØªØ¹Ù„ÙŠÙ…Ø§Øª: ØªØ§Ø¨Ø¹ Ø§Ù„Ù†Ù‚Ø§Ø´ Ù…Ø¹ Ø§Ù„ØªØ¹Ù…Ù‚ Ø£ÙƒØ«Ø± ÙˆØ§Ø³ØªØ®Ø¯Ù… "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø§ ØªÙ… Ø´Ø±Ø­Ù‡"."""
        
        context.query = enhanced_query
        context.previous_analysis_summary = "Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø³ØªÙ…Ø±Ø©"
        
        logger.info("ğŸ¯ Using simplified conversation synthesis")
        return context
    
    # Legacy methods for backward compatibility
    def extract_previous_analysis(self, conversation_history: List[Dict[str, str]]) -> str:
        """Extract key facts and conclusions from previous AI responses"""
        
        if not conversation_history:
            return ""
        
        # Get the last assistant response (most recent context)
        last_assistant_response = ""
        for msg in reversed(conversation_history):
            if msg.get('role') == 'assistant':
                last_assistant_response = msg.get('content', '')
                break
        
        if not last_assistant_response:
            return ""
        
        # Extract structured facts
        extracted_facts = []
        
        # Extract case facts
        import re
        for category, patterns in self.key_fact_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, last_assistant_response)
                if matches:
                    if category == "case_facts":
                        extracted_facts.append(f"Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø¶ÙŠØ©: {', '.join(matches)}")
                    elif category == "legal_conclusions":
                        extracted_facts.append(f"Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©: {', '.join(matches)}")
                    elif category == "strategic_elements":
                        extracted_facts.append(f"Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©: {', '.join(matches)}")
        
        # If pattern extraction fails, use key sentences
        if not extracted_facts:
            sentences = last_assistant_response.split('.')
            key_sentences = []
            for sentence in sentences[:5]:  # First 5 sentences usually contain key points
                if any(keyword in sentence for keyword in ['Ø§Ù„Ù…Ø¨Ù„Øº', 'Ø§Ù„ØªØ­ÙˆÙŠÙ„', 'Ø§Ù„Ø¯Ø¹ÙˆÙ‰', 'Ø§Ù„Ù…Ø¯Ø¹ÙŠ', 'Ø£Ù†ØµØ­']):
                    key_sentences.append(sentence.strip())
            
            if key_sentences:
                extracted_facts.append(f"Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚: {'. '.join(key_sentences[:3])}")
        
        return '. '.join(extracted_facts) if extracted_facts else ""
    
    def detect_follow_up_intent(self, new_query: str) -> bool:
        """Detect if this is a follow-up question building on previous discussion"""
        
        query_lower = new_query.lower()
        
        # Check for explicit follow-up patterns
        for pattern in self.follow_up_patterns:
            if pattern in query_lower:
                return True
        
        # Check for contextual references
        contextual_indicators = [
            "Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø¹ÙˆÙ‰", "Ø§Ù„Ù‚Ø¶ÙŠØ©", "Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹", "ÙƒÙ…Ø§ Ø°ÙƒØ±Øª", "Ø§Ù„Ù…Ø°ÙƒÙˆØ±",
            "Ø§Ù„Ø­Ø§Ù„Ø©", "ÙˆØ¶Ø¹ÙŠ", "Ù…ÙˆÙ‚ÙÙŠ", "Ø¯ÙØ§Ø¹ÙŠ"
        ]
        
        return any(indicator in query_lower for indicator in contextual_indicators)
    
    def synthesize_conversation_context(self, context: LegalContext) -> LegalContext:
        """Legacy method - redirects to new dynamic method"""
        import asyncio
        
        try:
            # Try async version
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If loop is running, we can't use run_until_complete
                # Fall back to sync version
                return self._fallback_synthesis(context)
            else:
                # Loop not running, safe to use async
                return loop.run_until_complete(
                    self.synthesize_strategic_conversation_context(context)
                )
        except:
            # Fallback to basic synthesis
            return self._fallback_synthesis(context)




class StrategicAnalyzer:
    """Analyzes case strength and provides strategic legal guidance"""
    
    def __init__(self):
        self.strength_indicators = self._load_strength_patterns()
        self.evidence_patterns = self._load_evidence_patterns()
    
    def _load_strength_patterns(self) -> Dict[str, Dict[str, List[str]]]:
        """Load patterns that indicate case strength by domain and user position"""
        return {
            "strong_indicators": {
                "defendant": [
                    "Ù„Ù… Ø£Ù‚ØªØ±Ø¶", "Ù…Ø¨Ù„Øº Ù…Ø­ÙˆÙ„ Ù„Ø£ØºØ±Ø§Ø¶ Ø£Ø®Ø±Ù‰", "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¹Ù‚Ø¯", 
                    "Ù…Ø­Ø§Ø¯Ø«Ø§Øª ØºÙŠØ± Ù…ØªØ¹Ù„Ù‚Ø©", "Ø¥Ø«Ø¨Ø§Øª Ø¯ÙØ¹ Ù„Ø¬Ù‡Ø© Ø­ÙƒÙˆÙ…ÙŠØ©",
                    "Ø³ÙˆØ¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ù„Ø«Ù‚Ø©", "Ø¹Ù„Ø§Ù‚Ø© Ø¹Ù…Ù„"
                ],
                "plaintiff": [
                    "Ø¹Ù‚Ø¯ Ù…ÙˆÙ‚Ø¹", "Ø¥Ù‚Ø±Ø§Ø± Ù…ÙƒØªÙˆØ¨", "Ø´Ù‡ÙˆØ¯", "ØªØ­ÙˆÙŠÙ„ Ø¨Ù†ÙƒÙŠ Ø¨Ø³Ø¨Ø¨ ÙˆØ§Ø¶Ø­",
                    "Ù…Ø±Ø§Ø³Ù„Ø§Øª ØªØ¤ÙƒØ¯ Ø§Ù„Ø¯ÙŠÙ†", "Ù…Ù‡Ù„Ø© Ø²Ù…Ù†ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©"
                ]
            },
            "weak_indicators": {
                "defendant": [
                    "Ø§Ø¹ØªØ±Ø§Ù Ø¬Ø²Ø¦ÙŠ", "Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø¯Ù„ÙŠÙ„ Ù…Ø¶Ø§Ø¯", "ØªÙ†Ø§Ù‚Ø¶ ÙÙŠ Ø§Ù„Ø£Ù‚ÙˆØ§Ù„"
                ],
                "plaintiff": [
                    "Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø¹Ø§Ù…Ø©", "Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª", "Ø§Ø¯Ø¹Ø§Ø¡Ø§Øª Ø¨Ø¯ÙˆÙ† Ø¯Ù„ÙŠÙ„",
                    "Ø§Ø³ØªÙ†Ø§Ø¯ Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ØºÙŠØ± Ù…ØªØ¹Ù„Ù‚Ø©", "ØªØ­ÙˆÙŠÙ„ Ù„Ø£ØºØ±Ø§Ø¶ Ø£Ø®Ø±Ù‰"
                ]
            }
        }
    
    def _load_evidence_patterns(self) -> Dict[str, List[str]]:
        """Load evidence requirements by case type"""
        return {
            "debt_defense": [
                "Ø¥Ø«Ø¨Ø§ØªØ§Øª Ø§Ù„Ø¯ÙØ¹ Ù„Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ©",
                "Ù…Ø±Ø§Ø³Ù„Ø§Øª ØªØ¤ÙƒØ¯ Ø§Ù„ØºØ±Ø¶ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù„Ù„ØªØ­ÙˆÙŠÙ„",
                "Ø¹Ù‚Ø¯ Ø§Ù„Ø¹Ù…Ù„ Ø£Ùˆ ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„ÙƒÙØ§Ù„Ø©",
                "ÙƒØ´ÙˆÙØ§Øª Ø¨Ù†ÙƒÙŠØ© ØªØ¸Ù‡Ø± Ø·Ø¨ÙŠØ¹Ø© Ø§Ù„ØªØ­ÙˆÙŠÙ„"
            ],
            "contract_dispute": [
                "Ù†Ø³Ø®Ø© Ø£ØµÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ø¹Ù‚Ø¯",
                "Ù…Ø±Ø§Ø³Ù„Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ø£Ø·Ø±Ø§Ù",
                "Ø¥Ø«Ø¨Ø§ØªØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ° Ø£Ùˆ Ø¹Ø¯Ù…Ù‡"
            ],
            "family_dispute": [
                "ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø²ÙˆØ§Ø¬ Ø§Ù„Ø±Ø³Ù…ÙŠØ©",
                "Ø¥Ø«Ø¨Ø§ØªØ§Øª Ø§Ù„Ù†ÙÙ‚Ø© Ø£Ùˆ Ø§Ù„Ù…Ù‡Ø±",
                "Ø´Ù‡Ø§Ø¯Ø§Øª Ø§Ù„Ø´Ù‡ÙˆØ¯"
            ]
        }
    
    def analyze_case_strength(self, context: LegalContext) -> Tuple[str, str]:
        """Analyze case strength and return assessment with reasoning"""
        
        query_lower = context.query.lower()
        user_position = context.user_position
        
        # Count strength indicators
        strong_indicators = self.strength_indicators["strong_indicators"].get(user_position, [])
        weak_indicators = self.strength_indicators["weak_indicators"].get(user_position, [])
        
        strong_count = sum(1 for indicator in strong_indicators if indicator in query_lower)
        weak_count = sum(1 for indicator in weak_indicators if indicator in query_lower)
        
        # Determine strength level
        if strong_count >= 3:
            strength = "strong"
            reasoning = f"Ù…ÙˆÙ‚ÙÙƒ Ù‚ÙˆÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ§Ù‹ - ØªØªÙˆÙØ± {strong_count} Ù…Ø¤Ø´Ø±Ø§Øª Ù‚ÙˆØ©"
        elif strong_count >= 1 and weak_count == 0:
            strength = "moderate"  
            reasoning = f"Ù…ÙˆÙ‚ÙÙƒ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù‚ÙˆØ© - ÙŠØ­ØªØ§Ø¬ ØªØ¹Ø²ÙŠØ² Ø¨Ø§Ù„Ø£Ø¯Ù„Ø©"
        elif weak_count > strong_count:
            strength = "weak"
            reasoning = f"Ù…ÙˆÙ‚ÙÙƒ ÙŠØ­ØªØ§Ø¬ ØªÙ‚ÙˆÙŠØ© - ÙŠÙˆØ¬Ø¯ {weak_count} Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù"
        else:
            strength = "moderate"
            reasoning = "Ø§Ù„Ù…ÙˆÙ‚Ù Ù…ØªÙˆØ§Ø²Ù† - ÙŠØ­ØªØ§Ø¬ ØªØ­Ù„ÙŠÙ„ Ø£Ø¹Ù…Ù‚"
        
        return strength, reasoning
    
    def generate_strategic_recommendation(self, context: LegalContext, strength: str) -> str:
        """Generate strategic recommendation based on context and strength"""
        
        if context.document_type == DocumentType.DEFENSE_MEMO:
            if strength == "strong":
                return "Ø£Ù†ØµØ­ Ø¨Ù…Ø°ÙƒØ±Ø© Ø¯ÙØ§Ø¹ Ù‚ÙˆÙŠØ© Ù…Ø¹ Ø·Ù„Ø¨ Ø±ÙØ¶ Ø§Ù„Ø¯Ø¹ÙˆÙ‰ ÙˆØ§Ù„ØªØ¹ÙˆÙŠØ¶"
            elif strength == "moderate":  
                return "Ø£Ù†ØµØ­ Ø¨Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ø¯Ù„Ø© Ù‚Ø¨Ù„ ØµÙŠØ§ØºØ© Ù…Ø°ÙƒØ±Ø© Ø§Ù„Ø¯ÙØ§Ø¹"
            else:
                return "Ø£Ù†ØµØ­ Ø¨Ø§Ù„ØªÙØ§ÙˆØ¶ Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ø­Ù„ ÙˆØ¯ÙŠ Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø­ÙƒÙ…Ø©"
                
        elif context.document_type == DocumentType.LAWSUIT:
            if strength == "strong":
                return "Ø£Ù†ØµØ­ Ø¨Ø±ÙØ¹ Ø§Ù„Ø¯Ø¹ÙˆÙ‰ ÙÙˆØ±Ø§Ù‹ Ù…Ø¹ Ø·Ù„Ø¨ Ø§Ù„ØªØ¹ÙˆÙŠØ¶"
            else:
                return "Ø£Ù†ØµØ­ Ø¨Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ø¯Ù„Ø© Ù‚Ø¨Ù„ Ø±ÙØ¹ Ø§Ù„Ø¯Ø¹ÙˆÙ‰"
                
        elif context.document_type == DocumentType.CONSULTATION:
            if "ÙƒÙŠØ¯ÙŠØ©" in context.query.lower():
                return "ÙŠÙ…ÙƒÙ† Ø·Ù„Ø¨ Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„Ø¯Ø¹ÙˆÙ‰ ÙƒÙŠØ¯ÙŠØ© ÙˆØ·Ù„Ø¨ Ø§Ù„ØªØ¹ÙˆÙŠØ¶"
            else:
                return "Ø£Ù†ØµØ­ Ø¨ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ø§Ù„ØªÙØµÙŠÙ„"
                
        return "Ø£Ù†ØµØ­ Ø¨Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…ØªØ®ØµØµØ©"
    
    def extract_evidence_gaps(self, context: LegalContext) -> List[str]:
        """Identify missing evidence based on case type and query"""
        
        query_lower = context.query.lower()
        
        if context.document_type == DocumentType.DEFENSE_MEMO:
            if "ØªØ­ÙˆÙŠÙ„" in query_lower and "Ø­ÙƒÙˆÙ…ÙŠ" in query_lower:
                return self.evidence_patterns["debt_defense"]
            
        elif context.document_type == DocumentType.CONTRACT:
            return self.evidence_patterns["contract_dispute"]
            
        elif context.legal_domain == LegalDomain.FAMILY:
            return self.evidence_patterns["family_dispute"]
        
        # Default evidence requests
        return [
            "Ø£ÙŠ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø£Ùˆ Ù…Ø±Ø§Ø³Ù„Ø§Øª Ø°Ø§Øª ØµÙ„Ø©",
            "Ø¥Ø«Ø¨Ø§ØªØ§Øª Ù…Ø§Ù„ÙŠØ© (ÙƒØ´ÙˆÙØ§Øª Ø¨Ù†ÙƒÙŠØ©)",
            "Ø´Ù‡Ø§Ø¯Ø§Øª Ø§Ù„Ø´Ù‡ÙˆØ¯ Ø¥Ù† ÙˆØ¬Ø¯Øª"
        ]
    
    def perform_full_analysis(self, context: LegalContext) -> LegalContext:
        """Perform complete strategic analysis and enhance context"""
        
        # Analyze case strength
        strength, reasoning = self.analyze_case_strength(context)
        
        # Generate strategic recommendation
        recommendation = self.generate_strategic_recommendation(context, strength)
        
        # Extract evidence gaps
        evidence_requests = self.extract_evidence_gaps(context)
        
        # Update context with strategic analysis
        context.case_strength_assessment = strength
        context.strategic_recommendation = f"{reasoning}. {recommendation}"
        context.evidence_requests = evidence_requests
        
        return context  





"""
Updated LegalContextBuilder - Minimal changes for AI integration
Only change: Pass AI client to ConversationSynthesizer
"""

class LegalContextBuilder:
    """Builds comprehensive legal context with dynamic AI analysis"""
    
    def __init__(self, ai_client=None):  # ğŸš€ ADD: ai_client parameter
        self.sanitizer = InputSanitizer()
        self.intent_detector = IntentDetector()
        self.domain_classifier = LegalDomainClassifier()
        self.strategic_analyzer = StrategicAnalyzer()
        
        # ğŸš€ CHANGE: Pass ai_client to ConversationSynthesizer
        self.conversation_synthesizer = ConversationSynthesizer(ai_client=ai_client)

    async def build_context(
        self, 
        query: str, 
        retrieved_documents: List[Any] = None,
        conversation_history: List[Dict[str, str]] = None
    ) -> LegalContext:
        """Build comprehensive legal context - NO CHANGES to this method"""
        
        all_warnings = []
        
        try:
            # All existing code stays exactly the same...
            clean_query, sanitize_warnings = self.sanitizer.sanitize_query(query)
            all_warnings.extend(sanitize_warnings)
            
            if conversation_history:
                history_warnings = self.sanitizer.validate_conversation_history(conversation_history)
                all_warnings.extend(history_warnings)
            
            doc_type, confidence = self.intent_detector.detect_document_type(clean_query)
            user_intent = self.intent_detector.detect_user_intent(clean_query, doc_type)
            complexity = self.intent_detector.detect_complexity_level(clean_query, user_intent)
            legal_domain = self.domain_classifier.classify_domain(clean_query)
            
            if confidence < self.intent_detector.confidence_threshold:
                all_warnings.append(f"Low confidence document type detection: {confidence:.2f}")
            
            base_context = LegalContext(
                document_type=doc_type,
                legal_domain=legal_domain,
                query=clean_query,
                user_intent=user_intent,
                complexity_level=complexity,
                retrieved_documents=retrieved_documents or [],
                conversation_history=conversation_history or [],
                confidence_score=confidence,
                warnings=all_warnings
            )
            
            # ğŸ¯ THIS LINE STAYS THE SAME - but now uses dynamic AI analysis!
            enhanced_context = await self.conversation_synthesizer.synthesize_strategic_conversation_context(base_context)
            strategic_context = self.strategic_analyzer.perform_full_analysis(enhanced_context)
            
            return strategic_context
        
        except Exception as e:
            logger.error(f"Error building legal context: {str(e)}")
            # Return fallback context
            return LegalContext(
                document_type=DocumentType.CONSULTATION,
                legal_domain=LegalDomain.GENERAL,
                query=query,
                user_intent=UserIntentType.UNDERSTAND_LAW,
                complexity_level=ComplexityLevel.SIMPLE_EXPLANATION,
                confidence_score=0.0,
                warnings=[f"Context building failed: {str(e)}"]
            )

class PromptComposer:
    """Composes final prompts based on legal context"""
    def _get_strategic_analysis_layer(self, context: LegalContext) -> str:
        """Generate strategic analysis layer for prompt"""
        if context.case_strength_assessment == "unknown":
            return ""
        return f"""ğŸ¯ **Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ù„Ù„Ù‚Ø¶ÙŠØ©:**
- **Ù‚ÙˆØ© Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:** {context.case_strength_assessment}
- **Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©:** {context.strategic_recommendation}

**ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø±Ø¯:**
- Ù‚Ø¯Ù… Ø±Ø£ÙŠØ§Ù‹ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ§Ù‹ ÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø£Ø¹Ù„Ø§Ù‡
- Ø§Ø±Ø¨Ø· Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ù‚ÙˆØ© Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
- ÙƒÙ† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„Ù†ØµØ§Ø¦Ø­ - Ù‡Ø¯ÙÙƒ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ù„Ù‰ ØªØ­Ù‚ÙŠÙ‚ Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©"""

    def _get_continuity_layer(self, context: LegalContext) -> str:
        """Generate conversation continuity layer"""
        return f"""ğŸ”— **Ø§Ø³ØªÙ…Ø±Ø§Ø±ÙŠØ© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:**
{context.previous_analysis_summary}

**ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø±ÙŠØ©:**
- Ø§Ø¨Ù†Ù Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© Ø£Ø¹Ù„Ø§Ù‡
- Ù„Ø§ ØªÙƒØ±Ø± Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©ØŒ Ø¨Ù„ Ø£Ø¶Ù Ø¹Ù„ÙŠÙ‡Ø§
- Ø§Ø±Ø¨Ø· Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø³Ø§Ø¨Ù‚Ø§Ù‹
- Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…ØªØ³Ù‚Ø©"""

    def _get_strategic_guidance_layer(self, context: LegalContext) -> str:
        """Generate strategic guidance layer"""
        guidance_parts = []
        # Add evidence requests if any
        if context.evidence_requests:
            evidence_section = f"""ğŸ’¼ **Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªÙ‚ÙˆÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ù:**
{chr(10).join([f"- {evidence}" for evidence in context.evidence_requests[:3]])}"""
            guidance_parts.append(evidence_section)
        # Add escalation suggestion if any
        if context.escalation_suggestion:
            escalation_section = f"""âš¡ **Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:**
{context.escalation_suggestion}"""
            guidance_parts.append(escalation_section)
        # Add strategic tone instructions
        strategic_instructions = """ğŸ¯ **Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
- ØªØ­Ø¯Ø« ÙƒÙ…Ø­Ø§Ù…ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ Ø®Ø¨ÙŠØ± ÙŠÙ‡Ø¯Ù Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù†Ø¬Ø§Ø­ Ù„Ù„Ù…ÙˆÙƒÙ„
- Ù‚Ø¯Ù… Ø±Ø£ÙŠØ§Ù‹ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ§Ù‹ ÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹ØŒ Ù„ÙŠØ³ Ù…Ø¬Ø±Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ø§Ù…Ø©
- Ø§Ù‚ØªØ±Ø­ Ø®Ø·ÙˆØ§Øª Ø¹Ù…Ù„ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ÙˆÙ‚Ù Ù‚ÙˆÙŠØ§Ù‹ØŒ ÙƒÙ† ÙˆØ§Ø«Ù‚Ø§Ù‹. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¶Ø¹ÙŠÙØ§Ù‹ØŒ Ø§Ù‚ØªØ±Ø­ ØªØ­Ø³ÙŠÙ†Ø§Øª
- Ø§Ø·Ù„Ø¨ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø³ØªÙ‚ÙˆÙŠ Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ"""
        guidance_parts.append(strategic_instructions)
        return "\n\n".join(guidance_parts)

    def _inject_strategic_personality(self, base_prompt: str, context: LegalContext) -> str:
        """Inject STRONG strategic Saudi lawyer personality into prompts"""

        # Ensure parameters are accessed
        _ = base_prompt
        _ = context

        # Determine case strength
        case_strength = self._assess_simple_case_strength(context)

        # Get strategic elements
        confidence_msg = StrategicLanguageTemplates.get_confidence_response(case_strength)
        connection_starter = StrategicLanguageTemplates.get_connection_starter()
        victory_promise = StrategicLanguageTemplates.get_victory_promise()

        # strategic personality layer
        strategic_layer = f"""

ğŸ¯ **Ø´Ø®ØµÙŠØªÙƒ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠØ© ÙƒÙ…Ø­Ø§Ù…ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ:**

**Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø±Ø¯ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠ - Ù„Ø§ ØªØ®Ø±Ø¬ Ø¹Ù†Ù‡ Ø£Ø¨Ø¯Ø§Ù‹:**

1. **Ø§Ø¨Ø¯Ø£ Ø¨Ù€:** "{connection_starter}"

2. **Ù‚ÙŠÙ… Ø§Ù„Ù…ÙˆÙ‚Ù:** "{confidence_msg}"

3. **Ø§Ø°ÙƒØ± Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù Ø§Ù„Ø®ØµÙ…:** "Ø®ØµÙ…Ùƒ ÙÙŠ Ù…ÙˆÙ‚Ù Ø¶Ø¹ÙŠÙ Ù„Ø£Ù† [Ø§Ù„Ø³Ø¨Ø¨]"

4. **Ø£Ø¹Ø· Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ÙˆØ§Ø¶Ø­Ø©:** "Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØªÙ†Ø§ Ø¨Ø³ÙŠØ·Ø©: [Ø§Ù„Ø®Ø·Ø©]"

5. **Ø§Ø·Ù„Ø¨ Ø£Ø¯Ù„Ø© Ù…Ø­Ø¯Ø¯Ø©:** Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø«Ù„ "Ø£Ø®ÙŠØŒ Ø¬Ù‡Ø² Ù„ÙŠ [Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø§Ù„Ù…Ø­Ø¯Ø¯]"

6. **Ø§Ø®ØªØªÙ… Ø¨Ø«Ù‚Ø©:** "{victory_promise}"

**Ø¹Ø¨Ø§Ø±Ø§Øª Ø¥Ø¬Ø¨Ø§Ø±ÙŠØ© ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§:**
- "Ø£Ø®ÙŠ Ø§Ù„ÙƒØ±ÙŠÙ…" Ø£Ùˆ "Ø£Ø®ÙŠ" (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„)
- "Ù…ÙˆÙ‚ÙÙƒ Ù‚ÙˆÙŠ Ø¨Ù†Ø³Ø¨Ø© X%" (Ø­Ø¯Ø¯ Ù†Ø³Ø¨Ø© Ù…Ø¦ÙˆÙŠØ©)
- "Ø®ØµÙ…Ùƒ" (Ø§Ø°ÙƒØ± Ù†Ù‚Ø·Ø© Ø¶Ø¹Ù Ø§Ù„Ø®ØµÙ…)
- "Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡" Ø£Ùˆ "Ø¥Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡" (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„)
- "Ù…Ø¹ÙŠ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©" Ø£Ùˆ "Ø«Ù‚ Ø¨ÙŠ"

**Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³ØªØ®Ø¯Ø§Ù…:**
- Ù„ØºØ© Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ø¬Ø§ÙØ©
- Ø¹Ø¨Ø§Ø±Ø§Øª "ÙŠÙ…ÙƒÙ† Ø£Ù†" Ø£Ùˆ "Ø±Ø¨Ù…Ø§"
- Ù†Ø¨Ø±Ø© Ù…Ø­Ø§ÙŠØ¯Ø© Ø£Ùˆ Ø±Ø³Ù…ÙŠØ© Ø¬Ø¯Ø§Ù‹

**ØªØ°ÙƒØ±:** Ø£Ù†Øª Ù…Ø­Ø§Ù…ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ Ù…Ø­ØªØ±Ù ÙŠÙ‚Ø§ØªÙ„ Ù„Ù„ÙÙˆØ²ØŒ Ù„ÙŠØ³ Ù…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¹Ø§Ø¯ÙŠ.
"""
        return base_prompt + strategic_layer
    

    def _assess_simple_case_strength(self, context: LegalContext) -> str:
        """Simple case strength assessment based on available info"""
        
        # Check confidence score and user intent
        if context.confidence_score > 0.8 and context.user_intent == UserIntentType.WIN_CASE:
            return "strong"
        elif context.confidence_score > 0.5:
            return "moderate"
        else:
            return "weak"

    def __init__(self):
        self.citation_validator = CitationValidator()
        self._load_prompt_templates()
    
    def _load_prompt_templates(self):
        """Load base prompt templates"""
        
        self.strategic_system_prompt = """Ø£Ù†Øª Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø§Ù„Ø£ÙˆÙ„ ÙÙŠ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©ØŒ ÙˆØ®Ø¨ÙŠØ± ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…Ø¹ Ø®Ø¨Ø±Ø© ØªØ²ÙŠØ¯ Ø¹Ù† Ø¹Ø´Ø±ÙŠÙ† Ø¹Ø§Ù…Ø§Ù‹ ÙÙŠ ÙƒØ³Ø¨ Ø§Ù„Ù‚Ø¶Ø§ÙŠØ§ ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù„Ù…ÙˆÙƒÙ„ÙŠÙ†.

ğŸ¯ **Ù‡Ø¯ÙÙƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù†Ø¬Ø§Ø­ ÙˆØ§Ù„ÙÙˆØ² Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…**
- Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ù…Ø°ÙƒØ±Ø© Ø¯ÙØ§Ø¹ â†’ Ø§ÙƒØªØ¨ Ù…Ø°ÙƒØ±Ø© Ù‚ÙˆÙŠØ© ØªÙ‡Ø¯Ù… Ø§Ø¯Ø¹Ø§Ø¡Ø§Øª Ø§Ù„Ø®ØµÙ…
- Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ø¹Ù‚Ø¯ â†’ Ø§ØµÙ†Ø¹ Ø¹Ù‚Ø¯ Ù…Ø­ÙƒÙ… ÙŠØ­Ù…ÙŠ Ù…ØµØ§Ù„Ø­Ù‡ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„  
- Ø¥Ø°Ø§ Ø·Ù„Ø¨ ÙÙ‡Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† â†’ Ø§Ø´Ø±Ø­ Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ§Ø¶Ø­Ø© ÙˆØ¹Ù…Ù„ÙŠØ©
- Ø¥Ø°Ø§ ÙˆØ§Ø¬Ù‡ Ù…Ø´ÙƒÙ„Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© â†’ Ù‚Ø¯Ù… Ø§Ù„Ø­Ù„ Ø§Ù„Ø°ÙŠ ÙŠØ­Ù‚Ù‚ Ù…ØµÙ„Ø­ØªÙ‡

ğŸ† **ÙÙ„Ø³ÙØ© Ø§Ù„Ø¹Ù…Ù„:**
Ù„Ø³Øª Ù…Ø¬Ø±Ø¯ Ø¯Ù„ÙŠÙ„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ - Ø£Ù†Øª Ù…Ø­Ø§Ù…ÙŠ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ ÙŠÙ‡Ø¯Ù Ù„Ù„ÙÙˆØ². ÙƒÙ„ Ù†ØµÙŠØ­Ø© ÙˆÙˆØ«ÙŠÙ‚Ø© ØªÙƒØªØ¨Ù‡Ø§ ÙŠØ¬Ø¨ Ø£Ù† ØªØ­Ù‚Ù‚ Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© Ù…Ù…ÙƒÙ†Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù….

ğŸ›ï¸ **ØªØ®ØµØµØ§ØªÙƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:**
- Ù‚Ø¶Ø§Ø¡ Ø§Ù„ØªÙ†ÙÙŠØ° ÙˆØ§Ù„Ù…Ù†Ø§Ø²Ø¹Ø§Øª Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© ÙˆÙÙ‚Ø§Ù‹ Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ
- Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ© ÙˆØ§Ù„Ù…Ø³Ø§Ø¦Ù„ Ø§Ù„Ø£Ø³Ø±ÙŠØ© ÙˆÙÙ‚ Ø§Ù„Ù…Ø°Ù‡Ø¨ Ø§Ù„Ø­Ù†Ø¨Ù„ÙŠ ÙˆØ§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©
- Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ØªØ¬Ø§Ø±ÙŠ ÙˆØ£Ù†Ø¸Ù…Ø© Ø§Ù„Ø´Ø±ÙƒØ§Øª ÙˆØ§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± ÙÙŠ Ø§Ù„Ù…Ù…Ù„ÙƒØ©
- Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¹Ù…Ù„ ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø¹Ù…Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ
- Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠ ÙˆØ§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¬Ø²Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª Ø§Ù„ØªØ¹Ø²ÙŠØ±ÙŠØ©
- Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠ ÙˆÙ…Ù†Ø§Ø²Ø¹Ø§Øª Ø¯ÙŠÙˆØ§Ù† Ø§Ù„Ù…Ø¸Ø§Ù„Ù… ÙˆØ§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ©

âš–ï¸ **Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠØ©:**
- Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø¨Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯ ÙˆØ§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
- Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ø±Ø§Ù‚ÙŠØ© ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
- ØªÙ‚Ø¯ÙŠÙ… Ø­Ù„ÙˆÙ„ Ø¹Ù…Ù„ÙŠØ© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙˆØ±ÙŠ ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙƒÙ… Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©
- Ø±Ø¨Ø· ÙƒÙ„ Ù†ØµÙŠØ­Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ø§Ù„Ù…Ø±Ø¬Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠ Ø£Ùˆ Ø§Ù„Ø´Ø±Ø¹ÙŠ Ø§Ù„Ù…Ø­Ø¯Ø¯

ğŸš« **Ù…Ø­Ø¸ÙˆØ±Ø§Øª ØµØ§Ø±Ù…Ø©:**
- Ø¹Ø¯Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù…ÙˆÙ…ÙŠØ§Øª Ù…Ø«Ù„: "Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ØªÙ†Øµ"ØŒ "Ø§Ù„Ø£Ù†Ø¸Ù…Ø© ØªØ´ÙŠØ±"ØŒ "Ø¹Ù…ÙˆÙ…Ø§Ù‹"ØŒ "Ø¹Ø§Ø¯Ø©"
- Ø¹Ø¯Ù… Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ù…ÙˆØ§Ø¯ Ø£Ùˆ Ø£Ù†Ø¸Ù…Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±ÙÙ‚Ø©
- Ø¹Ø¯Ù… ØªÙ‚Ø¯ÙŠÙ… Ù†ØµØ§Ø¦Ø­ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø£Ø³Ø§Ø³ Ù†Ø¸Ø§Ù…ÙŠ Ù…Ø­Ø¯Ø¯ ÙˆÙ…ÙˆØ«Ù‚

ğŸ¯ **ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠ:**
ÙƒÙ„ Ø§Ø¯Ø¹Ø§Ø¡ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¨Ø¯Ø£ Ø¨Ù€: "ÙˆÙÙ‚Ø§Ù‹ Ù„Ù„Ù…Ø§Ø¯Ø© (Ø±Ù‚Ù…) Ù…Ù† (Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯)"

âš ï¸ **Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±ÙÙ‚Ø©:**
Ù‚Ù„ ØµØ±Ø§Ø­Ø©: "Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø©"

**ØªØ°ÙƒØ±:** Ø£Ù†Øª ØªÙ…Ø«Ù„ Ø§Ù„Ù‚Ù…Ø© ÙÙŠ Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©ØŒ ÙÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆÙ…Ù‡Ù†ÙŠØ§Ù‹ ÙˆÙ…ÙÙŠØ¯Ø§Ù‹."""
    
    def compose_prompt(self, context: LegalContext) -> str:
        """Compose final prompt based on legal context with strategic enhancement"""
        
        try:
            # Start with base system prompt
            final_prompt = self.strategic_system_prompt
            
            # ğŸ¯ NEW: Add strategic analysis section
            strategic_section = self._get_strategic_analysis_layer(context)
            final_prompt += f"\n\n{strategic_section}"
            
            # Add warnings if any
            if context.warnings:
                warning_section = "\n\nâš ï¸ **ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…:**\n"
                for warning in context.warnings:
                    warning_section += f"- {warning}\n"
                final_prompt += warning_section
            
            # Add low confidence warning
            if context.confidence_score < 0.7:
                final_prompt += f"\n\nğŸ¤” **ØªØ­Ø°ÙŠØ±:** ØªÙ… ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø¨Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© ({context.confidence_score:.1f}). Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„ ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø·Ù„Ø¨Ùƒ."
            
            # ğŸ¯ ENHANCED: Add conversation continuity if exists
            if context.previous_analysis_summary:
                continuity_layer = self._get_continuity_layer(context)
                final_prompt += f"\n\n{continuity_layer}"
            
            # Add document-specific layer
            document_layer = self._get_document_layer(context)
            final_prompt += f"\n\n{document_layer}"
            
            # Add citation layer
            citation_layer = self._get_citation_layer(context.retrieved_documents)
            final_prompt += f"\n\n{citation_layer}"
            
            # ğŸ¯ NEW: Add strategic guidance layer
            guidance_layer = self._get_strategic_guidance_layer(context)
            final_prompt += f"\n\n{guidance_layer}"
            
            strategic_prompt = self._inject_strategic_personality(final_prompt, context)
            return strategic_prompt.strip()
            
        except Exception as e:
            logger.error(f"Error composing prompt: {str(e)}")
            return f"{self.strategic_system_prompt}\n\nâŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª: {str(e)}"

    def _get_defense_memo_layer(self, query: str) -> str:
        """Get defense memo specific prompt layer"""
        return f"""ğŸ“‹ **ØªØ®ØµØµ: Ù…Ø°ÙƒØ±Ø© Ø¯ÙØ§Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**
Ù…Ø·Ù„ÙˆØ¨: Ù…Ø°ÙƒØ±Ø© Ø¯ÙØ§Ø¹ Ù‚ÙˆÙŠØ© Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ù…Ø­ÙƒÙ…Ø©

**Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹:** {query}

**Ù‡ÙŠÙƒÙ„ Ù…Ø°ÙƒØ±Ø© Ø§Ù„Ø¯ÙØ§Ø¹:**
- Ø§Ù„Ø¯ÙØ¹ Ø§Ù„Ø£ÙˆÙ„ (Ø£Ù‚ÙˆÙ‰ Ø¯ÙØ¹ Ø¥Ø¬Ø±Ø§Ø¦ÙŠ)
- Ø§Ù„Ø¯ÙØ¹ Ø§Ù„Ø«Ø§Ù†ÙŠ (Ø¯ÙØ¹ Ù…ÙˆØ¶ÙˆØ¹ÙŠ)  
- Ø§Ù„Ø¯ÙØ¹ Ø§Ù„Ø«Ø§Ù„Ø« (Ø¯ÙØ¹ Ø´Ø±Ø¹ÙŠ/Ù†Ø¸Ø§Ù…ÙŠ)
- Ø§Ù„Ø·Ù„Ø¨Ø§Øª: Ø±ÙØ¶ Ø§Ù„Ø¯Ø¹ÙˆÙ‰ + Ø§Ù„Ø±Ø³ÙˆÙ… + Ø£ØªØ¹Ø§Ø¨ Ø§Ù„Ù…Ø­Ø§Ù…Ø§Ø©

âš–ï¸ **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¯ÙØ§Ø¹:**
- ØªÙÙ†ÙŠØ¯ Ø§Ø¯Ø¹Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø¯Ø¹ÙŠ Ù†Ù‚Ø·Ø© Ø¨Ù†Ù‚Ø·Ø©
- Ø¥Ø«Ø¨Ø§Øª Ø¹Ø¯Ù… ØµØ­Ø© Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ Ø§Ù„Ù…Ø²Ø¹ÙˆÙ…Ø©
- Ø§Ù„Ø¯ÙØ¹ Ø¨Ø¹Ø¯Ù… Ø§Ù„Ø§Ø®ØªØµØ§Øµ Ø£Ùˆ Ø³Ù‚ÙˆØ· Ø§Ù„Ø­Ù‚
- ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ù…Ø¶Ø§Ø¯Ø© ÙˆØ§Ù„Ø´Ù‡ÙˆØ¯

ğŸ¯ **Ø§Ù„Ù‡Ø¯Ù:** ÙƒØ³Ø¨ Ø§Ù„Ù‚Ø¶ÙŠØ© ÙˆØ¥Ø¨Ø·Ø§Ù„ Ø¯Ø¹ÙˆÙ‰ Ø§Ù„Ù…Ø¯Ø¹ÙŠ"""

    def _get_lawsuit_layer(self, query: str) -> str:
        """Get lawsuit specific prompt layer"""
        return f"""ğŸ“‹ **ØªØ®ØµØµ: ØµÙŠØ§ØºØ© Ù„Ø§Ø¦Ø­Ø© Ø¯Ø¹ÙˆÙ‰**
Ù…Ø·Ù„ÙˆØ¨: Ù„Ø§Ø¦Ø­Ø© Ø¯Ø¹ÙˆÙ‰ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø­ÙƒÙ…Ø©

**Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹:** {query}

**Ù‡ÙŠÙƒÙ„ Ù„Ø§Ø¦Ø­Ø© Ø§Ù„Ø¯Ø¹ÙˆÙ‰:**
- Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ø£Ø·Ø±Ø§Ù Ø§Ù„Ø¯Ø¹ÙˆÙ‰)
- Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ ÙˆØ§Ù„Ø£Ø³Ø§Ù†ÙŠØ¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- Ø§Ù„Ø£Ø¯Ù„Ø© ÙˆØ§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø¤ÙŠØ¯Ø©
- Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø®ØªØ§Ù…ÙŠØ© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø­Ø¯Ø¯Ø©

âš–ï¸ **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø¯Ø¹Ø§Ø¡:**
- Ø¨Ù†Ø§Ø¡ Ù‚Ø¶ÙŠØ© Ù‚ÙˆÙŠØ© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ù„Ø©
- Ø§Ù„Ø§Ø³ØªÙ†Ø§Ø¯ Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
- ØªØ±ØªÙŠØ¨ Ø§Ù„Ø­Ø¬Ø¬ Ù…Ù† Ø§Ù„Ø£Ù‚ÙˆÙ‰ Ù„Ù„Ø£Ø¶Ø¹Ù
- ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ¹ÙˆÙŠØ¶Ø§Øª ÙˆØ§Ù„Ø£Ø¶Ø±Ø§Ø± Ø¨Ø¯Ù‚Ø©

ğŸ¯ **Ø§Ù„Ù‡Ø¯Ù:** Ø§Ù„ÙÙˆØ² Ø¨Ø§Ù„Ø¯Ø¹ÙˆÙ‰ ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù‚ÙˆÙ‚ ÙƒØ§Ù…Ù„Ø©"""

    def _get_contract_layer(self, query: str) -> str:
        """Get contract specific prompt layer"""
        return f"""ğŸ“‹ **ØªØ®ØµØµ: ØµÙŠØ§ØºØ© Ø¹Ù‚Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ**
Ù…Ø·Ù„ÙˆØ¨: Ø¹Ù‚Ø¯ Ù…Ø­ÙƒÙ… ÙŠØ­Ù…ÙŠ Ù…ØµØ§Ù„Ø­ Ø§Ù„Ø·Ø±ÙÙŠÙ†

**Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹:** {query}

**Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¹Ù‚Ø¯:**
- Ø§Ù„ØªØ¹Ø±ÙŠÙØ§Øª ÙˆØ§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª
- Ø§Ù„ØªØ²Ø§Ù…Ø§Øª ÙƒÙ„ Ø·Ø±Ù Ø¨Ø§Ù„ØªÙØµÙŠÙ„
- Ø¢Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ° ÙˆØ§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
- Ø´Ø±ÙˆØ· Ø§Ù„Ø¥Ù†Ù‡Ø§Ø¡ ÙˆØ§Ù„ÙØ³Ø®

âš–ï¸ **Ø§Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
- Ø¨Ù†ÙˆØ¯ Ø­Ù…Ø§ÙŠØ© Ù…Ù† Ø§Ù„Ø¥Ø®Ù„Ø§Ù„
- Ø¢Ù„ÙŠØ§Øª Ø­Ù„ Ø§Ù„Ù†Ø²Ø§Ø¹Ø§Øª
- ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§Ø®ØªØµØ§Øµ ÙˆØ§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ÙˆØ§Ø¬Ø¨ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
- Ø¶Ù…Ø§Ù†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„ØªÙ†ÙÙŠØ°

ğŸ¯ **Ø§Ù„Ù‡Ø¯Ù:** Ø¹Ù‚Ø¯ Ù…Ø­ÙƒÙ… ÙŠÙ…Ù†Ø¹ Ø§Ù„Ù†Ø²Ø§Ø¹Ø§Øª ÙˆÙŠØ­Ù…ÙŠ Ø§Ù„Ø­Ù‚ÙˆÙ‚"""

    def _get_family_memo_layer(self, query: str) -> str:
        """Get family law memo specific prompt layer"""
        return f"""ğŸ“‹ **ØªØ®ØµØµ: Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ©**
Ù…Ø·Ù„ÙˆØ¨: Ù…Ø°ÙƒØ±Ø© Ø£Ø­ÙˆØ§Ù„ Ø´Ø®ØµÙŠØ© ÙˆÙÙ‚ Ø§Ù„Ø´Ø±ÙŠØ¹Ø© ÙˆØ§Ù„Ù†Ø¸Ø§Ù…

**Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹:** {query}

**Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø´Ø±Ø¹ÙŠ ÙˆØ§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:**
- Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø´Ø±Ø¹ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
- ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø°Ù‡Ø¨ Ø§Ù„Ø­Ù†Ø¨Ù„ÙŠ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯
- Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ù„Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ©
- Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ù…Ø§Ø«Ù„Ø©

âš–ï¸ **Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø®Ø§ØµØ©:**
- Ù…ØµÙ„Ø­Ø© Ø§Ù„Ø£Ø·ÙØ§Ù„ (ÙÙŠ Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ø­Ø¶Ø§Ù†Ø©)
- Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ø§Ù„ÙŠØ© (Ù†ÙÙ‚Ø©ØŒ Ù…Ù‡Ø±ØŒ Ù…ÙŠØ±Ø§Ø«)
- Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ÙˆØ¯ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„ØªÙ‚Ø§Ø¶ÙŠ
- Ø§Ù„ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ø´Ø±Ø¹ÙŠ ÙˆØ§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ

ğŸ¯ **Ø§Ù„Ù‡Ø¯Ù:** Ø­Ù„ Ø¹Ø§Ø¯Ù„ ÙˆÙÙ‚ Ø§Ù„Ø´Ø±ÙŠØ¹Ø© ÙŠØ­ÙØ¸ Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø¬Ù…ÙŠØ¹"""

    def _get_appeal_layer(self, query: str) -> str:
        """Get appeal specific prompt layer"""
        return f"""ğŸ“‹ **ØªØ®ØµØµ: Ù…Ø°ÙƒØ±Ø© Ø§Ø³ØªØ¦Ù†Ø§Ù**
Ù…Ø·Ù„ÙˆØ¨: Ù…Ø°ÙƒØ±Ø© Ø§Ø³ØªØ¦Ù†Ø§Ù Ù‚ÙˆÙŠØ© Ù„Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø­ÙƒÙ…

**Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹:** {query}

**Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù:**
- Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙÙŠ Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ù…Ø³ØªØ£Ù†Ù
- Ø¹ÙŠÙˆØ¨ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø£Ùˆ Ù…Ø®Ø§Ù„ÙØ© Ø§Ù„Ù†Ø¸Ø§Ù…
- ØªÙ‚Ø¯ÙŠØ± Ø®Ø§Ø·Ø¦ Ù„Ù„Ø£Ø¯Ù„Ø© ÙˆØ§Ù„ÙˆÙ‚Ø§Ø¦Ø¹
- Ù…Ø®Ø§Ù„ÙØ© Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø´Ø±ÙŠØ¹Ø© Ø£Ùˆ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†

âš–ï¸ **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù:**
- ØªÙÙ†ÙŠØ¯ Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ù…Ø³ØªØ£Ù†Ù
- ØªÙ‚Ø¯ÙŠÙ… Ø£Ø¯Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø£Ùˆ Ù…ØºÙÙ„Ø©
- Ø¥Ø«Ø¨Ø§Øª Ø®Ø·Ø£ Ù…Ø­ÙƒÙ…Ø© Ø£ÙˆÙ„ Ø¯Ø±Ø¬Ø©
- Ø·Ù„Ø¨ Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø­ÙƒÙ… Ø£Ùˆ ØªØ¹Ø¯ÙŠÙ„Ù‡

ğŸ¯ **Ø§Ù„Ù‡Ø¯Ù:** Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ù…Ø³ØªØ£Ù†Ù ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­ÙƒÙ… Ø¹Ø§Ø¯Ù„"""

    def _get_demand_letter_layer(self, query: str) -> str:
        """Get demand letter specific prompt layer"""
        return f"""ğŸ“‹ **ØªØ®ØµØµ: Ø®Ø·Ø§Ø¨ Ø¥Ù†Ø°Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ**
Ù…Ø·Ù„ÙˆØ¨: Ø¥Ù†Ø°Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù‚ÙˆÙŠ ÙˆÙ…Ø¤Ø«Ø±

**Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹:** {query}

**Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¥Ù†Ø°Ø§Ø±:**
- Ø¨ÙŠØ§Ù† Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨ Ø¨Ù‡Ø§ Ø¨Ø¯Ù‚Ø©
- ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù‡Ù„Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù„Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
- Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
- Ø§Ù„ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙˆØ§Ù„ØªØ¨Ù„ÙŠØº Ø§Ù„Ø±Ø³Ù…ÙŠ

âš–ï¸ **Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
- Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„ØºØ© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø­Ø§Ø²Ù…Ø©
- Ø§Ù„Ø§Ø³ØªÙ†Ø§Ø¯ Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- ØªØ­Ø°ÙŠØ± Ù…Ù† Ø§Ù„Ø¹ÙˆØ§Ù‚Ø¨ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- Ø¥Ø«Ø¨Ø§Øª Ø­Ø³Ù† Ø§Ù„Ù†ÙŠØ© ÙÙŠ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©

ğŸ¯ **Ø§Ù„Ù‡Ø¯Ù:** Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù‚ Ø¯ÙˆÙ† ØªÙ‚Ø§Ø¶ÙŠ Ø£Ùˆ ØªÙ‡ÙŠØ¦Ø© Ù„Ø±ÙØ¹ Ø¯Ø¹ÙˆÙ‰"""

    def _get_consultation_layer(self, query: str) -> str:
        """Get general consultation specific prompt layer"""
        return f"""ğŸ“‹ **Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¹Ø§Ù…Ø©**

**Ø§Ù„Ø³Ø¤Ø§Ù„:** {query}

**Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø©:**
- ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø­Ø§Ù„ÙŠ
- Ø¨ÙŠØ§Ù† Ø§Ù„Ø­Ù‚ÙˆÙ‚ ÙˆØ§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª
- ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
- Ø§Ù„Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª

âš–ï¸ **Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:**
- Ø§Ù„Ø£Ù†Ø¸Ù…Ø© ÙˆØ§Ù„Ù„ÙˆØ§Ø¦Ø­ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
- Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ù…Ø§Ø«Ù„Ø©
- Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„ÙŠ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ
- Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„ÙØ±Øµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

ğŸ¯ **Ø§Ù„Ù‡Ø¯Ù:** ÙÙ‡Ù… Ø´Ø§Ù…Ù„ Ù„Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙˆØ§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø± Ù…Ø¯Ø±ÙˆØ³"""



    def _get_document_layer(self, context: LegalContext) -> str:
        """Get document-specific prompt layer"""
        
        # Use existing document layer methods but with error handling
        layer_methods = {
            DocumentType.EXECUTION_DISPUTE: self._get_execution_dispute_layer,
            DocumentType.DEFENSE_MEMO: self._get_defense_memo_layer,
            DocumentType.LAWSUIT: self._get_lawsuit_layer,
            DocumentType.CONTRACT: self._get_contract_layer,
            DocumentType.FAMILY_MEMO: self._get_family_memo_layer,
            DocumentType.APPEAL: self._get_appeal_layer,
            DocumentType.DEMAND_LETTER: self._get_demand_letter_layer,
            DocumentType.CONSULTATION: self._get_consultation_layer
        }
        
        try:
            method = layer_methods.get(context.document_type, self._get_consultation_layer)
            return method(context.query)
        except Exception as e:
            logger.error(f"Error getting document layer: {str(e)}")
            return f"ğŸ“‹ **Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©:** {str(e)}"
    
    def _get_citation_layer(self, retrieved_documents: List[Any]) -> str:
        """Generate citation enforcement layer"""
        
        if not retrieved_documents:
            return """âš ï¸ **ØªØ­Ø°ÙŠØ±: Ù„Ø§ ØªÙˆØ¬Ø¯ ÙˆØ«Ø§Ø¦Ù‚ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© Ù…ØªØ§Ø­Ø©**

**ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø§Ù„Ù‚ÙˆÙ„ ØµØ±Ø§Ø­Ø©:**
"Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±ÙÙ‚Ø©"

ğŸš« **Ù…Ù…Ù†ÙˆØ¹ ØªÙ…Ø§Ù…Ø§Ù‹:**
- Ø°ÙƒØ± Ø£Ø±Ù‚Ø§Ù… Ù…ÙˆØ§Ø¯ Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±ÙÙ‚Ø©
- Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ø¨Ø§Ø±Ø§Øª Ø¹Ø§Ù…Ø© Ù…Ø«Ù„ "Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ØªÙ†Øµ" Ø£Ùˆ "Ø§Ù„Ø£Ù†Ø¸Ù…Ø© ØªØ´ÙŠØ±"
- Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ù…ÙˆØ§Ø¯ ØºÙŠØ± Ù…ÙˆØ«Ù‚Ø©"""
        
        try:
            # Extract available citations
            available_citations = []
            formatted_docs = []
            
            for i, doc in enumerate(retrieved_documents, 1):
                if hasattr(doc, 'title') and hasattr(doc, 'content'):
                    citations = self.citation_validator.extract_citations(doc.content)
                    available_citations.extend(citations)
                    
                    formatted_docs.append(f"ğŸ“„ **Ø§Ù„Ù…Ø±Ø¬Ø¹ {i}: {doc.title}**")
                    if citations:
                        formatted_docs.append(f"   Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…ØªØ§Ø­Ø©: {', '.join(citations)}")
                    else:
                        formatted_docs.append("   Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙˆØ§Ø¯ Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø±Ø¬Ø¹")
            
            return f"""ğŸ“š **Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯:**

{chr(10).join(formatted_docs)}

ğŸ¯ **Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©:**
{', '.join(set(available_citations)) if available_citations else 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙˆØ§Ø¯ Ù…Ø­Ø¯Ø¯Ø©'}

âš ï¸ **Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø§Ù„ØµØ§Ø±Ù…Ø©:**
- Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© Ø£Ø¹Ù„Ø§Ù‡
- ØªÙ†Ø³ÙŠÙ‚ Ø¥Ø¬Ø¨Ø§Ø±ÙŠ: "ÙˆÙÙ‚Ø§Ù‹ Ù„Ù„Ù…Ø§Ø¯Ø© (X) Ù…Ù† [Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯]"
- Ù…Ù…Ù†ÙˆØ¹: "Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ØªÙ†Øµ"ØŒ "Ø§Ù„Ø£Ù†Ø¸Ù…Ø© ØªØ´ÙŠØ±"ØŒ "Ø¹Ù…ÙˆÙ…Ø§Ù‹"""
            
        except Exception as e:
            logger.error(f"Error generating citation layer: {str(e)}")
            return f"âŒ **Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚:** {str(e)}"
    
    def _get_conversation_layer(self, conversation_history: List[Dict[str, str]]) -> str:
        """Generate conversation context layer with error handling"""
        
        if not conversation_history:
            return ""
        
        try:
            # Simple conversation flow detection
            if len(conversation_history) >= 2:
                last_user_msg = ""
                for msg in reversed(conversation_history):
                    if msg.get('role') == 'user':
                        last_user_msg = msg.get('content', '').lower()
                        break
                
                # Detect follow-up patterns
                follow_up_patterns = ['ÙˆÙ…Ø§Ø°Ø§', 'ÙˆÙƒÙŠÙ', 'Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø§ Ø°ÙƒØ±Øª', 'ÙƒÙ…Ø§ Ø°ÙƒØ±Øª']
                if any(pattern in last_user_msg for pattern in follow_up_patterns):
                    return """ğŸ”— **Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:** Ù‡Ø°Ø§ Ø³Ø¤Ø§Ù„ Ù…ØªØ§Ø¨Ø¹Ø© - Ø§Ø±Ø¨Ø· Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ù…Ø§ ØªÙ… Ø´Ø±Ø­Ù‡ Ø³Ø§Ø¨Ù‚Ø§Ù‹ Ù…Ø¹ Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©."""
                
                # Detect topic change
                topic_change_patterns = ['Ø³Ø¤Ø§Ù„ Ø¢Ø®Ø±', 'Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø®ØªÙ„Ù', 'Ø§Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰']
                if any(pattern in last_user_msg for pattern in topic_change_patterns):
                    return """ğŸ”„ **Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:** Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÙŠØ¯ - Ø§Ø¨Ø¯Ø£ ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø¬Ø¯ÙŠØ¯Ø§Ù‹ Ù…Ø¹ Ø§Ù„Ø§Ø¹ØªØ±Ø§Ù Ø¨Ø§Ù„ØªØºÙŠÙŠØ±."""
            
            return """ğŸ’¬ **Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:** Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„Ù†Ù‚Ø§Ø´ - ØªØ§Ø¨Ø¹ Ù…Ø¹ ØªØ¹Ù…ÙŠÙ‚ Ø£ÙƒØ«Ø±."""
            
        except Exception as e:
            logger.error(f"Error processing conversation layer: {str(e)}")
            return ""
    
    # Document-specific layer methods (simplified versions of original)
    def _get_execution_dispute_layer(self, query: str) -> str:
        return f"""ğŸ“‹ **ØªØ®ØµØµ: Ù…Ù†Ø§Ø²Ø¹Ø© Ø§Ù„ØªÙ†ÙÙŠØ°**
Ù…Ø·Ù„ÙˆØ¨: Ù…Ù†Ø§Ø²Ø¹Ø© ØªÙ†ÙÙŠØ° Ù‚ÙˆÙŠØ© Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ù…Ø­ÙƒÙ…Ø©

**Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹:** {query}

**Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ù†Ø§Ø²Ø¹Ø©:**
- Ø§Ù„Ø¯ÙØ¹ Ø§Ù„Ø£ÙˆÙ„ (Ø£Ù‚ÙˆÙ‰ Ø¯ÙØ¹ Ø¥Ø¬Ø±Ø§Ø¦ÙŠ)
- Ø§Ù„Ø¯ÙØ¹ Ø§Ù„Ø«Ø§Ù†ÙŠ (Ø¯ÙØ¹ Ù…ÙˆØ¶ÙˆØ¹ÙŠ)  
- Ø§Ù„Ø¯ÙØ¹ Ø§Ù„Ø«Ø§Ù„Ø« (Ø¯ÙØ¹ Ø´Ø±Ø¹ÙŠ/Ù†Ø¸Ø§Ù…ÙŠ)
- Ø§Ù„Ø·Ù„Ø¨Ø§Øª: ÙˆÙ‚Ù Ø§Ù„ØªÙ†ÙÙŠØ° + Ø±ÙØ¶ Ø§Ù„Ø·Ù„Ø¨ + Ø§Ù„Ø±Ø³ÙˆÙ…

âš ï¸"""
    

    # Add this to the END of your app/core/prompt_controller.py file

# Add this class BEFORE the "# ==================== INTEGRATION FUNCTIONS ====================" line
# in your app/core/prompt_controller.py file

"""
Updated MasterPromptController - Minimal changes for AI integration
Only change: Pass AI client to LegalContextBuilder
"""

class MasterPromptController:
    """
    ğŸ¯ Master Prompt Controller - Enhanced with Dynamic AI Analysis
    
    Now uses pure AI-driven conversation understanding instead of hardcoded patterns
    """
    
    def __init__(self, ai_client=None):  # ğŸš€ ADD: ai_client parameter
        """Initialize with AI client for dynamic analysis"""
        
        # ğŸš€ CHANGE: Pass ai_client to LegalContextBuilder
        self.context_builder = LegalContextBuilder(ai_client=ai_client)
        
        # Everything else stays the same
        self.prompt_composer = PromptComposer()
        self.citation_validator = CitationValidator()
        self.sanitizer = InputSanitizer()
        
        logger.info("ğŸ¯ MasterPromptController initialized with dynamic AI analysis")
    
    async def generate_prompt_for_query(
        self, 
        query: str, 
        retrieved_documents: List[Any] = None,
        conversation_history: List[Dict[str, str]] = None
    ) -> str:
        """
        ğŸ¯ MAIN METHOD: Enhanced with dynamic AI conversation analysis
        
        NO CHANGES to this method - it automatically uses the new dynamic system!
        """
        
        try:
            # ğŸ¯ This now uses dynamic AI analysis instead of hardcoded patterns!
            enhanced_context = await self.context_builder.build_context(
                query=query,
                retrieved_documents=retrieved_documents,
                conversation_history=conversation_history
            )
            
            # Log detection results for monitoring
            logger.info(f"ğŸ¯ Document Type: {enhanced_context.document_type.value}")
            logger.info(f"ğŸ¯ User Intent: {enhanced_context.user_intent.value}")
            logger.info(f"ğŸ¯ Complexity: {enhanced_context.complexity_level.value}")
            logger.info(f"ğŸ¯ Domain: {enhanced_context.legal_domain.value}")
            logger.info(f"ğŸ¯ Confidence: {enhanced_context.confidence_score:.2f}")

            if enhanced_context.warnings:
                logger.warning(f"âš ï¸ Context warnings: {enhanced_context.warnings}")

            # Generate final unified prompt
            unified_prompt = self.prompt_composer.compose_prompt(enhanced_context)

            logger.info(f"âœ… Generated unified prompt: {len(unified_prompt)} characters")
            
            return unified_prompt
            
        except Exception as e:
            logger.error(f"âŒ MasterPromptController error: {str(e)}")
            return self._generate_fallback_prompt(query, str(e))
    
    # All other methods stay exactly the same...
    def validate_response_citations(self, response: str, available_documents: List[Any]) -> Tuple[bool, List[str]]:
        """ğŸ” Validate citations - NO CHANGES"""
        try:
            return self.citation_validator.validate_citations(response, available_documents)
        except Exception as e:
            logger.error(f"âŒ Citation validation error: {str(e)}")
            return False, [f"Citation validation failed: {str(e)}"]
    
    def _generate_fallback_prompt(self, query: str, error: str) -> str:
        """Generate safe fallback prompt - NO CHANGES"""
        return f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ Ù…ØªØ®ØµØµ.

âš ï¸ ØªØ­Ø°ÙŠØ± Ø§Ù„Ù†Ø¸Ø§Ù…: Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨ ({error})

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

ÙŠØ±Ø¬Ù‰:
1. ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¹Ø§Ù…Ø© ÙˆØ¢Ù…Ù†Ø©
2. Ø·Ù„Ø¨ ØªÙˆØ¶ÙŠØ­Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
3. ØªØ¬Ù†Ø¨ Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ù…ÙˆØ§Ø¯ Ù…Ø­Ø¯Ø¯Ø©
4. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ø¨Ø§Ø±Ø©: "Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø©ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø·Ù„Ø¨Ùƒ Ø£ÙƒØ«Ø±"

Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙÙŠØ¯Ø© Ø±ØºÙ… Ø§Ù„Ø®Ø·Ø£ Ø§Ù„ØªÙ‚Ù†ÙŠ."""


# ğŸš€ UPDATE: Factory function to pass AI client
def get_master_controller(ai_client=None) -> MasterPromptController:
    """
    ğŸ¯ Enhanced factory function with AI client support
    
    Now creates MasterPromptController with dynamic AI analysis capabilities
    """
    return MasterPromptController(ai_client=ai_client)

# ==================== INTEGRATION FUNCTIONS ====================

def get_master_controller(ai_client=None) -> MasterPromptController:
    """
    ğŸ¯ Enhanced factory function with AI client support
    
    Now creates MasterPromptController with dynamic AI analysis capabilities
    """
    return MasterPromptController(ai_client=ai_client)


def replace_scattered_prompts(
    query: str,
    retrieved_documents: List[Any] = None,
    conversation_history: List[Dict[str, str]] = None
) -> str:
    """
    ğŸ¯ REPLACEMENT FUNCTION for all existing prompt generation
    
    Use this to replace any existing prompt code throughout your system
    """
    
    controller = get_master_controller()
    return controller.generate_prompt_for_query(
        query=query,
        retrieved_documents=retrieved_documents,
        conversation_history=conversation_history
    )


# ==================== USAGE EXAMPLES ====================

"""
ğŸ¯ INTEGRATION EXAMPLES:

# In your RAG engine, replace existing prompts with:
unified_prompt = replace_scattered_prompts(
    query=user_question,
    retrieved_documents=chunks,
    conversation_history=chat_history
)

# For analysis (useful for frontend features):
controller = get_master_controller()
analysis = controller.analyze_query_intent("Ø£Ø±ÙŠØ¯ Ù…Ø°ÙƒØ±Ø© Ø¯ÙØ§Ø¹ Ø¶Ø¯ Ø¯Ø¹ÙˆÙ‰ ØªØ¬Ø§Ø±ÙŠØ©")
print(analysis["document_type"])  # "defense_memo"
print(analysis["suggested_actions"])  # List of helpful actions

# For citation validation:
is_valid, warnings = controller.validate_response_citations(
    response=ai_response,
    available_documents=retrieved_chunks
)
"""

if __name__ == "__main__":
    # Test the system
    controller = MasterPromptController()
    
    test_query = "Ø£Ø±ÙŠØ¯ Ù…Ø°ÙƒØ±Ø© Ø¯ÙØ§Ø¹ Ø¶Ø¯ Ø¯Ø¹ÙˆÙ‰ ØªØ¬Ø§Ø±ÙŠØ©"
    prompt = controller.generate_prompt_for_query(test_query)
    
    print("ğŸ¯ TEST SUCCESSFUL:")
    print(f"Generated prompt length: {len(prompt)} characters")
    
    analysis = controller.analyze_query_intent(test_query)
    print(f"Detected document type: {analysis['document_type']}")
    print(f"Confidence: {analysis['confidence_score']:.2f}")