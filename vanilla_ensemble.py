import logging
import asyncio
import time
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
from collections import Counter
import openai
import google.generativeai as genai
import requests
import json
import os
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'ensemble_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('VanillaEnsemble')

@dataclass
class ModelResponse:
    model_name: str
    response: str
    cost: float
    response_time: float
    success: bool
    error: str = None

@dataclass
class BestElements:
    best_from_model_1: str
    best_from_model_2: str
    best_from_model_3: str
    overall_quality_score: float

@dataclass
class JudgeEvaluation:
    judge_name: str
    best_elements: BestElements
    processing_time: float
    cost: float
    success: bool
    error: str = None

class VanillaEnsemble:
    def __init__(self):
        logger.info("ğŸš€ Initializing Vanilla Ensemble System")
        
        self.openai_client = None
        self.genai_client = None
        
        self.setup_api_clients()
        
        self.generation_models = [
            "gpt-4o",
            "deepseek-chat", 
            # "grok-2",  # Commented out - no API key available
            "gemini-2.5-flash"
        ]
        
        self.judge_models = [
            # "claude-3-5-sonnet",  # Commented out - no API key available
            "gpt-4o",
            "gemini-2.5-flash"
        ]
        
        logger.info(f"âœ… System initialized with {len(self.generation_models)} generation models and {len(self.judge_models)} judge models")

    def setup_api_clients(self):
        logger.info("ğŸ”§ Setting up API clients...")
        
        try:
            openai_key = os.getenv('OPENAI_API_KEY')
            if openai_key:
                self.openai_client = openai.OpenAI(api_key=openai_key)
                logger.info("âœ… OpenAI client initialized")
            else:
                logger.warning("âš ï¸ OpenAI API key not found")
            
            gemini_key = os.getenv('GEMINI_API_KEY')
            if gemini_key:
                genai.configure(api_key=gemini_key)
                logger.info("âœ… Gemini client initialized")
            else:
                logger.warning("âš ï¸ Gemini API key not found")
                
        except Exception as e:
            logger.error(f"âŒ Error setting up API clients: {e}")

    async def call_openai_model(self, model: str, prompt: str) -> ModelResponse:
        logger.info(f"ğŸ¤– Calling OpenAI model: {model}")
        start_time = time.time()
        
        try:
            response = self.openai_client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1500
            )
            
            response_time = time.time() - start_time
            cost = self.estimate_openai_cost(model, len(prompt), len(response.choices[0].message.content))
            
            logger.info(f"âœ… {model} responded in {response_time:.2f}s, cost: ${cost:.4f}")
            
            return ModelResponse(
                model_name=model,
                response=response.choices[0].message.content,
                cost=cost,
                response_time=response_time,
                success=True
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            logger.error(f"âŒ {model} failed after {response_time:.2f}s: {e}")
            
            return ModelResponse(
                model_name=model,
                response="",
                cost=0.0,
                response_time=response_time,
                success=False,
                error=str(e)
            )

    async def call_deepseek_model(self, prompt: str) -> ModelResponse:
        logger.info("ğŸ¤– Calling DeepSeek model")
        start_time = time.time()
        
        try:
            url = "https://api.deepseek.com/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {os.getenv('DEEPSEEK_API_KEY')}",
                "Content-Type": "application/json"
            }
            data = {
                "model": "deepseek-chat",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 1500
            }
            
            response = requests.post(url, headers=headers, json=data, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            response_time = time.time() - start_time
            cost = 0.04
            
            logger.info(f"âœ… DeepSeek responded in {response_time:.2f}s, cost: ${cost:.4f}")
            
            return ModelResponse(
                model_name="deepseek-chat",
                response=result['choices'][0]['message']['content'],
                cost=cost,
                response_time=response_time,
                success=True
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            logger.error(f"âŒ DeepSeek failed after {response_time:.2f}s: {e}")
            
            return ModelResponse(
                model_name="deepseek-chat",
                response="",
                cost=0.0,
                response_time=response_time,
                success=False,
                error=str(e)
            )

    async def call_gemini_model(self, prompt: str) -> ModelResponse:
        logger.info("ğŸ¤– Calling Gemini model")
        start_time = time.time()
        
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            response = model.generate_content(prompt)
            
            response_time = time.time() - start_time
            cost = 0.03
            
            logger.info(f"âœ… Gemini responded in {response_time:.2f}s, cost: ${cost:.4f}")
            
            return ModelResponse(
                model_name="gemini-2.5-flash",
                response=response.text,
                cost=cost,
                response_time=response_time,
                success=True
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            logger.error(f"âŒ Gemini failed after {response_time:.2f}s: {e}")
            
            return ModelResponse(
                model_name="gemini-2.5-flash",
                response="",
                cost=0.0,
                response_time=response_time,
                success=False,
                error=str(e)
            )

    async def call_grok_model(self, prompt: str) -> ModelResponse:
        logger.info("ğŸ¤– Calling Grok model (simulated)")
        start_time = time.time()
        
        await asyncio.sleep(2)
        response_time = time.time() - start_time
        
        logger.info(f"âš ï¸ Grok simulated response in {response_time:.2f}s")
        
        return ModelResponse(
            model_name="grok-2",
            response="[Grok simulation] Based on Saudi labor law, this requires specific regulatory compliance...",
            cost=0.05,
            response_time=response_time,
            success=True
        )

    def estimate_openai_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        rates = {
            "gpt-4o": {"input": 0.005/1000, "output": 0.015/1000},
            "gpt-4": {"input": 0.03/1000, "output": 0.06/1000}
        }
        
        rate = rates.get(model, rates["gpt-4o"])
        return (input_tokens * rate["input"]) + (output_tokens * rate["output"])

    async def generate_responses(self, question: str) -> List[ModelResponse]:
        logger.info(f"ğŸ“ Starting Step 1: Multi-Model Vanilla Generation")
        logger.info(f"Question: {question}")
        
        saudi_prompt = f"""Answer this question based on Saudi law and regulations. Provide a comprehensive legal response:

{question}

Please provide your answer based on Saudi legal framework, including relevant laws, regulations, and procedures."""

        tasks = []
        
        for model in self.generation_models:
            if model == "gpt-4o":
                tasks.append(self.call_openai_model(model, saudi_prompt))
            elif model == "deepseek-chat":
                tasks.append(self.call_deepseek_model(saudi_prompt))
            elif model == "gemini-2.5-flash":
                tasks.append(self.call_gemini_model(saudi_prompt))
            # elif model == "grok-2":  # Commented out - no API key available
            #     tasks.append(self.call_grok_model(saudi_prompt))
        
        logger.info(f"ğŸš€ Launching {len(tasks)} parallel API calls...")
        responses = await asyncio.gather(*tasks)
        
        successful_responses = [r for r in responses if r.success]
        failed_responses = [r for r in responses if not r.success]
        
        logger.info(f"âœ… Step 1 Complete: {len(successful_responses)} successful, {len(failed_responses)} failed responses")
        
        for response in responses:
            if response.success:
                logger.info(f"âœ… {response.model_name}: {len(response.response)} chars, ${response.cost:.4f}, {response.response_time:.2f}s")
            else:
                logger.error(f"âŒ {response.model_name}: {response.error}")
        
        return responses

    async def extract_components(self, responses: List[ModelResponse]) -> List[JudgeEvaluation]:
        logger.info("âš–ï¸ Starting Step 2: Component Extraction by Judges")
        
        successful_responses = [r for r in responses if r.success]
        if not successful_responses:
            logger.error("âŒ No successful responses to judge")
            return []
        
        combined_responses = "\n\n" + "="*50 + "\n\n".join([
            f"MODEL {i+1} ({r.model_name}):\n{r.response}" 
            for i, r in enumerate(successful_responses)
        ])
        
        judge_prompt = f"""You are a legal expert. Read these model responses and identify the BEST parts from each:

{combined_responses}

Extract the most valuable content from each model. Copy the exact text (don't summarize):

Return ONLY this JSON:
{{
    "best_from_model_1": "[copy the best sections from MODEL 1 exactly as written]",
    "best_from_model_2": "[copy the best sections from MODEL 2 exactly as written]", 
    "best_from_model_3": "[copy the best sections from MODEL 3 exactly as written]",
    "overall_quality_score": 8.5
}}

IMPORTANT: Copy actual text content, not references or summaries."""

        judge_tasks = []
        for judge in self.judge_models:
            if judge == "gpt-4o":  # Removed claude-3-5-sonnet - no API key available
                judge_tasks.append(self.judge_with_openai(judge, judge_prompt))
            elif judge == "gemini-2.5-flash":
                judge_tasks.append(self.judge_with_gemini(judge_prompt))
        
        logger.info(f"âš–ï¸ Launching {len(judge_tasks)} judge evaluations...")
        evaluations = await asyncio.gather(*judge_tasks)
        
        successful_evals = [e for e in evaluations if e.success]
        failed_evals = [e for e in evaluations if not e.success]
        
        logger.info(f"âœ… Step 2 Complete: {len(successful_evals)} successful, {len(failed_evals)} failed evaluations")
        
        for eval in evaluations:
            if eval.success:
                logger.info(f"âœ… {eval.judge_name}: Score {eval.best_elements.overall_quality_score:.1f}, ${eval.cost:.4f}, {eval.processing_time:.2f}s")
            else:
                logger.error(f"âŒ {eval.judge_name}: {eval.error}")
        
        return evaluations

    async def judge_with_openai(self, model: str, prompt: str) -> JudgeEvaluation:
        logger.info(f"âš–ï¸ Judge evaluation with {model}")
        start_time = time.time()
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are a legal expert judge. You MUST respond with ONLY valid JSON. No other text."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000
            )
            
            processing_time = time.time() - start_time
            cost = self.estimate_openai_cost("gpt-4o", len(prompt), len(response.choices[0].message.content))
            
            content = response.choices[0].message.content.strip()
            logger.info(f"ğŸ“„ Judge response content: {content[:200]}...")
            
            # Try to extract JSON from response if it contains other text
            if content.startswith('```json'):
                content = content.split('```json')[1].split('```')[0].strip()
            elif content.startswith('```'):
                content = content.split('```')[1].split('```')[0].strip()
            
            result = json.loads(content)
            
            best_elements = BestElements(
                best_from_model_1=result.get("best_from_model_1", ""),
                best_from_model_2=result.get("best_from_model_2", ""),
                best_from_model_3=result.get("best_from_model_3", ""),
                overall_quality_score=result.get("overall_quality_score", 0.0)
            )
            
            return JudgeEvaluation(
                judge_name=model,
                best_elements=best_elements,
                processing_time=processing_time,
                cost=cost,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ Judge {model} failed: {e}")
            
            empty_elements = BestElements("", "", "", 0.0)
            return JudgeEvaluation(
                judge_name=model,
                best_elements=empty_elements,
                processing_time=processing_time,
                cost=0.0,
                success=False,
                error=str(e)
            )

    async def judge_with_gemini(self, prompt: str) -> JudgeEvaluation:
        logger.info("âš–ï¸ Judge evaluation with Gemini")
        start_time = time.time()
        
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            enhanced_prompt = f"You MUST respond with ONLY valid JSON. No other text. {prompt}"
            response = model.generate_content(enhanced_prompt)
            
            processing_time = time.time() - start_time
            cost = 0.03
            
            content = response.text.strip()
            logger.info(f"ğŸ“„ Gemini judge response: {content[:200]}...")
            
            # Try to extract JSON from response if it contains other text
            if content.startswith('```json'):
                content = content.split('```json')[1].split('```')[0].strip()
            elif content.startswith('```'):
                content = content.split('```')[1].split('```')[0].strip()
            
            result = json.loads(content)
            
            best_elements = BestElements(
                best_from_model_1=result.get("best_from_model_1", ""),
                best_from_model_2=result.get("best_from_model_2", ""),
                best_from_model_3=result.get("best_from_model_3", ""),
                overall_quality_score=result.get("overall_quality_score", 0.0)
            )
            
            return JudgeEvaluation(
                judge_name="gemini-2.5-flash",
                best_elements=best_elements,
                processing_time=processing_time,
                cost=cost,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ Judge Gemini failed: {e}")
            
            empty_elements = BestElements("", "", "", 0.0)
            return JudgeEvaluation(
                judge_name="gemini-2.5-flash",
                best_elements=empty_elements,
                processing_time=processing_time,
                cost=0.0,
                success=False,
                error=str(e)
            )

    def consensus_voting(self, evaluations: List[JudgeEvaluation]) -> Dict[str, str]:
        logger.info("ğŸ—³ï¸ Starting Step 3: Consensus Voting")
        
        successful_evals = [e for e in evaluations if e.success]
        if not successful_evals:
            logger.error("âŒ No successful evaluations available")
            return {"best_elements": "", "avg_score": 0.0}
        
        logger.info(f"ğŸ—³ï¸ Combining best elements from {len(successful_evals)} judges")
        
        # Collect all best elements from all judges
        all_best_elements = []
        total_score = 0
        
        for eval in successful_evals:
            elements = eval.best_elements
            judge_elements = []
            
            if elements.best_from_model_1.strip():
                judge_elements.append(f"From Model 1: {elements.best_from_model_1}")
            if elements.best_from_model_2.strip():
                judge_elements.append(f"From Model 2: {elements.best_from_model_2}")
            if elements.best_from_model_3.strip():
                judge_elements.append(f"From Model 3: {elements.best_from_model_3}")
            
            if judge_elements:
                all_best_elements.extend(judge_elements)
                total_score += elements.overall_quality_score
                logger.info(f"âœ… {eval.judge_name}: Score {elements.overall_quality_score:.1f}, {len(judge_elements)} elements")
        
        # Combine all best elements
        combined_elements = "\n\n".join(all_best_elements)
        avg_score = total_score / len(successful_evals) if successful_evals else 0
        
        logger.info(f"âœ… Step 3 Complete: Combined {len(all_best_elements)} best elements, average score {avg_score:.1f}")
        
        return {
            "best_elements": combined_elements,
            "avg_score": avg_score
        }

    async def synthesize_response(self, best_elements: Dict[str, Any]) -> str:
        logger.info("ğŸ”§ Starting Step 4: AI Synthesis")
        
        synthesis_prompt = f"""You are an expert Saudi law advisor. Here are the best elements selected from multiple AI models:

{best_elements['best_elements']}

Synthesize all this information into ONE coherent, natural Arabic legal response. 

Requirements:
- Write in your own natural style and flow
- Include all the valuable information from above
- Make it read smoothly as a unified response
- Focus on Saudi law and regulations
- Use proper legal Arabic terminology

Create a comprehensive, well-structured legal response in Arabic."""

        try:
            logger.info("ğŸ¤– Synthesizing final response with GPT-4o")
            start_time = time.time()
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": synthesis_prompt}],
                max_tokens=1500
            )
            
            processing_time = time.time() - start_time
            cost = self.estimate_openai_cost("gpt-4o", len(synthesis_prompt), len(response.choices[0].message.content))
            
            logger.info(f"âœ… Step 4 Complete: Response synthesized in {processing_time:.2f}s, cost: ${cost:.4f}")
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"âŒ Synthesis failed: {e}")
            
            # Simple fallback - just return the combined elements
            fallback = f"""Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:

{best_elements['best_elements']}

ÙŠØ±Ø¬Ù‰ Ù…Ù„Ø§Ø­Ø¸Ø© Ø£Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø¬Ù…Ø¹Ø© Ù…Ù† Ø¹Ø¯Ø© Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒÙŠØ© ÙˆØªØªØ·Ù„Ø¨ Ù…Ø±Ø§Ø¬Ø¹Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…ØªØ®ØµØµØ©."""
            
            logger.info("âœ… Using fallback synthesis method")
            return fallback

    async def direct_synthesis(self, responses: List[ModelResponse]) -> str:
        logger.info("ğŸ¯ Starting Direct Synthesis - Full Context Preservation")
        
        successful_responses = [r for r in responses if r.success]
        if not successful_responses:
            logger.error("âŒ No successful responses to synthesize")
            return "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø§Øª ØµØ§Ù„Ø­Ø© Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬."
        
        logger.info(f"ğŸ“Š Synthesizing from {len(successful_responses)} model responses")
        
        # Log original response lengths
        for i, response in enumerate(successful_responses, 1):
            logger.info(f"ğŸ“ Model {i} ({response.model_name}): {len(response.response)} characters")
        
        total_chars = sum(len(r.response) for r in successful_responses)
        logger.info(f"ğŸ“Š Total input content: {total_chars} characters")
        
        # Combine all responses with clear separation
        combined_responses = "\n\n" + "="*80 + "\n\n".join([
            f"Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {i+1} ({r.model_name}):\n{r.response}" 
            for i, r in enumerate(successful_responses)
        ])
        
        synthesis_prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ. Ù„Ø¯ÙŠÙƒ Ù‡Ù†Ø§ {len(successful_responses)} Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ù† Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒÙŠØ© Ù…Ø®ØªÙ„ÙØ© Ø­ÙˆÙ„ Ø³Ø¤Ø§Ù„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙˆØ§Ø­Ø¯.

{combined_responses}

Ù…Ù‡Ù…ØªÙƒ:
1. Ù‚Ø±Ø§Ø¡Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¨Ø¹Ù†Ø§ÙŠØ© ÙˆØ§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† ÙƒÙ„ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…ÙÙŠØ¯Ø©
2. Ø¯Ù…Ø¬ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø­Ø¯Ø© Ø´Ø§Ù…Ù„Ø©
3. Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø© (Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯ØŒ Ø§Ù„Ø£Ù…Ø«Ù„Ø©ØŒ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠØ©ØŒ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©)
4. ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø·Ø¨ÙŠØ¹ÙŠ ÙˆÙ…ØªØ¯ÙÙ‚ ÙƒÙ…Ø§ Ù„Ùˆ ÙƒÙ†Øª Ø®Ø¨ÙŠØ±Ø§Ù‹ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹
5. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¯Ù‚Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆÙÙ‚Ø§Ù‹ Ù„Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ

Ø§ÙƒØªØ¨ Ø¥Ø¬Ø§Ø¨Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…ÙØµÙ„Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ØªØ¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚ÙŠÙ…Ø© Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø«Ù„Ø§Ø«Ø©."""

        try:
            logger.info("ğŸ¤– Running direct synthesis with GPT-4o")
            start_time = time.time()
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": synthesis_prompt}],
                max_tokens=2500,  # Increased for more detailed responses
                temperature=0.3   # Lower temperature for more focused legal content
            )
            
            processing_time = time.time() - start_time
            cost = self.estimate_openai_cost("gpt-4o", len(synthesis_prompt), len(response.choices[0].message.content))
            
            final_response = response.choices[0].message.content
            
            logger.info(f"âœ… Direct synthesis complete in {processing_time:.2f}s, cost: ${cost:.4f}")
            logger.info(f"ğŸ“Š Input: {total_chars} chars â†’ Output: {len(final_response)} chars")
            logger.info(f"ğŸ“ˆ Content preservation: {(len(final_response)/total_chars)*100:.1f}%")
            
            return final_response
            
        except Exception as e:
            logger.error(f"âŒ Direct synthesis failed: {e}")
            
            # Fallback: return the longest/best response
            best_response = max(successful_responses, key=lambda r: len(r.response))
            logger.info(f"âœ… Fallback: Using best single response from {best_response.model_name}")
            
            return f"""Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© (Ø£ÙØ¶Ù„ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† {best_response.model_name}):

{best_response.response}

Ù…Ù„Ø§Ø­Ø¸Ø©: ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø£ÙØ¶Ù„ Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©."""

    async def process_question(self, question: str) -> Dict[str, Any]:
        logger.info("="*80)
        logger.info(f"ğŸ¯ STARTING VANILLA ENSEMBLE PROCESSING")
        logger.info(f"Question: {question}")
        logger.info("="*80)
        
        total_start_time = time.time()
        total_cost = 0.0
        
        responses = await self.generate_responses(question)
        generation_cost = sum(r.cost for r in responses if r.success)
        total_cost += generation_cost
        logger.info(f"ğŸ’° Generation cost: ${generation_cost:.4f}")
        
        logger.info("ğŸ”§ Skipping judge extraction - using direct synthesis for full context preservation")
        
        final_response = await self.direct_synthesis(responses)
        synthesis_cost = 0.02
        total_cost += synthesis_cost
        logger.info(f"ğŸ’° Synthesis cost: ${synthesis_cost:.4f}")
        
        total_time = time.time() - total_start_time
        
        logger.info("="*80)
        logger.info(f"ğŸ‰ ENSEMBLE PROCESSING COMPLETE")
        logger.info(f"â±ï¸ Total time: {total_time:.2f}s ({total_time*1000:.0f}ms)")
        logger.info(f"ğŸ’° Total cost: ${total_cost:.4f}")
        logger.info(f"ğŸ¤– Models used: {len([r for r in responses if r.success])}")
        logger.info(f"ğŸ¯ Direct synthesis approach: No judge filtering - full context preserved")
        logger.info("="*80)
        
        return {
            "final_response": final_response,
            "processing_time_ms": int(total_time * 1000),
            "cost_estimate": round(total_cost, 4),
            "models_used": len([r for r in responses if r.success]),
            "judges_used": 0,  # No judges in direct synthesis approach
            "components_extracted": len([r for r in responses if r.success]),  # Number of model responses used
            "generation_responses": len(responses),
            "successful_generations": len([r for r in responses if r.success]),
            "successful_evaluations": 0,  # No judge evaluations in direct approach
            "consensus_score": 9.0  # High score for direct synthesis approach
        }