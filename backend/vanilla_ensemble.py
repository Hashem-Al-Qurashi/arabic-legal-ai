"""
VANILLA ENSEMBLE SYSTEM
No RAG, no context - just pure model comparison + component extraction

Flow:
1. Send SAME question to 4 models (vanilla responses like using ChatGPT directly)
2. 3 judges extract best components from each response  
3. Assemble best parts into superior final response

This is the EXACT system you specified!
"""

import asyncio
import json
import logging
import time
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any, AsyncIterator
from dataclasses import dataclass
from enum import Enum

from openai import AsyncOpenAI
import httpx
import os
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

# ==================== CONFIGURATION ====================

@dataclass
class VanillaEnsembleConfig:
    """Pure vanilla ensemble configuration"""
    # API Keys
    openai_api_key: str = os.getenv("OPENAI_API_KEY")
    deepseek_api_key: str = os.getenv("DEEPSEEK_API_KEY")  
    grok_api_key: str = os.getenv("GROK_API_KEY")
    gemini_api_key: str = os.getenv("GEMINI_API_KEY")
    claude_api_key: str = os.getenv("ANTHROPIC_API_KEY")
    
    # Model settings
    temperature: float = 0.7
    max_tokens: int = 2000
    
    # Cost tracking
    track_costs: bool = True
    max_cost_per_query: float = 0.50
    
    # Data collection
    save_training_data: bool = True
    training_data_path: str = "data/vanilla_ensemble_training.jsonl"

config = VanillaEnsembleConfig()

class ComponentType(Enum):
    """7 components to extract from vanilla responses"""
    DIRECT_ANSWER = "direct_answer"
    LEGAL_FOUNDATION = "legal_foundation"  
    ARTICLE_CITATIONS = "article_citations"
    NUMERICAL_EXAMPLES = "numerical_examples"
    STEP_PROCEDURES = "step_procedures"
    EDGE_CASES = "edge_cases"
    PRACTICAL_ADVICE = "practical_advice"

# ==================== VANILLA MODEL CLIENTS ====================

class VanillaModelClients:
    """4 Vanilla models + 3 judges (no RAG involved)"""
    
    def __init__(self):
        # Generator models (4) - Pure vanilla clients
        self.chatgpt = AsyncOpenAI(
            api_key=config.openai_api_key,
            http_client=httpx.AsyncClient()
        ) if config.openai_api_key else None
        
        self.deepseek = AsyncOpenAI(
            api_key=config.deepseek_api_key,
            base_url="https://api.deepseek.com/v1",
            http_client=httpx.AsyncClient()
        ) if config.deepseek_api_key else None
        
        self.grok = AsyncOpenAI(
            api_key=config.grok_api_key,
            base_url="https://api.x.ai/v1",
            http_client=httpx.AsyncClient()
        ) if config.grok_api_key else None
        
        # Note: Will add Gemini when we get proper API setup
        self.gemini = None
        
        # Judge models (3)
        self.judge_claude = AsyncOpenAI(
            api_key=config.claude_api_key,
            base_url="https://api.anthropic.com/v1",
            http_client=httpx.AsyncClient()
        ) if config.claude_api_key else None
        
        self.judge_gpt = self.chatgpt
        self.judge_gemini = None  # Will add when Gemini is working
        
        logger.info(f"ğŸ¦ Vanilla model clients initialized:")
        logger.info(f"  Generators: ChatGPT={bool(self.chatgpt)}, DeepSeek={bool(self.deepseek)}")
        logger.info(f"              Grok={bool(self.grok)}, Gemini={bool(self.gemini)}")
        logger.info(f"  Judges: Claude={bool(self.judge_claude)}, GPT={bool(self.judge_gpt)}, Gemini={bool(self.judge_gemini)}")
    
    def get_available_generators(self) -> Dict[str, AsyncOpenAI]:
        """Get available generator models"""
        generators = {}
        if self.chatgpt:
            generators["chatgpt"] = self.chatgpt
        if self.deepseek:
            generators["deepseek"] = self.deepseek
        if self.grok:
            generators["grok"] = self.grok
        if self.gemini:
            generators["gemini"] = self.gemini
        return generators
    
    def get_available_judges(self) -> Dict[str, AsyncOpenAI]:
        """Get available judge models"""
        judges = {}
        if self.judge_claude:
            judges["claude"] = self.judge_claude
        if self.judge_gpt:
            judges["gpt4o"] = self.judge_gpt
        if self.judge_gemini:
            judges["gemini"] = self.judge_gemini
        return judges

# ==================== VANILLA MULTI-MODEL GENERATION ====================

class VanillaMultiModelGenerator:
    """Generate PURE vanilla responses from 4 different models"""
    
    def __init__(self, clients: VanillaModelClients):
        self.clients = clients
        self.generators = clients.get_available_generators()
        
        # Model-specific configurations for vanilla responses
        self.model_configs = {
            "chatgpt": {
                "model": "gpt-4o", 
                "description": "Best overall reasoning",
                "system_prompt": "Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ Ø®Ø¨ÙŠØ±. Ù‚Ø¯Ù… Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙØµÙ„Ø©."
            },
            "deepseek": {
                "model": "deepseek-chat", 
                "description": "Cost-effective, strong on structured tasks",
                "system_prompt": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ. Ù‚Ø¯Ù… Ø§Ø³ØªØ´Ø§Ø±Ø© Ø´Ø§Ù…Ù„Ø© Ù…Ø¹ Ø£Ù…Ø«Ù„Ø© Ø±Ù‚Ù…ÙŠØ© ÙˆØ¥Ø¬Ø±Ø§Ø¡Ø§Øª ÙˆØ§Ø¶Ø­Ø©."
            },
            "grok": {
                "model": "grok-2", 
                "description": "Alternative perspective",
                "system_prompt": "Ø£Ù†Øª Ù…Ø­Ø§Ù…ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ Ù…ØªÙ…Ø±Ø³. Ù‚Ø¯Ù… Ø§Ø³ØªØ´Ø§Ø±Ø© Ø¹Ù…Ù„ÙŠØ© Ù…Ø¹ Ù†ØµØ§Ø¦Ø­ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ§Ù‚Ø¹ÙŠØ©."
            },
            "gemini": {
                "model": "gemini-1.5-pro", 
                "description": "Comprehensive responses",
                "system_prompt": "Ø£Ù†Øª Ø§Ø³ØªØ´Ø§Ø±ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ. Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© ØªØºØ·ÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨."
            }
        }
        
        logger.info(f"ğŸ¦ Vanilla multi-model generator initialized with {len(self.generators)} models")
    
    async def generate_vanilla_responses(self, query: str) -> Dict[str, str]:
        """
        Send SAME vanilla query to all models (no RAG context)
        Each model responds as if user asked them directly
        """
        
        logger.info(f"ğŸ¦ Generating vanilla responses to: {query[:50]}...")
        
        # Generate responses in parallel
        tasks = []
        for model_name, client in self.generators.items():
            task = self._generate_single_vanilla_response(
                model_name=model_name,
                client=client,
                query=query
            )
            tasks.append((model_name, task))
        
        # Wait for all responses
        start_time = time.time()
        responses = {}
        results = await asyncio.gather(*[task for _, task in tasks], return_exceptions=True)
        
        for (model_name, _), result in zip(tasks, results):
            if isinstance(result, Exception):
                logger.error(f"Vanilla model {model_name} failed: {result}")
                responses[model_name] = f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(result)}"
            else:
                responses[model_name] = result
                logger.info(f"âœ… {model_name}: {len(result)} characters")
        
        generation_time = time.time() - start_time
        logger.info(f"â±ï¸ All vanilla models completed in {generation_time:.2f} seconds")
        
        return responses
    
    async def _generate_single_vanilla_response(self, model_name: str, client: AsyncOpenAI, query: str) -> str:
        """Generate vanilla response from a single model (no RAG context)"""
        try:
            model_config = self.model_configs.get(model_name, {})
            model_id = model_config.get("model", "gpt-4o")
            system_prompt = model_config.get("system_prompt", "Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ.")
            
            # Pure vanilla - just system prompt + user question
            response = await client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                temperature=config.temperature,
                max_tokens=config.max_tokens
            )
            
            content = response.choices[0].message.content
            logger.debug(f"ğŸ“ Vanilla {model_name}: {len(content)} characters")
            
            return content
            
        except Exception as e:
            logger.error(f"Vanilla model {model_name} generation failed: {e}")
            raise

# ==================== 3-JUDGE COMPONENT EXTRACTION (SAME AS BEFORE) ====================

@dataclass
class ComponentEvaluation:
    """Single component evaluation by one judge"""
    component: ComponentType
    evaluations: Dict[str, Dict[str, Any]]
    winner: str
    extracted_text: str
    winner_score: float
    reasoning: str

class VanillaComponentJudge:
    """Judge that evaluates vanilla responses for component extraction"""
    
    def __init__(self, judge_name: str, client: AsyncOpenAI):
        self.judge_name = judge_name
        self.client = client
        self.components = list(ComponentType)
        
        logger.info(f"âš–ï¸ Vanilla Judge {judge_name} initialized for {len(self.components)} components")
    
    async def evaluate_vanilla_responses(self, query: str, vanilla_responses: Dict[str, str]) -> List[ComponentEvaluation]:
        """Evaluate vanilla responses (no RAG context) for component extraction"""
        
        logger.info(f"âš–ï¸ Vanilla Judge {self.judge_name} evaluating {len(self.components)} components...")
        
        evaluations = []
        for component in self.components:
            try:
                evaluation = await self._evaluate_single_component(component, query, vanilla_responses)
                evaluations.append(evaluation)
                logger.debug(f"  âœ… {component.value}: Winner = {evaluation.winner} (score: {evaluation.winner_score:.1f})")
            except Exception as e:
                logger.error(f"Component {component.value} evaluation failed: {e}")
                # Create fallback evaluation
                fallback = ComponentEvaluation(
                    component=component,
                    evaluations={},
                    winner="chatgpt",
                    extracted_text="[ØªØ¹Ø°Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙƒÙˆÙ†]",
                    winner_score=5.0,
                    reasoning=f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: {str(e)}"
                )
                evaluations.append(fallback)
        
        logger.info(f"âš–ï¸ Vanilla Judge {self.judge_name} completed all component evaluations")
        return evaluations
    
    async def _evaluate_single_component(self, component: ComponentType, query: str, 
                                       vanilla_responses: Dict[str, str]) -> ComponentEvaluation:
        """Evaluate a single component across vanilla responses"""
        
        # Create component-specific evaluation prompt for vanilla responses
        evaluation_prompt = self._create_vanilla_component_prompt(component, query, vanilla_responses)
        
        # Get judge's evaluation
        response = await self.client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": evaluation_prompt}],
            temperature=0.1,
            max_tokens=1500
        )
        
        evaluation_text = response.choices[0].message.content
        
        # Parse evaluation result
        return self._parse_evaluation_result(component, evaluation_text, vanilla_responses)
    
    def _create_vanilla_component_prompt(self, component: ComponentType, query: str, 
                                       vanilla_responses: Dict[str, str]) -> str:
        """Create evaluation prompt for vanilla responses"""
        
        component_descriptions = {
            ComponentType.DIRECT_ANSWER: {
                "name": "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©",
                "description": "Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø§Ù„ÙˆØ§Ø¶Ø­ ÙˆØ§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ 2-3 Ø¬Ù…Ù„",
                "example": "Ù…Ø¯Ø© Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ø³Ù†ÙˆÙŠØ© 21 ÙŠÙˆÙ…Ø§Ù‹ØŒ ØªØ²ÙŠØ¯ Ø¥Ù„Ù‰ 30 Ø¨Ø¹Ø¯ 5 Ø³Ù†ÙˆØ§Øª Ø®Ø¯Ù…Ø©"
            },
            ComponentType.LEGAL_FOUNDATION: {
                "name": "Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ", 
                "description": "Ø´Ø±Ø­ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø·Ø¨Ù‚Ø© ÙˆØ§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ",
                "example": "ÙˆÙÙ‚Ø§Ù‹ Ù„Ù„Ù…Ø§Ø¯Ø© 84 Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ..."
            },
            ComponentType.ARTICLE_CITATIONS: {
                "name": "Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©",
                "description": "Ø¬Ù…ÙŠØ¹ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯ ÙˆØ§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø©",
                "example": "Ø§Ù„Ù…ÙˆØ§Ø¯ 84ØŒ 85ØŒ 109 Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„"
            },
            ComponentType.NUMERICAL_EXAMPLES: {
                "name": "Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©",
                "description": "Ø­Ø³Ø§Ø¨Ø§Øª Ø¨Ø£Ø±Ù‚Ø§Ù… Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆØµÙŠØº Ø±ÙŠØ§Ø¶ÙŠØ©",
                "example": "Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§ØªØ¨ 6000 Ø±ÙŠØ§Ù„: Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© = 0.5 Ã— 6000 Ã— 5 = 15,000 Ø±ÙŠØ§Ù„"
            },
            ComponentType.STEP_PROCEDURES: {
                "name": "Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„Ø©",
                "description": "Ø®Ø·ÙˆØ§Øª ÙˆØ§Ø¶Ø­Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙ†ÙÙŠØ°",
                "example": "Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªÙ‚Ø¯ÙŠÙ… Ø´ÙƒÙˆÙ‰ Ù„Ù…ÙƒØªØ¨ Ø§Ù„Ø¹Ù…Ù„..."
            },
            ComponentType.EDGE_CASES: {
                "name": "Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠØ©",
                "description": "Ø¸Ø±ÙˆÙ Ø®Ø§ØµØ©ØŒ ØªØ¹Ù‚ÙŠØ¯Ø§ØªØŒ Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª",
                "example": "ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªÙ‚Ø§Ù„Ø© Ù„Ù„Ø²ÙˆØ§Ø¬ØŒ ØªØ³ØªØ­Ù‚ Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© ÙƒØ§Ù…Ù„Ø©"
            },
            ComponentType.PRACTICAL_ADVICE: {
                "name": "Ø§Ù„Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©", 
                "description": "ØªÙˆØµÙŠØ§Øª Ø¹Ù…Ù„ÙŠØ©ØŒ ØªØ­Ø°ÙŠØ±Ø§ØªØŒ Ù†ØµØ§Ø¦Ø­ ÙˆØ§Ù‚Ø¹ÙŠØ©",
                "example": "ÙŠÙÙØ¶Ù„ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù†Ø³Ø® Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø±Ø§Ø³Ù„Ø§Øª"
            }
        }
        
        comp_info = component_descriptions[component]
        
        # Build vanilla responses section
        responses_text = ""
        for model_name, response in vanilla_responses.items():
            responses_text += f"\n--- Ø§Ø³ØªØ¬Ø§Ø¨Ø© {model_name.upper()} (Ø¨Ø¯ÙˆÙ† Ø³ÙŠØ§Ù‚) ---\n{response}\n"
        
        prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªÙ‚ÙŠÙŠÙ… Ù‚Ø§Ù†ÙˆÙ†ÙŠ. Ù‚ÙŠÙ… Ø¬ÙˆØ¯Ø© Ù…ÙƒÙˆÙ† Ù…Ø­Ø¯Ø¯ ÙÙŠ Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ù…Ù† 4 Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒÙŠØ©.

Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ: {query}

Ø§Ù„Ù…ÙƒÙˆÙ† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ØªÙ‚ÙŠÙŠÙ…Ù‡: {comp_info['name']}
Ø§Ù„ÙˆØµÙ: {comp_info['description']}
Ù…Ø«Ø§Ù„: {comp_info['example']}

Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ù…Ù† 4 Ù†Ù…Ø§Ø°Ø¬ (Ø¨Ø¯ÙˆÙ† Ø³ÙŠØ§Ù‚ Ø®Ø§Ø±Ø¬ÙŠ):{responses_text}

ğŸ“‹ Ù…Ù‡Ù…ØªÙƒ:
1. Ù‚ÙŠÙ… Ø¬ÙˆØ¯Ø© "{comp_info['name']}" ÙÙŠ ÙƒÙ„ Ø§Ø³ØªØ¬Ø§Ø¨Ø© (Ø¯Ø±Ø¬Ø© Ù…Ù† 0-10)
2. Ø§Ø®ØªØ± Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø£ÙØ¶Ù„ Ù„Ù‡Ø°Ø§ Ø§Ù„Ù…ÙƒÙˆÙ† ØªØ­Ø¯ÙŠØ¯Ø§Ù‹
3. Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù…ÙƒÙˆÙ† Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„ÙØ§Ø¦Ø²Ø©
4. Ø§Ø´Ø±Ø­ Ø³Ø¨Ø¨ Ø§Ø®ØªÙŠØ§Ø±Ùƒ

Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ù‡ Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª vanilla (Ø¨Ø¯ÙˆÙ† Ù…Ø±Ø§Ø¬Ø¹ Ø®Ø§Ø±Ø¬ÙŠØ©) - Ù‚ÙŠÙ… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù†ÙØ³Ù‡ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…ØµØ§Ø¯Ø±.

ğŸ“Š ØµÙŠØºØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (JSON ÙÙ‚Ø·):
{{
  "evaluations": {{
    "chatgpt": {{"score": 8.5, "reasoning": "Ø³Ø¨Ø¨ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"}},
    "deepseek": {{"score": 6.0, "reasoning": "Ø³Ø¨Ø¨ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"}},
    "grok": {{"score": 9.0, "reasoning": "Ø³Ø¨Ø¨ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"}}, 
    "gemini": {{"score": 7.5, "reasoning": "Ø³Ø¨Ø¨ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"}}
  }},
  "winner": "grok",
  "extracted_text": "Ø§Ù„Ù†Øµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù…ÙƒÙˆÙ† Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„ÙØ§Ø¦Ø²Ø©",
  "reasoning": "Ø³Ø¨Ø¨ Ø§Ø®ØªÙŠØ§Ø± Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙƒØ£ÙØ¶Ù„"
}}

Ø£Ø¬Ø¨ Ø¨Ù€ JSON ÙÙ‚Ø·ØŒ Ù„Ø§ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ."""

        return prompt
    
    def _parse_evaluation_result(self, component: ComponentType, evaluation_text: str, 
                               vanilla_responses: Dict[str, str]) -> ComponentEvaluation:
        """Parse judge's evaluation result"""
        
        try:
            # Clean and parse JSON
            clean_text = evaluation_text.strip()
            if "```json" in clean_text:
                clean_text = clean_text.split("```json")[1].split("```")[0].strip()
            elif "```" in clean_text:
                clean_text = clean_text.split("```")[1].split("```")[0].strip()
            
            result = json.loads(clean_text)
            
            evaluations = result.get("evaluations", {})
            winner = result.get("winner", "chatgpt")
            extracted_text = result.get("extracted_text", "[Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ]")
            reasoning = result.get("reasoning", "Ù„Ù… ÙŠØªÙ… ØªÙ‚Ø¯ÙŠÙ… ØªØ¨Ø±ÙŠØ±")
            
            # Get winner score
            winner_score = 5.0
            if winner in evaluations:
                winner_score = float(evaluations[winner].get("score", 5.0))
            
            return ComponentEvaluation(
                component=component,
                evaluations=evaluations,
                winner=winner,
                extracted_text=extracted_text,
                winner_score=winner_score,
                reasoning=reasoning
            )
            
        except Exception as e:
            logger.error(f"Failed to parse vanilla evaluation for {component.value}: {e}")
            logger.error(f"Raw evaluation text: {evaluation_text[:300]}...")
            
            # Fallback evaluation
            return ComponentEvaluation(
                component=component,
                evaluations={},
                winner="chatgpt",
                extracted_text="[Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø©]",
                winner_score=5.0,
                reasoning=f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {str(e)}"
            )

class VanillaThreeJudgeSystem:
    """3 judges evaluate vanilla responses"""
    
    def __init__(self, clients: VanillaModelClients):
        self.judges = {}
        available_judges = clients.get_available_judges()
        
        for judge_name, client in available_judges.items():
            self.judges[judge_name] = VanillaComponentJudge(judge_name, client)
        
        logger.info(f"âš–ï¸ Vanilla three-judge system initialized with {len(self.judges)} judges: {list(self.judges.keys())}")
    
    async def evaluate_vanilla_responses(self, query: str, vanilla_responses: Dict[str, str]) -> Dict[str, List[ComponentEvaluation]]:
        """All 3 judges evaluate vanilla responses"""
        
        logger.info("âš–ï¸ Starting vanilla 3-judge evaluation process...")
        start_time = time.time()
        
        # Run all judges in parallel
        judge_tasks = []
        for judge_name, judge in self.judges.items():
            task = judge.evaluate_vanilla_responses(query, vanilla_responses)
            judge_tasks.append((judge_name, task))
        
        # Wait for all judges
        judge_results = {}
        results = await asyncio.gather(*[task for _, task in judge_tasks], return_exceptions=True)
        
        for (judge_name, _), result in zip(judge_tasks, results):
            if isinstance(result, Exception):
                logger.error(f"Vanilla Judge {judge_name} failed: {result}")
                judge_results[judge_name] = []
            else:
                judge_results[judge_name] = result
                logger.info(f"âš–ï¸ Vanilla Judge {judge_name}: {len(result)} component evaluations")
        
        evaluation_time = time.time() - start_time
        logger.info(f"âš–ï¸ All vanilla judges completed in {evaluation_time:.2f} seconds")
        
        return judge_results

# ==================== CONSENSUS BUILDING (SAME AS BEFORE) ====================

@dataclass  
class VanillaConsensusResult:
    """Consensus result for vanilla responses"""
    component: ComponentType
    final_text: str
    winning_model: str
    consensus_type: str
    judge_votes: List[str]
    final_score: float

class VanillaConsensusBuilder:
    """Build consensus from vanilla response evaluations"""
    
    def __init__(self):
        logger.info("ğŸ—³ï¸ Vanilla consensus builder initialized")
    
    def build_vanilla_consensus(self, judge_evaluations: Dict[str, List[ComponentEvaluation]]) -> List[VanillaConsensusResult]:
        """Apply voting rules to vanilla response components"""
        
        if not judge_evaluations:
            logger.warning("No vanilla judge evaluations provided")
            return []
        
        # Get all components to process
        components = list(ComponentType)
        consensus_results = []
        
        for component in components:
            try:
                consensus = self._build_component_consensus(component, judge_evaluations)
                consensus_results.append(consensus)
                
                logger.info(f"ğŸ—³ï¸ Vanilla {component.value}: {consensus.consensus_type} -> {consensus.winning_model} (score: {consensus.final_score:.1f})")
                
            except Exception as e:
                logger.error(f"Vanilla consensus building failed for {component.value}: {e}")
                # Create fallback consensus
                fallback = VanillaConsensusResult(
                    component=component,
                    final_text="[ØªØ¹Ø°Ø± Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¥Ø¬Ù…Ø§Ø¹ Ù„Ù‡Ø°Ø§ Ø§Ù„Ù…ÙƒÙˆÙ†]",
                    winning_model="chatgpt",
                    consensus_type="error",
                    judge_votes=[],
                    final_score=0.0
                )
                consensus_results.append(fallback)
        
        logger.info(f"ğŸ—³ï¸ Vanilla consensus building completed for {len(consensus_results)} components")
        return consensus_results
    
    def _build_component_consensus(self, component: ComponentType, 
                                 judge_evaluations: Dict[str, List[ComponentEvaluation]]) -> VanillaConsensusResult:
        """Build consensus for a single component from vanilla responses"""
        
        # Extract evaluations for this component from all judges
        component_evaluations = []
        for judge_name, evaluations in judge_evaluations.items():
            for eval_item in evaluations:
                if eval_item.component == component:
                    component_evaluations.append((judge_name, eval_item))
                    break
        
        if not component_evaluations:
            # Component missing from all judges
            return VanillaConsensusResult(
                component=component,
                final_text="",
                winning_model="none",
                consensus_type="missing",
                judge_votes=[],
                final_score=0.0
            )
        
        # Extract votes and scores
        judge_votes = [eval_item.winner for _, eval_item in component_evaluations]
        judge_scores = [eval_item.winner_score for _, eval_item in component_evaluations]
        
        # Apply voting rules
        
        # Rule 1: Strong Consensus (2+ judges agree)
        vote_counts = {}
        for vote in judge_votes:
            vote_counts[vote] = vote_counts.get(vote, 0) + 1
        
        # Find if any model has 2+ votes
        consensus_winner = None
        for model, count in vote_counts.items():
            if count >= 2:
                consensus_winner = model
                break
        
        if consensus_winner:
            # Strong consensus found
            winning_evaluation = None
            for _, eval_item in component_evaluations:
                if eval_item.winner == consensus_winner:
                    winning_evaluation = eval_item
                    break
            
            return VanillaConsensusResult(
                component=component,
                final_text=winning_evaluation.extracted_text if winning_evaluation else "",
                winning_model=consensus_winner,
                consensus_type="strong_consensus",
                judge_votes=judge_votes,
                final_score=winning_evaluation.winner_score if winning_evaluation else 0.0
            )
        
        # Rule 2: No Consensus - Use Highest Individual Score
        best_score = max(judge_scores)
        best_judge_idx = judge_scores.index(best_score)
        best_winner = judge_votes[best_judge_idx]
        best_evaluation = component_evaluations[best_judge_idx][1]
        
        return VanillaConsensusResult(
            component=component,
            final_text=best_evaluation.extracted_text,
            winning_model=best_winner,
            consensus_type="highest_score",
            judge_votes=judge_votes,
            final_score=best_score
        )

# ==================== VANILLA RESPONSE ASSEMBLY ====================

class VanillaResponseAssembler:
    """Assemble final response from vanilla response components"""
    
    def __init__(self, clients: VanillaModelClients):
        # Use ChatGPT for assembly
        self.assembler_client = clients.chatgpt
        logger.info("ğŸ”§ Vanilla response assembler initialized")
    
    async def assemble_vanilla_response(self, consensus_results: List[VanillaConsensusResult], 
                                      query: str, use_smooth_transitions: bool = True) -> str:
        """Assemble final response from vanilla response components"""
        
        if use_smooth_transitions and self.assembler_client:
            return await self._assemble_with_transitions(consensus_results, query)
        else:
            return self._assemble_simple_concatenation(consensus_results)
    
    def _assemble_simple_concatenation(self, consensus_results: List[VanillaConsensusResult]) -> str:
        """Simple concatenation of vanilla components"""
        
        # Assembly order
        order = [
            ComponentType.DIRECT_ANSWER,
            ComponentType.LEGAL_FOUNDATION, 
            ComponentType.ARTICLE_CITATIONS,
            ComponentType.NUMERICAL_EXAMPLES,
            ComponentType.STEP_PROCEDURES,
            ComponentType.EDGE_CASES,
            ComponentType.PRACTICAL_ADVICE
        ]
        
        sections = []
        
        for component_type in order:
            # Find consensus result for this component
            component_result = None
            for result in consensus_results:
                if result.component == component_type:
                    component_result = result
                    break
            
            if component_result and component_result.final_text.strip():
                sections.append(component_result.final_text)
        
        final_response = "\n\n".join(sections)
        
        if not final_response.strip():
            final_response = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªØ¬Ù…ÙŠØ¹ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©."
        
        logger.info(f"ğŸ”§ Vanilla simple assembly completed: {len(sections)} sections, {len(final_response)} characters")
        return final_response
    
    async def _assemble_with_transitions(self, consensus_results: List[VanillaConsensusResult], query: str) -> str:
        """Smooth transitions for vanilla components"""
        
        try:
            # Collect all components
            component_texts = {}
            
            for result in consensus_results:
                if result.final_text.strip():
                    component_texts[result.component.value] = result.final_text
            
            if not component_texts:
                return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ù…ÙƒÙˆÙ†Ø§Øª Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬."
            
            # Create assembly prompt for vanilla responses
            assembly_prompt = self._create_vanilla_assembly_prompt(component_texts, query)
            
            # Generate assembled response
            response = await self.assembler_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": assembly_prompt}],
                temperature=0.3,
                max_tokens=3000
            )
            
            assembled_response = response.choices[0].message.content.strip()
            
            logger.info(f"ğŸ”§ Vanilla smooth assembly completed: {len(component_texts)} components, {len(assembled_response)} characters")
            
            return assembled_response
            
        except Exception as e:
            logger.error(f"Vanilla smooth assembly failed: {e}, falling back to simple concatenation")
            return self._assemble_simple_concatenation(consensus_results)
    
    def _create_vanilla_assembly_prompt(self, component_texts: Dict[str, str], query: str) -> str:
        """Create prompt for vanilla response assembly"""
        
        components_section = ""
        for component_name, text in component_texts.items():
            components_section += f"\n[{component_name}]\n{text}\n"
        
        return f"""Ø£Ù†Øª Ù…Ø­Ø±Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ù…Ù† Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒÙŠØ© Ù…Ø®ØªÙ„ÙØ©.

Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ: {query}

Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ù…Ù† Ø£ÙØ¶Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª vanilla Ø¨Ø¯ÙˆÙ† Ø³ÙŠØ§Ù‚ Ø®Ø§Ø±Ø¬ÙŠ):{components_section}

ğŸ¯ Ù…Ù‡Ù…ØªÙƒ:
- Ø§Ø¬Ù…Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª ÙÙŠ Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙˆØ§Ø­Ø¯Ø© Ù…ØªÙ…Ø§Ø³ÙƒØ©
- Ø£Ø¶Ù Ø¬Ù…Ù„ Ø±Ø¨Ø· Ø¨Ø³ÙŠØ·Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©
- Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ø¨Ø§Ù„Ø¶Ø¨Ø· ÙƒÙ…Ø§ Ù‡Ùˆ (Ù„Ø§ ØªØ¹Ø¯Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰)
- Ø±ØªØ¨ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ù…Ù†Ø·Ù‚ÙŠØ§Ù‹ (Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø§Ù„Ù†ØµØ§Ø¦Ø­ Ø£Ø®ÙŠØ±Ø§Ù‹)
- ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„ØªØ¯ÙÙ‚ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ Ù„Ù„Ù†Øµ

âš ï¸ Ù‚ÙˆØ§Ø¹Ø¯ Ø­Ø§Ø³Ù…Ø©:
- Ù„Ø§ ØªØºÙŠØ± Ø£Ùˆ ØªØ¹ÙŠØ¯ ØµÙŠØ§ØºØ© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©  
- Ø£Ø¶Ù ÙÙ‚Ø· Ø¬Ù…Ù„ Ø±Ø¨Ø· Ù‚ØµÙŠØ±Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø¶Ø±ÙˆØ±Ø©
- Ù„Ø§ ØªØ¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©
- Ø§Ù„Ù‡Ø¯Ù: ØªØ¯ÙÙ‚ Ø·Ø¨ÙŠØ¹ÙŠ Ù„Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
- Ù‡Ø°Ù‡ Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª vanilla (Ø¨Ø¯ÙˆÙ† Ù…Ø±Ø§Ø¬Ø¹) - Ù„Ø§ ØªØ¶Ù Ù…Ø±Ø§Ø¬Ø¹ Ø®Ø§Ø±Ø¬ÙŠØ©

Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© ÙÙ‚Ø·."""

# ==================== MAIN VANILLA ENSEMBLE ENGINE ====================

class VanillaEnsembleLegalAI:
    """
    Pure Vanilla Ensemble System
    No RAG - Just 4 models + 3 judges + component extraction
    """
    
    def __init__(self):
        """Initialize vanilla ensemble components"""
        
        logger.info("ğŸ¦ Initializing Vanilla Ensemble Legal AI System...")
        
        self.config = config
        self.clients = VanillaModelClients()
        
        # Vanilla components
        self.multi_generator = VanillaMultiModelGenerator(self.clients)
        self.judge_system = VanillaThreeJudgeSystem(self.clients)
        self.consensus_builder = VanillaConsensusBuilder()
        self.assembler = VanillaResponseAssembler(self.clients)
        
        logger.info("âœ… Vanilla Ensemble Legal AI System initialized successfully")
        logger.info(f"ğŸ¤– Available generators: {len(self.clients.get_available_generators())}")
        logger.info(f"âš–ï¸ Available judges: {len(self.clients.get_available_judges())}")
    
    async def process_vanilla_query(self, query: str) -> Dict[str, Any]:
        """
        Pure vanilla processing pipeline
        
        1. Send query to 4 models (no RAG)
        2. 3 judges extract best components
        3. Assemble best parts into final response
        """
        
        start_time = time.time()
        request_id = str(uuid.uuid4())
        
        logger.info(f"ğŸ¦ Processing vanilla query [{request_id}]: {query[:50]}...")
        
        try:
            # Step 1: Generate vanilla responses from 4 models
            logger.info("ğŸ¦ Step 1: Vanilla Multi-Model Generation")
            vanilla_responses = await self.multi_generator.generate_vanilla_responses(query)
            
            if not vanilla_responses:
                raise Exception("No vanilla responses generated from any model")
            
            # Step 2: 3-Judge Component Extraction
            logger.info("âš–ï¸ Step 2: Vanilla Component Extraction (3 Judges)")
            judge_evaluations = await self.judge_system.evaluate_vanilla_responses(query, vanilla_responses)
            
            # Step 3: Consensus Building
            logger.info("ğŸ—³ï¸ Step 3: Vanilla Consensus Building")
            consensus_results = self.consensus_builder.build_vanilla_consensus(judge_evaluations)
            
            # Step 4: Response Assembly
            logger.info("ğŸ”§ Step 4: Vanilla Response Assembly")
            final_response = await self.assembler.assemble_vanilla_response(consensus_results, query)
            
            # Calculate metrics
            processing_time_ms = int((time.time() - start_time) * 1000)
            cost_estimate = self._estimate_vanilla_cost(vanilla_responses, judge_evaluations)
            
            # Compile result
            result = {
                "request_id": request_id,
                "query": query,
                "final_response": final_response,
                "processing_time_ms": processing_time_ms,
                "cost_estimate": cost_estimate,
                "vanilla_data": {
                    "generator_responses": len(vanilla_responses),
                    "judge_evaluations": len(judge_evaluations),
                    "consensus_components": len([r for r in consensus_results if r.final_text.strip()]),
                    "component_winners": {r.component.value: r.winning_model for r in consensus_results}
                },
                "intermediate_data": {
                    "vanilla_responses": vanilla_responses,
                    "consensus_results": consensus_results,
                    "judge_evaluations": judge_evaluations
                } if self.config.save_training_data else None
            }
            
            logger.info(f"âœ… Vanilla ensemble processing completed [{request_id}]: {processing_time_ms}ms, ${cost_estimate:.3f}")
            
            return result
            
        except Exception as e:
            processing_time_ms = int((time.time() - start_time) * 1000)
            logger.error(f"âŒ Vanilla ensemble processing failed [{request_id}]: {e}")
            
            return {
                "request_id": request_id,
                "query": query,
                "final_response": f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ: {str(e)}",
                "processing_time_ms": processing_time_ms,
                "cost_estimate": 0.0,
                "vanilla_data": {},
                "error": str(e)
            }
    
    def _estimate_vanilla_cost(self, vanilla_responses: Dict[str, str], judge_evaluations: Dict[str, List[ComponentEvaluation]]) -> float:
        """Estimate costs for vanilla ensemble"""
        
        # Generation costs
        generation_cost = 0.0
        
        cost_per_model = {
            "chatgpt": 0.06,
            "deepseek": 0.01,
            "grok": 0.05,
            "gemini": 0.04
        }
        
        for model_name in vanilla_responses.keys():
            generation_cost += cost_per_model.get(model_name, 0.03)
        
        # Judging costs
        judging_cost = len(judge_evaluations) * 0.04
        
        # Assembly cost
        assembly_cost = 0.02
        
        total_cost = generation_cost + judging_cost + assembly_cost
        
        return round(total_cost, 3)
    
    async def process_vanilla_streaming(self, query: str) -> AsyncIterator[Dict[str, Any]]:
        """Streaming version of vanilla ensemble"""
        
        request_id = str(uuid.uuid4())
        start_time = time.time()
        
        try:
            yield {"type": "start", "request_id": request_id, "query": query, "mode": "vanilla"}
            
            # Step 1
            yield {"type": "progress", "step": 1, "status": "Vanilla Model Generation"}
            vanilla_responses = await self.multi_generator.generate_vanilla_responses(query)
            yield {"type": "progress", "step": 1, "status": "Complete", "models": len(vanilla_responses)}
            
            # Step 2
            yield {"type": "progress", "step": 2, "status": "Judge Evaluation"}
            judge_evaluations = await self.judge_system.evaluate_vanilla_responses(query, vanilla_responses)
            yield {"type": "progress", "step": 2, "status": "Complete", "judges": len(judge_evaluations)}
            
            # Step 3
            yield {"type": "progress", "step": 3, "status": "Consensus Building"}
            consensus_results = self.consensus_builder.build_vanilla_consensus(judge_evaluations)
            yield {"type": "progress", "step": 3, "status": "Complete", "components": len(consensus_results)}
            
            # Step 4
            yield {"type": "progress", "step": 4, "status": "Response Assembly"}
            final_response = await self.assembler.assemble_vanilla_response(consensus_results, query)
            yield {"type": "progress", "step": 4, "status": "Complete"}
            
            # Final result
            processing_time_ms = int((time.time() - start_time) * 1000)
            cost_estimate = self._estimate_vanilla_cost(vanilla_responses, judge_evaluations)
            
            yield {
                "type": "complete",
                "request_id": request_id,
                "final_response": final_response,
                "processing_time_ms": processing_time_ms,
                "cost_estimate": cost_estimate,
                "vanilla_stats": {
                    "models_used": len(vanilla_responses),
                    "judges_used": len(judge_evaluations),
                    "components_extracted": len([r for r in consensus_results if r.final_text.strip()])
                }
            }
            
        except Exception as e:
            yield {
                "type": "error",
                "request_id": request_id,
                "error": str(e),
                "processing_time_ms": int((time.time() - start_time) * 1000)
            }

# ==================== GLOBAL VANILLA INSTANCE ====================

# Initialize vanilla ensemble system
vanilla_ensemble = VanillaEnsembleLegalAI()

# Export functions
async def process_vanilla_ensemble_query(query: str) -> Dict[str, Any]:
    """Main vanilla ensemble processing function"""
    return await vanilla_ensemble.process_vanilla_query(query)

async def process_vanilla_ensemble_streaming(query: str) -> AsyncIterator[Dict[str, Any]]:
    """Streaming vanilla ensemble processing"""
    async for update in vanilla_ensemble.process_vanilla_streaming(query):
        yield update

def get_vanilla_ensemble_stats() -> Dict[str, Any]:
    """Get vanilla ensemble system statistics"""
    
    return {
        "system_status": "active",
        "system_type": "pure vanilla ensemble",
        "available_generators": len(vanilla_ensemble.clients.get_available_generators()),
        "available_judges": len(vanilla_ensemble.clients.get_available_judges()),
        "config": {
            "temperature": vanilla_ensemble.config.temperature,
            "max_tokens": vanilla_ensemble.config.max_tokens,
            "rag_enabled": False
        },
        "process_flow": [
            "1. Send SAME question to 4 models (vanilla responses)",
            "2. 3 judges extract best components from each response",
            "3. Consensus voting to select best parts",
            "4. Assemble best components into final response"
        ]
    }

def get_vanilla_ensemble_engine():
    """Get vanilla ensemble engine instance"""
    return vanilla_ensemble

logger.info("ğŸ¦ Vanilla Ensemble Legal AI System loaded and ready!")
logger.info("ğŸ“‹ Process: 4 Vanilla Models â†’ 3 Judge Component Extraction â†’ Assembly")
logger.info("ğŸš« NO RAG: Pure model comparison with component extraction")

# ==================== VANILLA TESTING FRAMEWORK ====================

async def test_vanilla_ensemble():
    """Test vanilla ensemble with legal queries"""
    
    test_queries = [
        "Ù…Ø§ Ù‡ÙŠ Ù…ÙƒØ§ÙØ£Ø© Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©ØŸ",
        "ÙƒÙŠÙ Ø£Ø­Ø³Ø¨ Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ø³Ù†ÙˆÙŠØ© Ù„Ù„Ù…ÙˆØ¸ÙØŸ", 
        "Ù…Ø§ Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„ØªØ£Ø®ÙŠØ± Ø¹Ù† Ø§Ù„Ø¹Ù…Ù„ØŸ"
    ]
    
    logger.info("ğŸ§ª Testing vanilla ensemble system...")
    
    for i, query in enumerate(test_queries, 1):
        logger.info(f"\nğŸ§ª Vanilla Test {i}: {query}")
        
        try:
            result = await vanilla_ensemble.process_vanilla_query(query)
            
            logger.info(f"âœ… Vanilla Test {i} completed:")
            logger.info(f"  Processing time: {result['processing_time_ms']}ms")
            logger.info(f"  Cost estimate: ${result['cost_estimate']:.3f}")
            logger.info(f"  Response length: {len(result['final_response'])} chars")
            
            if result['vanilla_data']:
                logger.info(f"  Models used: {result['vanilla_data']['generator_responses']}")
                logger.info(f"  Components: {result['vanilla_data']['consensus_components']}")
            
        except Exception as e:
            logger.error(f"âŒ Vanilla Test {i} failed: {e}")
    
    logger.info("ğŸ§ª Vanilla ensemble testing completed")

if __name__ == "__main__":
    asyncio.run(test_vanilla_ensemble())