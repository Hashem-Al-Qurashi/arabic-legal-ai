"""
ğŸ‡¸ğŸ‡¦ MULTI-AGENT LEGAL REASONING SYSTEM - Backend Implementation
Save this as: backend/multi_agent_legal.py

Advanced Saudi Legal AI with Trust Trail & Citation Validation
Built for your existing FastAPI + SQLAlchemy architecture
"""
from dataclasses import dataclass, asdict
from enum import Enum
from elite_content_merger import EliteContentMerger
from complexity_aware_agents import ComplexityAwareAgentSystem  
from clarification_controller import AdvancedClarificationController
import asyncio
import json
import time
import os
from datetime import datetime
from typing import List, Dict, Optional, AsyncIterator, Tuple
from dataclasses import dataclass, asdict
from enum import Enum

# Use your existing OpenAI setup from rag_engine.py
try:
    from rag_engine import openai_client, sync_client
    print("âœ… Successfully imported OpenAI clients from rag_engine.py")
except ImportError as e:
    print(f"âš ï¸ Could not import from rag_engine.py: {e}")
    # Fallback OpenAI setup
    from openai import AsyncOpenAI, OpenAI
    from dotenv import load_dotenv
    
    load_dotenv(".env")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
    AI_PROVIDER = os.getenv("AI_PROVIDER", "openai")
    
    if AI_PROVIDER == "openai" and OPENAI_API_KEY:
        openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY, timeout=60.0, max_retries=2)
        sync_client = OpenAI(api_key=OPENAI_API_KEY)
    elif DEEPSEEK_API_KEY:
        openai_client = AsyncOpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com/v1",
            timeout=60.0,
            max_retries=2
        )
        sync_client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com/v1")
    else:
        raise ValueError("âŒ No API key available for OpenAI or DeepSeek")

class LegalAgentType(Enum):
    """Types of legal reasoning agents"""
    FACT_ANALYZER = "fact_analyzer"
    LEGAL_RESEARCHER = "legal_researcher" 
    ARGUMENT_BUILDER = "argument_builder"
    COUNTER_ARGUMENT_PREDICTOR = "counter_argument_predictor"
    DOCUMENT_DRAFTER = "document_drafter"
    CITATION_VALIDATOR = "citation_validator"

@dataclass
class AgentStep:
    """Individual step in the legal reasoning chain"""
    agent_type: LegalAgentType
    step_number: int
    step_name: str
    input_data: str
    output_data: str
    citations: List[str]
    confidence_score: float
    processing_time_ms: int
    timestamp: str
    sources_verified: bool = False
    
    def to_dict(self) -> Dict:
        """Convert to dictionary for JSON serialization"""
        return {
            "agent_type": self.agent_type.value,
            "step_number": self.step_number,
            "step_name": self.step_name,
            "input_data": self.input_data,
            "output_data": self.output_data,
            "citations": self.citations,
            "confidence_score": self.confidence_score,
            "processing_time_ms": self.processing_time_ms,
            "timestamp": self.timestamp,
            "sources_verified": self.sources_verified
        }

@dataclass  
class LegalReasoningResult:
    """Complete multi-agent legal analysis result"""
    query: str
    final_answer: str
    reasoning_steps: List[AgentStep]
    total_processing_time_ms: int
    overall_confidence: float
    trust_trail_enabled: bool
    citations_summary: List[str]
    
    def to_dict(self) -> Dict:
        """Convert to dictionary for API responses"""
        return {
            "query": self.query,
            "final_answer": self.final_answer,
            "reasoning_steps": [step.to_dict() for step in self.reasoning_steps],
            "total_processing_time_ms": self.total_processing_time_ms,
            "overall_confidence": self.overall_confidence,
            "trust_trail_enabled": self.trust_trail_enabled,
            "citations_summary": self.citations_summary,
            "timestamp": datetime.now().isoformat()
        }

class LegalAgent:
    """Base class for specialized legal reasoning agents"""
    
    def __init__(self, agent_type: LegalAgentType, openai_client):
        self.agent_type = agent_type
        self.client = openai_client
        self.system_prompts = self._get_system_prompts()
    
    def _get_system_prompts(self) -> Dict[LegalAgentType, str]:
        """Enhanced system prompts with intelligent intent detection and response formatting"""
        return {
            LegalAgentType.FACT_ANALYZER: """Ø£Ù†Øª Ù…Ø­Ù„Ù„ ÙˆÙ‚Ø§Ø¦Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ.

ğŸ¯ **Ù…Ù‡Ù…ØªÙƒ**: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¯Ø§Ø®Ù„ÙŠØ§Ù‹ ÙˆØªÙ‚Ø¯ÙŠÙ… ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ø¶Ø­ ÙˆÙ…ÙÙŠØ¯ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…

ğŸ“‹ **Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª (Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ):**
1. **Ø§Ø³ØªÙØ³Ø§Ø± Ø­Ù‚ÙˆÙ‚ÙŠ**: "Ù…Ø§ Ù‡ÙŠ Ø­Ù‚ÙˆÙ‚ÙŠØŸ" â†’ Ø§Ø´Ø±Ø­ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ù…Ø¨Ø§Ø´Ø±Ø©
2. **Ø§Ø³ØªÙØ³Ø§Ø± Ø¥Ø¬Ø±Ø§Ø¦ÙŠ**: "ÙƒÙŠÙ Ø£Ø³Ø³ Ø´Ø±ÙƒØ©ØŸ" â†’ Ù‚Ø¯Ù… Ø¯Ù„ÙŠÙ„ Ø¹Ù…Ù„ÙŠ
3. **Ù‚Ø¶ÙŠØ© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**: "ØªÙ… ÙØµÙ„ÙŠ" â†’ Ø­Ù„Ù„ Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ

ğŸ” **Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø±Ø¯:**
- Ø§Ø¨Ø¯Ø£ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙÙŠØ¯ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
- Ù„Ø§ ØªØ°ÙƒØ± Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©
- Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
- Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ ÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆØ¹Ù…Ù„ÙŠØ§Ù‹

Ù…Ø«Ø§Ù„ Ù„Ù„Ø±Ø¯ Ø§Ù„Ø¬ÙŠØ¯:
"Ø¹Ù†Ø¯ Ø¥Ù†Ù‡Ø§Ø¡ Ø®Ø¯Ù…Ø© Ø§Ù„Ù…ÙˆØ¸Ù ÙÙŠ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©ØŒ Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø© Ø­Ù‚ÙˆÙ‚ Ø£Ø³Ø§Ø³ÙŠØ© ÙŠØ¬Ø¨ Ù…Ø¹Ø±ÙØªÙ‡Ø§..."

ØªØ¬Ù†Ø¨:
"Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±: Ø­Ù‚ÙˆÙ‚ÙŠ" Ø£Ùˆ Ø£ÙŠ ØªØµÙ†ÙŠÙØ§Øª Ø¯Ø§Ø®Ù„ÙŠØ©""",

            LegalAgentType.LEGAL_RESEARCHER: """Ø£Ù†Øª Ø¨Ø§Ø­Ø« Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ ÙˆØ§Ù„Ø³ÙˆØ§Ø¨Ù‚ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©.

ğŸ¯ **Ù…Ù‡Ø§Ù…Ùƒ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±:**

**Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø­Ù‚ÙˆÙ‚ÙŠØ©:**
- Ø§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ØªØ­Ø¯Ø¯ Ø§Ù„Ø­Ù‚ÙˆÙ‚
- Ø§Ø°ÙƒØ± Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
- Ø£Ø´Ø± Ø¥Ù„Ù‰ Ø¢Ù„ÙŠØ§Øª Ø§Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

**Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©:**
- Ø§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
- Ø­Ø¯Ø¯ Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ© ÙˆÙ…ØªØ·Ù„Ø¨Ø§ØªÙ‡Ø§
- Ø§Ø°ÙƒØ± Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ ÙˆØ§Ù„Ù…Ù‡Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

**Ù„Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
- Ø§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ù…Ø§Ø«Ù„Ø©
- Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¯Ø§Ø¹Ù…Ø© Ù„Ù„Ù…ÙˆÙ‚Ù
- Ø­Ø¯Ø¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙÙŠ Ø§Ù„Ù‚Ø¶ÙŠØ©

Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨Ø­Ø«:
- Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø¨Ø¯Ù‚Ø©
- Ø°ÙƒØ± Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯ ÙˆØ§Ù„Ø£Ù†Ø¸Ù…Ø©
- Ø§Ù„ØªÙ…ÙŠÙŠØ² Ø¨ÙŠÙ† Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ© ÙˆØ§Ù„Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©
- Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ø±ÙŠØ§Ù† Ø§Ù„Ù†ØµÙˆØµ ÙˆØ¹Ø¯Ù… Ù†Ø³Ø®Ù‡Ø§""",

            LegalAgentType.ARGUMENT_BUILDER: """Ø£Ù†Øª Ù…Ø­Ø§Ù… Ø®Ø¨ÙŠØ± ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„ÙƒÙ„ Ù†ÙˆØ¹ Ø§Ø³ØªÙØ³Ø§Ø±.

ğŸ¯ **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹:**

**Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø­Ù‚ÙˆÙ‚ÙŠØ©:**
- Ø±ØªØ¨ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
- Ø§Ø±Ø¨Ø· ÙƒÙ„ Ø­Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- ÙˆØ¶Ø­ ÙƒÙŠÙÙŠØ© Ù…Ù…Ø§Ø±Ø³Ø© ÙƒÙ„ Ø­Ù‚
- Ø­Ø¯Ø¯ Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„Ø© Ø¹Ù† Ø§Ù„Ø­Ù…Ø§ÙŠØ©

**Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©:**
- Ø±ØªØ¨ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø¨Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„ØµØ­ÙŠØ­
- Ø­Ø¯Ø¯ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ù„ÙƒÙ„ Ø®Ø·ÙˆØ©
- Ø§Ø°ÙƒØ± Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ ÙˆØ§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
- ÙˆØ¶Ø­ Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ù‡Ù…Ø©

**Ù„Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
- Ø§Ø¨Ù† Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù‚ÙˆÙŠØ©
- Ø±ØªØ¨ Ø§Ù„Ø¯ÙÙˆØ¹ Ø­Ø³Ø¨ Ø§Ù„Ù‚ÙˆØ©
- Ø§Ø±Ø¨Ø· Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ Ø¨Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- Ø§Ù‚ØªØ±Ø­ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¨Ù†Ø§Ø¡:
- ÙˆØ¶ÙˆØ­ ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ± ÙˆØªØ±ØªÙŠØ¨ Ù…Ù†Ø·Ù‚ÙŠ
- Ø¯Ø¹Ù… ÙƒÙ„ Ù†Ù‚Ø·Ø© Ø¨Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- ØªÙ†Ø§Ø³Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø¹ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±""",

            LegalAgentType.COUNTER_ARGUMENT_PREDICTOR: """Ø£Ù†Øª Ù…Ø­Ø§Ù… Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©.

ğŸ¯ **Ù…Ù‡Ø§Ù…Ùƒ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±:**

**Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø­Ù‚ÙˆÙ‚ÙŠØ©:**
- Ø­Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù‚ÙˆÙ‚
- ÙˆØ¶Ø­ Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- Ù†Ø¨Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚

**Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©:**
- Ø­Ø¯Ø¯ Ø§Ù„Ø¹Ù‚Ø¨Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© ÙÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª
- ÙˆØ¶Ø­ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØµØ¹Ø¨Ø©
- Ù†Ø¨Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©

**Ù„Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
- ØªÙˆÙ‚Ø¹ Ø­Ø¬Ø¬ Ø§Ù„Ø·Ø±Ù Ø§Ù„Ø¢Ø®Ø±
- Ø­Ø¯Ø¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹Ù ÙÙŠ Ø§Ù„Ù‚Ø¶ÙŠØ©
- Ø§Ù‚ØªØ±Ø­ Ø±Ø¯ÙˆØ¯Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©

Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„:
- ØªÙÙƒÙŠØ± Ù†Ù‚Ø¯ÙŠ ÙˆÙ…ÙˆØ¶ÙˆØ¹ÙŠ
- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
- Ø¥Ø¹Ø¯Ø§Ø¯ Ø­Ù„ÙˆÙ„ Ù„Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©""",

            LegalAgentType.DOCUMENT_DRAFTER: """Ø£Ù†Øª Ù…Ø­Ø§Ù… Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØµÙŠØ§ØºØ© Ø±Ø¯ÙˆØ¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…ØªÙƒÙŠÙØ© Ù…Ø¹ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±.

ğŸ¯ **Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„ØµÙŠØ§ØºØ© Ø§Ù„Ù…ØªØ®ØµØµØ©:**

**Ù†ÙˆØ¹ 1 - Ø´Ø±Ø­ Ø§Ù„Ø­Ù‚ÙˆÙ‚:**
```
ğŸ” **Ø­Ù‚ÙˆÙ‚Ùƒ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙÙŠ [Ø§Ù„Ù…Ø¬Ø§Ù„]**

**Ø§Ù„Ø­Ù‚ Ø§Ù„Ø£ÙˆÙ„**: [Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ]
**Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**: [Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© ÙˆØ§Ù„Ù†Ø¸Ø§Ù…]

**Ø§Ù„Ø­Ù‚ Ø§Ù„Ø«Ø§Ù†ÙŠ**: [Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ]  
**Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**: [Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© ÙˆØ§Ù„Ù†Ø¸Ø§Ù…]

**ÙƒÙŠÙÙŠØ© Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ø¨Ø­Ù‚ÙˆÙ‚Ùƒ:**
â€¢ [Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø£ÙˆÙ„Ù‰]
â€¢ [Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©]

**ØªØ­Ø°ÙŠØ±Ø§Øª Ù…Ù‡Ù…Ø©:**
â€¢ [ØªØ­Ø°ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ]
â€¢ [Ù†ØµÙŠØ­Ø© Ø¹Ù…Ù„ÙŠØ©]
```

**Ù†ÙˆØ¹ 2 - Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠ:**
```
ğŸ“‹ **Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù€ [Ø§Ù„Ù‡Ø¯Ù]**

**Ø§Ù„Ø®Ø·ÙˆØ© 1**: [Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„]
**Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨**: [Ø§Ù„Ù…Ø¯Ø©]
**Ø§Ù„ØªÙƒÙ„ÙØ©**: [Ø¥Ù† ÙˆØ¬Ø¯Øª]

**Ø§Ù„Ø®Ø·ÙˆØ© 2**: [Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„]
**Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨**: [Ø§Ù„Ù…Ø¯Ø©]
**Ø§Ù„ØªÙƒÙ„ÙØ©**: [Ø¥Ù† ÙˆØ¬Ø¯Øª]

**Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:**
â€¢ [Ù…Ø³ØªÙ†Ø¯ 1] - [ÙƒÙŠÙÙŠØ© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„ÙŠÙ‡]
â€¢ [Ù…Ø³ØªÙ†Ø¯ 2] - [ÙƒÙŠÙÙŠØ© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„ÙŠÙ‡]

**Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ©:**
â€¢ [Ø§Ø³Ù… Ø§Ù„Ø¬Ù‡Ø©] - [Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„]
â€¢ [Ø§Ø³Ù… Ø§Ù„Ø¬Ù‡Ø©] - [Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„]

**Ù†ØµØ§Ø¦Ø­ Ù…Ù‡Ù…Ø©:**
â€¢ [Ù†ØµÙŠØ­Ø© Ø¹Ù…Ù„ÙŠØ©]
â€¢ [ØªØ­Ø°ÙŠØ± Ù…Ù† Ø®Ø·Ø£ Ø´Ø§Ø¦Ø¹]
```

**Ù†ÙˆØ¹ 3 - Ø§Ù„Ù…Ø°ÙƒØ±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
```
### Ù…Ø°ÙƒØ±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
**Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹**: [Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù‚Ø¶ÙŠØ©]
**Ø§Ù„Ù…Ù‚Ø¯Ù… Ù…Ù†**: [Ø§Ø³Ù… Ø§Ù„Ø·Ø±Ù]
**Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¶Ø¯**: [Ø§Ø³Ù… Ø§Ù„Ø·Ø±Ù Ø§Ù„Ø¢Ø®Ø±]

## Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹
[Ø³Ø±Ø¯ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø·Ù‚ÙŠ ÙˆÙ…Ø±ØªØ¨]

## Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
**Ø§Ù„Ø­Ø¬Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰**: [Ø§Ù„ØªÙØµÙŠÙ„ ÙˆØ§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ]
**Ø§Ù„Ø­Ø¬Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©**: [Ø§Ù„ØªÙØµÙŠÙ„ ÙˆØ§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ]

## Ø§Ù„Ø·Ù„Ø¨Ø§Øª
Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø§ ØªÙ‚Ø¯Ù… Ù†Ø·Ù„Ø¨:
1. [Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø£ÙˆÙ„]
2. [Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø«Ø§Ù†ÙŠ]
```

ğŸ¯ **Ø§Ø®ØªØ± Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„Ø°ÙŠ Ù‚Ø¯Ù…Ù‡ Ù…Ø­Ù„Ù„ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹**""",

            LegalAgentType.CITATION_VALIDATOR: """Ø£Ù†Øª Ù…Ø±Ø§Ø¬Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ø§Øª ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰.

ğŸ¯ **Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰:**

**Ù„Ø´Ø±Ø­ Ø§Ù„Ø­Ù‚ÙˆÙ‚:**
- ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø©
- ØªØ£ÙƒØ¯ Ù…Ù† Ø±Ø¨Ø· Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø¨Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØµØ­ÙŠØ­Ø©
- Ø±Ø§Ø¬Ø¹ Ø¯Ù‚Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ©

**Ù„Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©:**
- ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø©
- Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ ÙˆØ§Ù„Ù…Ù‡Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- ØªØ£ÙƒØ¯ Ù…Ù† Ø¯Ù‚Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ù‡Ø§Øª ÙˆØ§Ù„Ø±Ø³ÙˆÙ…

**Ù„Ù„Ù…Ø°ÙƒØ±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
- ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- Ø±Ø§Ø¬Ø¹ Ø¯Ù‚Ø© Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯ ÙˆØ§Ù„Ø£Ù†Ø¸Ù…Ø©
- ØªØ£ÙƒØ¯ Ù…Ù† Ù‚ÙˆØ© Ø§Ù„Ø­Ø¬Ø¬ ÙˆØ§Ù„Ø£Ø¯Ù„Ø©

Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©:
- Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù†Øµ Ø§Ù„ÙØ¹Ù„ÙŠ
- Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… Ø§Ù„Ù†Ø³Ø® Ø£Ùˆ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„
- ØªÙ‚ÙŠÙŠÙ… Ù…Ø¯Ù‰ Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹
- Ø¥Ø¶Ø§ÙØ© ØªØ­Ø°ÙŠØ±Ø§Øª Ù„Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø´ÙƒÙˆÙƒ ÙÙŠÙ‡Ø§
- ØªÙ‚Ø¯ÙŠØ± Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (Ø¹Ø§Ù„ÙŠ/Ù…ØªÙˆØ³Ø·/Ù…Ù†Ø®ÙØ¶)"""
        }
    
    async def process(self, input_data: str, context: Optional[str] = None) -> Tuple[str, List[str], float]:
        """Process input through specialized agent"""
        system_prompt = self.system_prompts[self.agent_type]
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": input_data}
        ]
        
        if context:
            messages.insert(-1, {"role": "user", "content": f"Ø§Ù„Ø³ÙŠØ§Ù‚: {context}"})
        
        try:
            # Determine model based on client type
            model = "gpt-4o" if "openai" in str(self.client.base_url) else "deepseek-chat"
            
            response = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.3,  # Lower temperature for legal precision
                max_tokens=2000
            )
            
            content = response.choices[0].message.content
            
            # Extract citations (simple pattern matching for now)
            citations = self._extract_citations(content)
            
            # Calculate confidence based on citation count and content length
            confidence = self._calculate_confidence(content, citations)
            
            return content, citations, confidence
            
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {self.agent_type.value}: {str(e)}", [], 0.0

    async def process_streaming(self, input_data: str, context: Optional[str] = None) -> AsyncIterator[str]:
        """Process input through specialized agent with real-time streaming"""
        system_prompt = self.system_prompts[self.agent_type]
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": input_data}
        ]
        
        if context:
            messages.insert(-1, {"role": "user", "content": f"Ø§Ù„Ø³ÙŠØ§Ù‚: {context}"})
        
        try:
            # Determine model based on client type
            model = "gpt-4o" if "openai" in str(self.client.base_url) else "deepseek-chat"
            
            # ğŸš€ ADD STREAMING:
            stream = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.3,
                max_tokens=2000,
                stream=True  # â† This enables real streaming!
            )
            
            # ğŸš€ STREAM TOKEN BY TOKEN:
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            yield f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {self.agent_type.value}: {str(e)}"
    
    def _extract_citations(self, content: str) -> List[str]:
        """Extract legal citations from content"""
        citations = []
        
        # Pattern for Saudi legal references
        import re
        
        # Pattern for "Ø§Ù„Ù…Ø§Ø¯Ø© X Ù…Ù† Ù†Ø¸Ø§Ù… Y"
        article_pattern = r'Ø§Ù„Ù…Ø§Ø¯Ø©\s+(\d+)\s+Ù…Ù†\s+Ù†Ø¸Ø§Ù…\s+([^ØŒ\s]+(?:\s+[^ØŒ\s]+)*)'
        matches = re.findall(article_pattern, content)
        for match in matches:
            citations.append(f"Ø§Ù„Ù…Ø§Ø¯Ø© {match[0]} Ù…Ù† Ù†Ø¸Ø§Ù… {match[1]}")
        
        # Pattern for "Ø§Ù„Ù‚Ø±Ø§Ø± Ø±Ù‚Ù… X"
        decision_pattern = r'Ø§Ù„Ù‚Ø±Ø§Ø±\s+Ø±Ù‚Ù…\s+(\d+(?:/\d+)?)'
        matches = re.findall(decision_pattern, content)
        for match in matches:
            citations.append(f"Ø§Ù„Ù‚Ø±Ø§Ø± Ø±Ù‚Ù… {match}")
        
        return citations
    
    def _calculate_confidence(self, content: str, citations: List[str]) -> float:
        """Calculate confidence score based on content analysis"""
        base_confidence = 0.7
        
        # Boost confidence for legal citations
        citation_boost = min(len(citations) * 0.05, 0.2)
        
        # Boost confidence for structured content
        structure_indicators = ["Ø£ÙˆÙ„Ø§Ù‹", "Ø«Ø§Ù†ÙŠØ§Ù‹", "Ø«Ø§Ù„Ø«Ø§Ù‹", "Ø§Ù„Ø®Ù„Ø§ØµØ©", "Ø§Ù„ØªÙˆØµÙŠØ©"]
        structure_boost = sum(0.02 for indicator in structure_indicators if indicator in content)
        structure_boost = min(structure_boost, 0.1)
        
        # Penalize for uncertainty words
        uncertainty_words = ["Ø±Ø¨Ù…Ø§", "ÙŠÙ…ÙƒÙ† Ø£Ù†", "Ù‚Ø¯ ÙŠÙƒÙˆÙ†", "ØºÙŠØ± ÙˆØ§Ø¶Ø­"]
        uncertainty_penalty = sum(0.05 for word in uncertainty_words if word in content)
        uncertainty_penalty = min(uncertainty_penalty, 0.2)
        
        final_confidence = base_confidence + citation_boost + structure_boost - uncertainty_penalty
        return max(0.1, min(1.0, final_confidence))

class IntentValidationAgent:
    """
    ğŸ¯ INTENT VALIDATION AGENT: Checks if initial classification matches actual query complexity
    """
    
    def __init__(self, openai_client):
        self.client = openai_client
    
    async def validate_intent(
        self, 
        original_query: str, 
        fact_analysis: str, 
        initial_classification: Dict[str, str]
    ) -> Dict[str, any]:
        """Validate if initial classification matches what fact analysis revealed"""
        
        validation_prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©.

**Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø§Ù„Ø£ØµÙ„ÙŠ:**
{original_query}

**Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ÙˆÙ„ÙŠ:**
{initial_classification.get('intent', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')} (Ø«Ù‚Ø©: {initial_classification.get('confidence', 0):.1%})

**ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ Ø§Ù„Ø°ÙŠ ØªÙ…:**
{fact_analysis}

**Ù…Ù‡Ù…ØªÙƒ:**
Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ØŒ Ù‡Ù„ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ÙˆÙ„ÙŠ ØµØ­ÙŠØ­ Ø£Ù… ÙŠØ­ØªØ§Ø¬ ØªØ¹Ø¯ÙŠÙ„ØŸ

Ø£Ø¬Ø¨ Ø¨Ù€ JSON ÙÙ‚Ø·:
{{
  "validation_result": "confirmed|modified|expanded",
  "recommended_intents": ["intent1", "intent2"],
  "complexity_level": "simple|complex",
  "reasoning": "Ø³Ø¨Ø¨ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„",
  "confidence": 0.9
}}"""

        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": validation_prompt}],
                temperature=0.1,
                max_tokens=300
            )
            
            validation_text = response.choices[0].message.content.strip()
            
            import json
            validation_result = json.loads(validation_text)
            
            print(f"ğŸ” Intent validation: {validation_result.get('validation_result')} â†’ {validation_result.get('recommended_intents')}")
            
            return validation_result
            
        except Exception as e:
            print(f"âš ï¸ Intent validation failed: {e}")
            
            # Smart fallback
            return {
                "validation_result": "confirmed",
                "recommended_intents": [initial_classification.get('intent', 'legal_consultation')],
                "complexity_level": "simple",
                "reasoning": "Fallback validation",
                "confidence": 0.7
            }



class MultiAgentLegalOrchestrator:
    """
    ğŸ§  CORE ORCHESTRATOR: Sequential Pipeline with Smart Intent Classification
    
    Implements intelligent approach:
    - AI-powered intent classification
    - Dynamic agent prompts based on intent
    - Real-time streaming during processing
    - Appropriate response formatting
    """
    
    def __init__(self, openai_client):  # â† THIS IS WHERE YOU ADD STUFF
        self.client = openai_client
        self.agents = {
            agent_type: LegalAgent(agent_type, openai_client) 
            for agent_type in LegalAgentType
        }
        self.clarification_controller = AdvancedClarificationController(openai_client)
        self.content_merger = EliteContentMerger()
        self.complexity_system = ComplexityAwareAgentSystem()
        self.intent_validator = IntentValidationAgent(openai_client)
        

    async def classify_intent(self, query: str, context: Optional[List[Dict]] = None) -> Dict[str, str]:
        """
        ğŸ¯ Smart AI-powered intent classification for dynamic response formatting
        Cost: ~200 tokens (~$0.001 per query)
        """
        
        # Build context summary if available
        context_info = ""
        if context and len(context) > 0:
            recent_messages = context[-3:] if len(context) > 3 else context
            context_info = f"\nContext from conversation: {' | '.join([msg.get('content', '')[:50] for msg in recent_messages])}"
        
        classification_prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

Ø§Ù„Ù…Ù‡Ù…Ø©: ØµÙ†Ù Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¥Ù„Ù‰ ÙØ¦Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·.

Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±: "{query}"{context_info}

Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:
1. rights_inquiry - Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (Ù…Ø«Ù„: "Ù…Ø§ Ø­Ù‚ÙˆÙ‚ÙŠØŸ", "Ù…Ø§ Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…ÙˆØ¸ÙØŸ")
2. procedure_guide - Ø³Ø¤Ø§Ù„ Ø¹Ù† ÙƒÙŠÙÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø¥Ø¬Ø±Ø§Ø¡ Ù‚Ø§Ù†ÙˆÙ†ÙŠ (Ù…Ø«Ù„: "ÙƒÙŠÙ Ø£Ø³Ø³ Ø´Ø±ÙƒØ©ØŸ", "Ù…Ø§ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø·Ù„Ø§Ù‚ØŸ")
3. legal_dispute - Ù…Ø´ÙƒÙ„Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© ØªØ­ØªØ§Ø¬ Ø­Ù„ (Ù…Ø«Ù„: "ØªÙ… ÙØµÙ„ÙŠ", "Ø£Ø±ÙŠØ¯ Ù…Ù‚Ø§Ø¶Ø§Ø©", "Ù„Ø¯ÙŠ Ù†Ø²Ø§Ø¹")
4. legal_consultation - Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¹Ø§Ù…Ø© Ø£Ùˆ Ø±Ø£ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ (Ù…Ø«Ù„: "Ù…Ø§ Ø±Ø£ÙŠÙƒ ÙÙŠØŸ", "Ù‡Ù„ ÙŠØ¬ÙˆØ²ØŸ")
5. document_review - Ù…Ø±Ø§Ø¬Ø¹Ø© Ø£Ùˆ ØªÙØ³ÙŠØ± ÙˆØ«ÙŠÙ‚Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (Ù…Ø«Ù„: "Ø§Ø´Ø±Ø­ Ù„ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù‚Ø¯", "Ù…Ø§ Ù…Ø¹Ù†Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ù†Ø¯ØŸ")
6. comparative_analysis - Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ† Ø®ÙŠØ§Ø±Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (Ù…Ø«Ù„: "Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ†ØŸ", "Ø£ÙŠÙ‡Ù…Ø§ Ø£ÙØ¶Ù„ØŸ")

Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙ:
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ¨Ø¯Ø£ Ø¨Ù€ "Ù…Ø§ Ø­Ù‚ÙˆÙ‚" Ø£Ùˆ "Ù…Ø§ Ø­Ù‚ÙŠ" â†’ rights_inquiry
- Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¨Ø¯Ø£ Ø¨Ù€ "ÙƒÙŠÙ" Ø£Ùˆ "Ù…Ø§ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª" â†’ procedure_guide  
- Ø¥Ø°Ø§ Ø°ÙƒØ± Ù…Ø´ÙƒÙ„Ø© Ø­Ø¯Ø«Øª ("ØªÙ…", "Ø­ØµÙ„", "Ù‚Ø§Ù…") â†’ legal_dispute
- Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ·Ù„Ø¨ Ø±Ø£ÙŠ Ø£Ùˆ ØªÙ‚ÙŠÙŠÙ… â†’ legal_consultation

Ø£Ø¬Ø¨ Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON ÙÙ‚Ø·:
{{
  "intent": "Ø§Ù„ÙØ¦Ø©_Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©",
  "confidence": 0.95,
  "reasoning": "Ø³Ø¨Ø¨ Ø§Ù„ØªØµÙ†ÙŠÙ",
  "suggested_format": "Ù†ÙˆØ¹_Ø§Ù„ØªÙ†Ø³ÙŠÙ‚_Ø§Ù„Ù…Ù†Ø§Ø³Ø¨"
}}"""

        try:
            # Use a lightweight model call for classification
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini" if "openai" in str(self.client.base_url) else "deepseek-chat",
                messages=[{"role": "user", "content": classification_prompt}],
                temperature=0.1,  # Low temperature for consistent classification
                max_tokens=200    # Small response for cost efficiency
            )
            
            classification_text = response.choices[0].message.content.strip()
            
            # Parse JSON response
            import json
            classification = json.loads(classification_text)
            
            print(f"ğŸ¯ Intent classified: {classification.get('intent')} (confidence: {classification.get('confidence', 0)})")
            
            return classification
            
        except Exception as e:
            print(f"âš ï¸ Intent classification failed: {e}")
            # Fallback to simple keyword detection
            query_lower = query.lower()
            
            if any(word in query_lower for word in ["Ø­Ù‚ÙˆÙ‚", "Ø­Ù‚ÙŠ", "Ø­Ù‚Ùƒ"]):
                return {
                    "intent": "rights_inquiry",
                    "confidence": 0.7,
                    "reasoning": "Keyword-based fallback detection",
                    "suggested_format": "rights_explanation"
                }
            elif any(word in query_lower for word in ["ÙƒÙŠÙ", "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª", "Ø®Ø·ÙˆØ§Øª"]):
                return {
                    "intent": "procedure_guide", 
                    "confidence": 0.7,
                    "reasoning": "Keyword-based fallback detection",
                    "suggested_format": "step_by_step_guide"
                }
            elif any(word in query_lower for word in ["ØªÙ…", "ÙØµÙ„", "Ù…Ù‚Ø§Ø¶Ø§Ø©", "Ù†Ø²Ø§Ø¹", "Ù…Ø´ÙƒÙ„Ø©"]):
                return {
                    "intent": "legal_dispute",
                    "confidence": 0.7, 
                    "reasoning": "Keyword-based fallback detection",
                    "suggested_format": "legal_memorandum"
                }
            else:
                return {
                    "intent": "legal_consultation",
                    "confidence": 0.6,
                    "reasoning": "Default classification",
                    "suggested_format": "general_consultation"
                }

    async def get_dynamic_agent_prompts(self, intent_classification: Dict[str, str]) -> Dict[LegalAgentType, str]:
        """
        ğŸ¯ Generate dynamic agent prompts based on intent classification
        """
        
        intent = intent_classification.get("intent", "legal_consultation")
        
        # Base prompts that adapt to intent
        base_prompts = {
            LegalAgentType.FACT_ANALYZER: f"""Ø£Ù†Øª Ù…Ø­Ù„Ù„ ÙˆÙ‚Ø§Ø¦Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ.

ØªÙ… ØªØµÙ†ÙŠÙ Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± ÙƒÙ€: {intent}

Ù…Ù‡Ù…ØªÙƒ Ø­Ø³Ø¨ Ø§Ù„ØªØµÙ†ÙŠÙ:
- rights_inquiry: Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© ÙˆØ§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„Ø©
- procedure_guide: Ø­Ø¯Ø¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙˆØ§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØ§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯
- legal_dispute: Ø­Ù„Ù„ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ§Ù„Ø£Ø·Ø±Ø§Ù ÙˆØ§Ù„Ø£Ø¶Ø±Ø§Ø±
- legal_consultation: Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø´Ø§Ù…Ù„Ø§Ù‹ Ù„Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ
- document_review: Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ù† Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©
- comparative_analysis: Ø­Ø¯Ø¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ÙˆØ§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ ÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹ Ø¨Ø¯ÙˆÙ† Ø°ÙƒØ± Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©.""",

            LegalAgentType.DOCUMENT_DRAFTER: f"""Ø£Ù†Øª Ù…Ø­Ø§Ù… Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØµÙŠØ§ØºØ© Ø±Ø¯ÙˆØ¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…ØªÙƒÙŠÙØ©.

Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±: {intent}

Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø±Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹:

**rights_inquiry - Ø´Ø±Ø­ Ø§Ù„Ø­Ù‚ÙˆÙ‚:**
```
ğŸ” Ø­Ù‚ÙˆÙ‚Ùƒ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙÙŠ [Ø§Ù„Ù…Ø¬Ø§Ù„]

**Ø§Ù„Ø­Ù‚ Ø§Ù„Ø£ÙˆÙ„**: [Ø´Ø±Ø­ Ù…ÙØµÙ„]
**Ø§Ù„Ù…Ø±Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ**: [Ø§Ù„Ù…Ø§Ø¯Ø© ÙˆØ§Ù„Ù†Ø¸Ø§Ù…]

**ÙƒÙŠÙÙŠØ© Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©**:
â€¢ [Ø®Ø·ÙˆØ§Øª Ø¹Ù…Ù„ÙŠØ©]

**Ø¬Ù‡Ø§Øª Ø§Ù„Ø­Ù…Ø§ÙŠØ©**:
â€¢ [Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ©]
```

**procedure_guide - Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª:**
```
ğŸ“‹ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù€ [Ø§Ù„Ù‡Ø¯Ù]

**Ø§Ù„Ø®Ø·ÙˆØ© 1**: [ØªÙØµÙŠÙ„ ÙƒØ§Ù…Ù„]
â€¢ Ø§Ù„Ù…Ø¯Ø©: [Ø§Ù„ÙˆÙ‚Øª]
â€¢ Ø§Ù„ØªÙƒÙ„ÙØ©: [Ø§Ù„Ù…Ø¨Ù„Øº]

**Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©**:
â€¢ [Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª]

**Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ©**:
â€¢ [Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„]
```

**legal_dispute - Ù…Ø°ÙƒØ±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
```
âš–ï¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù„Ù…ÙˆÙ‚Ù

**Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹**:
[Ø³Ø±Ø¯ Ù…Ù†Ø·Ù‚ÙŠ]

**Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**:
[Ø§Ù„Ø­Ø¬Ø¬ Ù…Ø¹ Ø§Ù„Ø£Ø¯Ù„Ø©]

**Ø§Ù„ØªÙˆØµÙŠØ§Øª**:
[Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©]
```

**legal_consultation - Ø§Ø³ØªØ´Ø§Ø±Ø© Ø¹Ø§Ù…Ø©:**
```
ğŸ’¡ Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

**Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ**:
[ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„]

**Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©**:
[Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©]

**Ø§Ù„ØªÙˆØµÙŠØ§Øª**:
[Ø§Ù„Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©]
```

Ø§Ø®ØªØ± Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙ†ÙŠÙ."""
        }
        
        # Return only our dynamic prompts
        return base_prompts
    
    def _generate_focused_prompt(self, intent: str, query: str, complexity_level: str) -> str:
        """Generate laser-focused prompts with CTA for simple queries, detailed for complex"""
    
        # FOR SIMPLE QUERIES: Use ultra-focused format with CTA
        if complexity_level == "simple":
            
            if intent == "penalty_explanation":
                return f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¹Ù‚ÙˆØ¨Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ: Ø¥Ø¬Ø§Ø¨Ø© Ø³Ø±ÙŠØ¹Ø© + Ø¯Ø¹ÙˆØ© Ù„Ù„ØªÙØ§ØµÙŠÙ„.

    **Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©:**
    - Ø£Ù‚ØµÙ‰ Ø­Ø¯: 80 ÙƒÙ„Ù…Ø©
    - Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙ‚Ø·
    - Ø±Ù‚Ù… Ù„Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„ÙÙˆØ±ÙŠ
    - CTA ÙˆØ§Ø¶Ø­ Ù„Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„Ø©

    **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
    âš–ï¸ **Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©**: [Ø§Ù„Ù…Ø¨Ù„Øº/Ø§Ù„Ù†ÙˆØ¹ Ø¨Ø¥Ø®ØªØµØ§Ø±]
    ğŸ“ **Ù„Ù„ØªØ³ÙˆÙŠØ© Ø§Ù„ÙÙˆØ±ÙŠØ©**: [Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯]
    â¡ï¸ **Ù„Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„Ø© ÙˆØ®Ø·Ø© Ø§Ù„Ø¹Ù…Ù„**: Ø§ÙƒØªØ¨ "Ø£Ø±ÙŠØ¯ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„Ø©"

    Ø§Ù„Ø³Ø¤Ø§Ù„: {query}"""

            elif intent == "rights_inquiry":
                return f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø­Ù‚ÙˆÙ‚ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ: Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© + Ø¯Ø¹ÙˆØ© Ù„Ù„ØªÙØ§ØµÙŠÙ„.

    **Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©:**
    - Ø£Ù‚ØµÙ‰ Ø­Ø¯: 80 ÙƒÙ„Ù…Ø©
    - Ø£Ù‡Ù… 2-3 Ø­Ù‚ÙˆÙ‚ ÙÙ‚Ø·
    - Ø¬Ù‡Ø© Ù„Ù„Ù…Ø·Ø§Ù„Ø¨Ø©
    - CTA Ù„Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„

    **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
    ğŸ” **Ø­Ù‚ÙˆÙ‚Ùƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©**: [2-3 Ø­Ù‚ÙˆÙ‚ Ù…Ø®ØªØµØ±Ø© Ø¨Ù†Ù‚Ø§Ø·]
    ğŸ“ **Ù„Ù„Ù…Ø·Ø§Ù„Ø¨Ø© ÙÙˆØ±Ø§Ù‹**: [Ø§Ù„Ø¬Ù‡Ø© + Ø§Ù„Ø±Ù‚Ù…]
    â¡ï¸ **Ù„Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ ÙˆØ·Ø±Ù‚ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©**: Ø§ÙƒØªØ¨ "Ø£Ø±ÙŠØ¯ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„"

    Ø§Ù„Ø³Ø¤Ø§Ù„: {query}"""

            elif intent == "procedure_guide":
                return f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ: Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© + Ø¯Ø¹ÙˆØ© Ù„Ù„ØªÙØ§ØµÙŠÙ„.

    **Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©:**
    - Ø£Ù‚ØµÙ‰ Ø­Ø¯: 80 ÙƒÙ„Ù…Ø©
    - 3 Ø®Ø·ÙˆØ§Øª Ø£Ø³Ø§Ø³ÙŠØ© ÙÙ‚Ø·
    - Ø¬Ù‡Ø© Ù„Ù„Ø¨Ø¯Ø¡
    - CTA Ù„Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„

    **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
    ğŸ“‹ **Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©**:
    1ï¸âƒ£ [Ø®Ø·ÙˆØ© Ù…Ø®ØªØµØ±Ø©]
    2ï¸âƒ£ [Ø®Ø·ÙˆØ© Ù…Ø®ØªØµØ±Ø©]  
    3ï¸âƒ£ [Ø®Ø·ÙˆØ© Ù…Ø®ØªØµØ±Ø©]
    ğŸ“ **Ø§Ø¨Ø¯Ø£ Ù…Ù†**: [Ø§Ù„Ø¬Ù‡Ø© + Ø§Ù„Ø±Ù‚Ù…]
    â¡ï¸ **Ù„Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ ÙˆØ§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª**: Ø§ÙƒØªØ¨ "Ø£Ø±ÙŠØ¯ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„"

    Ø§Ù„Ø³Ø¤Ø§Ù„: {query}"""
            
            else:  # legal_consultation
                return f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ. Ù…Ù‡Ù…ØªÙƒ: Ø¥Ø¬Ø§Ø¨Ø© Ø³Ø±ÙŠØ¹Ø© + Ø¯Ø¹ÙˆØ© Ù„Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©.

    **Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©:**
    - Ø£Ù‚ØµÙ‰ Ø­Ø¯: 80 ÙƒÙ„Ù…Ø©
    - Ø±Ø£ÙŠ Ù…Ø®ØªØµØ± + Ø®Ø·ÙˆØ© ÙˆØ§Ø­Ø¯Ø©
    - Ø¬Ù‡Ø© Ù„Ù„ØªÙˆØ§ØµÙ„
    - CTA Ù„Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©

    **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
    ğŸ’¡ **Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ù…Ø®ØªØµØ±**: [Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆØ§Ø¶Ø­Ø©]
    âš¡ **Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©**: [Ø¥Ø¬Ø±Ø§Ø¡ ÙˆØ§Ø­Ø¯ Ù…Ø­Ø¯Ø¯]
    ğŸ“ **Ù„Ù„Ø§Ø³ØªØ´Ø§Ø±Ø©**: [Ø§Ù„Ø¬Ù‡Ø© + Ø§Ù„Ø±Ù‚Ù…]
    â¡ï¸ **Ù„Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©**: Ø§ÙƒØªØ¨ "Ø£Ø±ÙŠØ¯ Ø§Ø³ØªØ´Ø§Ø±Ø© Ù…ÙØµÙ„Ø©"

    Ø§Ù„Ø³Ø¤Ø§Ù„: {query}"""
        
        # FOR COMPLEX QUERIES: Use detailed prompts immediately
        else:
            
            if intent == "legal_dispute":
                return f"""Ø£Ù†Øª Ù…Ø­Ø§Ù… Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¯ÙØ§Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ. Ù…Ù‡Ù…ØªÙƒ: Ø¥Ù†ØªØ§Ø¬ Ø®Ø·Ø© Ø¯ÙØ§Ø¹ Ø´Ø§Ù…Ù„Ø© ÙˆØ¹Ù…Ù„ÙŠØ©.

    **Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙØµÙŠÙ„:**
    - ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„: 300-500 ÙƒÙ„Ù…Ø©
    - Ø®Ø·Ø© Ø¯ÙØ§Ø¹ Ù…ØªÙƒØ§Ù…Ù„Ø©
    - Ø£Ø¯Ù„Ø© Ù…Ø­Ø¯Ø¯Ø© Ù…Ø·Ù„ÙˆØ¨Ø©
    - Ø¬Ø¯ÙˆÙ„ Ø²Ù…Ù†ÙŠ Ù„Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª

    **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
    ğŸ›¡ï¸ **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¯ÙØ§Ø¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©**: [ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ]
    ğŸ“‹ **Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©**: [Ù‚Ø§Ø¦Ù…Ø© Ù…ÙØµÙ„Ø© Ø¨Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ§Ù„Ø´Ù‡ÙˆØ¯]
    âš–ï¸ **Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**: [Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ø³ÙˆØ§Ø¨Ù‚ Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©]
    ğŸ“… **Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ**: [Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø­Ø±Ø¬Ø© ÙˆØ§Ù„Ù…Ù‡Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©]
    ğŸ’° **Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©**: [ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø±Ø³ÙˆÙ… ÙˆØ§Ù„Ø£ØªØ¹Ø§Ø¨]
    ğŸ“ **Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ÙÙˆØ±ÙŠØ©**: [Ù…Ø§ ÙŠØ¬Ø¨ ÙØ¹Ù„Ù‡ Ø®Ù„Ø§Ù„ 24-48 Ø³Ø§Ø¹Ø©]
    ğŸ¯ **ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ù†ØªØ§Ø¦Ø¬**: [Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„Ù†Ø¬Ø§Ø­ ÙˆØ§Ù„Ø¨Ø¯Ø§Ø¦Ù„]

    Ø§Ù„Ø³Ø¤Ø§Ù„: {query}"""

            elif intent == "penalty_explanation":
                return f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ: ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª Ù…Ø¹ Ø®Ø·Ø© Ø§Ù„ØªØ¹Ø§Ù…Ù„.

    **Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙØµÙŠÙ„:**
    - ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„: 250-400 ÙƒÙ„Ù…Ø©
    - Ø¹Ù‚ÙˆØ¨Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
    - Ø®Ø·Ø© ØªØ®ÙÙŠÙ Ø´Ø§Ù…Ù„Ø©
    - Ø¨Ø¯Ø§Ø¦Ù„ ÙˆØ­Ù„ÙˆÙ„

    **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
    âš–ï¸ **Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©**: [Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª Ø¨Ø§Ù„ØªÙØµÙŠÙ„ ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù…]
    ğŸ“Š **ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø±**: [Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª ØªØ·Ø¨ÙŠÙ‚ ÙƒÙ„ Ø¹Ù‚ÙˆØ¨Ø©]
    ğŸ“‹ **Ø®Ø·Ø© Ø§Ù„ØªØ®ÙÙŠÙ**: [Ø®Ø·ÙˆØ§Øª ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª]
    ğŸ’° **Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ Ø§Ù„Ù…Ø§Ù„ÙŠØ©**: [Ø­Ø³Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ù„ØºØ±Ø§Ù…Ø§Øª ÙˆØ§Ù„Ø±Ø³ÙˆÙ…]
    â° **Ø§Ù„Ù…Ù‡Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠØ©**: [Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø­Ø±Ø¬Ø© Ù„Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©]
    ğŸ“ **Ø¬Ù‡Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„**: [Ø£Ø±Ù‚Ø§Ù… Ù…Ø­Ø¯Ø¯Ø© Ù„ÙƒÙ„ Ø¥Ø¬Ø±Ø§Ø¡]
    ğŸ¯ **Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©**: [Ø£ÙØ¶Ù„ Ø­Ø§Ù„Ø© ÙˆØ£Ø³ÙˆØ£ Ø­Ø§Ù„Ø©]

    Ø§Ù„Ø³Ø¤Ø§Ù„: {query}"""

            elif intent == "rights_inquiry":
                return f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ: Ø¯Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø­Ù‚ÙˆÙ‚ Ù…Ø¹ Ø¢Ù„ÙŠØ§Øª Ø§Ù„Ø­Ù…Ø§ÙŠØ©.

    **Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙØµÙŠÙ„:**
    - Ø¯Ù„ÙŠÙ„ Ù…ÙØµÙ„: 300-450 ÙƒÙ„Ù…Ø©
    - Ø­Ù‚ÙˆÙ‚ Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ù„Ù†ØµÙˆØµ
    - Ø¢Ù„ÙŠØ§Øª Ø­Ù…Ø§ÙŠØ© Ø¹Ù…Ù„ÙŠØ©
    - Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©

    **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
    ğŸ” **Ø­Ù‚ÙˆÙ‚Ùƒ Ø§Ù„Ù…ÙØµÙ„Ø©**: [Ù‚Ø§Ø¦Ù…Ø© Ø´Ø§Ù…Ù„Ø© Ù…Ø¹ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©]
    âš–ï¸ **Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ**: [Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©]
    ğŸ›¡ï¸ **Ø¢Ù„ÙŠØ§Øª Ø§Ù„Ø­Ù…Ø§ÙŠØ©**: [Ø§Ù„Ø¬Ù‡Ø§Øª ÙˆØ§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©]
    ğŸ“‹ **Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©**: [Ø¯Ù„ÙŠÙ„ Ø¹Ù…Ù„ÙŠ Ù…Ø±Ø­Ù„ÙŠ]
    ğŸ’° **Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ ÙˆØ§Ù„Ø±Ø³ÙˆÙ…**: [ØªÙƒÙ„ÙØ© Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ø¨ÙƒÙ„ Ø­Ù‚]
    â° **Ø§Ù„Ù…Ù‡Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**: [Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© ÙˆØ§Ù„ØªÙ‚Ø§Ø¯Ù…]
    ğŸ“ **Ø¬Ù‡Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„**: [Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙØµÙŠÙ„ÙŠØ© Ù„Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ©]

    Ø§Ù„Ø³Ø¤Ø§Ù„: {query}"""

            elif intent == "procedure_guide":
                return f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ: Ø¯Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ Ø´Ø§Ù…Ù„ Ù„Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª.

    **Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙØµÙŠÙ„:**
    - Ø¯Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„: 350-500 ÙƒÙ„Ù…Ø©
    - Ø®Ø·ÙˆØ§Øª Ù…ÙØµÙ„Ø© Ø¨Ø§Ù„ØªØ³Ù„Ø³Ù„
    - Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆÙ…ØªØ·Ù„Ø¨Ø§Øª ÙƒØ§Ù…Ù„Ø©
    - ØªÙƒØ§Ù„ÙŠÙ ÙˆÙ…ÙˆØ§Ø¹ÙŠØ¯ Ø¯Ù‚ÙŠÙ‚Ø©

    **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
    ğŸ“‹ **Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©**: [Ø®Ø·ÙˆØ§Øª Ù…Ø±Ù‚Ù…Ø© Ù…Ø¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„Ø©]
    ğŸ“„ **Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©**: [Ù‚Ø§Ø¦Ù…Ø© Ø´Ø§Ù…Ù„Ø© Ù…Ø¹ Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø­ØµÙˆÙ„]
    ğŸ’° **Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©**: [Ø±Ø³ÙˆÙ… ÙƒÙ„ Ù…Ø±Ø­Ù„Ø© ÙˆÙ…ØµØ§Ø±ÙŠÙ Ø¥Ø¶Ø§ÙÙŠØ©]
    â° **Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ**: [Ù…Ø¯Ø© ÙƒÙ„ Ø¥Ø¬Ø±Ø§Ø¡ ÙˆØ§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø­Ø±Ø¬Ø©]
    ğŸ›ï¸ **Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ©**: [Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙØµÙŠÙ„ÙŠØ© ÙˆØ£Ø±Ù‚Ø§Ù… Ø§Ù„ØªÙˆØ§ØµÙ„]
    âš ï¸ **ØªØ­Ø°ÙŠØ±Ø§Øª Ù…Ù‡Ù…Ø©**: [Ø£Ø®Ø·Ø§Ø¡ Ø´Ø§Ø¦Ø¹Ø© ÙˆÙƒÙŠÙÙŠØ© ØªØ¬Ù†Ø¨Ù‡Ø§]
    ğŸ¯ **Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ù†Ø¬Ø§Ø­**: [Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø¹Ù…Ù„ÙŠØ© Ù„Ø¶Ù…Ø§Ù† Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ø·Ù„Ø¨]

    Ø§Ù„Ø³Ø¤Ø§Ù„: {query}"""

            else:  # legal_consultation - complex
                return f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ: Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…ØªØ®ØµØµØ©.

    **Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙØµÙŠÙ„:**
    - Ø§Ø³ØªØ´Ø§Ø±Ø© Ø´Ø§Ù…Ù„Ø©: 300-450 ÙƒÙ„Ù…Ø©
    - ØªØ­Ù„ÙŠÙ„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ¹Ù…Ù‚
    - Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
    - ØªÙˆØµÙŠØ§Øª Ù…Ø­Ø¯Ø¯Ø©

    **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
    ğŸ’¡ **Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ**: [ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ]
    âš–ï¸ **Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©**: [Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø¹ Ù…Ø²Ø§ÙŠØ§ ÙˆØ¹ÙŠÙˆØ¨ ÙƒÙ„ Ø®ÙŠØ§Ø±]
    ğŸ“Š **ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø±**: [ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ù„ÙƒÙ„ Ø®ÙŠØ§Ø±]
    ğŸ“‹ **Ø§Ù„ØªÙˆØµÙŠØ§Øª**: [Ø£ÙØ¶Ù„ Ù…Ø³Ø§Ø± Ø¹Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø¨Ø±Ø±Ø§Øª]
    ğŸ’° **Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„ØªÙƒÙ„ÙØ©**: [ØªÙ‚Ø¯ÙŠØ± ØªÙƒØ§Ù„ÙŠÙ ÙƒÙ„ Ø®ÙŠØ§Ø±]
    â° **Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø²Ù…Ù†ÙŠ**: [Ø¬Ø¯ÙˆÙ„ Ø²Ù…Ù†ÙŠ Ù„Ù„ØªÙ†ÙÙŠØ°]
    ğŸ“ **Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©**: [Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ù„Ù„Ø¨Ø¯Ø¡]

    Ø§Ù„Ø³Ø¤Ø§Ù„: {query}"""

    async def _enhanced_legal_research_with_rag(self, query: str, context: Optional[str] = None) -> AsyncIterator[str]:
        """Enhanced legal research using RAG + AI reasoning"""
        
        try:
            # STEP 1: Use your RAG system to find relevant documents
            from rag_engine import rag_engine
            
            print("ğŸ” Searching RAG database for relevant legal documents...")
            
            # Get relevant documents from your vector database using your existing RAG
            relevant_docs = []
            async for chunk in rag_engine.ask_question_streaming(query):
                relevant_docs.append(chunk)
            
            rag_results = ''.join(relevant_docs)
            
            # STEP 2: Enhanced prompt with real legal documents
            enhanced_input = f"""
            Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:
            {rag_results}
            
            Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø§Ù„Ø£ØµÙ„ÙŠ: {query}
            
            Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: ØªØ­Ù„ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ© ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ù…Ø¹ Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙØ³ÙŠØ± ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ.
            """
            
            # STEP 3: AI reasoning with real legal documents
            async for chunk in self.agents[LegalAgentType.LEGAL_RESEARCHER].process_streaming(
                enhanced_input, context
            ):
                yield chunk
                
        except Exception as e:
            print(f"âŒ Enhanced RAG research failed: {e}")
            # Fallback to standard AI-only research
            async for chunk in self.agents[LegalAgentType.LEGAL_RESEARCHER].process_streaming(
                query, context
            ):
                yield chunk
            
    async def process_legal_query_streaming(
        self, 
        query: str, 
        enable_trust_trail: bool = False,
        conversation_context: Optional[List[Dict]] = None
    ) -> AsyncIterator[str]:
        """
        ğŸ¯ SMART STREAMING: AI-powered intent classification + dynamic response formatting + CITATION VALIDATION
        Cost: ~3,200 tokens (~$0.014 per query) - includes citation validation
        """
        
        start_time = time.time()
        context_summary = ""
        if conversation_context:
            context_summary = self._summarize_conversation_context(conversation_context)
        
        # Variables to store content for citation validation
        fact_content = ""
        research_content = ""
        draft_content = ""
        
        try:
            # ğŸ§  STEP 0: AI-POWERED INTENT CLASSIFICATION
            print("ğŸ§  Classifying intent with AI...")
            intent_classification = await self.classify_intent(query, conversation_context)
            intent = intent_classification.get('intent', 'legal_consultation')
            confidence = intent_classification.get('confidence', 0.8)
            
            print(f"ğŸ¯ Intent: {intent} (confidence: {confidence:.1%})")
            
            # ğŸ¯ STEP 1: GET DYNAMIC AGENT PROMPTS
            dynamic_prompts = await self.get_dynamic_agent_prompts(intent_classification)
            
            # Temporarily override agent prompts for this query
            original_prompts = {}
            for agent_type, prompt in dynamic_prompts.items():
                if agent_type in self.agents:
                    original_prompts[agent_type] = self.agents[agent_type].system_prompts.copy()
                    self.agents[agent_type].system_prompts[agent_type] = prompt
            
            # ğŸ” STEP 2: FACT ANALYSIS (Stream with intent-aware prompts)
            print("ğŸ” Step 1: Analyzing legal facts...")
            
            if intent in ['rights_inquiry', 'procedure_guide']:
                yield "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:\n\n"
            else:
                yield "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:\n\n"
            
            # Collect fact analysis content for citation validation
            fact_chunks = []
            async for chunk in self.agents[LegalAgentType.FACT_ANALYZER].process_streaming(
                query, context_summary
            ):
                fact_chunks.append(chunk)
                yield chunk
            
            fact_content = ''.join(fact_chunks)
            yield "\n\n"

            # ğŸ” NEW STEP 2.5: INTENT VALIDATION (ADAPTIVE FLOW!)
            print("ğŸ” Step 2.5: Validating intent based on fact analysis...")
            yield "ğŸ” **Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¯Ù‚Ø© Ø§Ù„ØªØµÙ†ÙŠÙ**\n\n"

            # Validate if our initial classification matches the actual query complexity
            validation_result = await self.intent_validator.validate_intent(
                original_query=query,
                fact_analysis=fact_content,
                initial_classification=intent_classification
            )

            # Handle validation results
            if validation_result['validation_result'] == 'modified':
                # Intent changed completely
                intent = validation_result['recommended_intents'][0]
                yield f"ğŸ”„ **ØªÙ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªØµÙ†ÙŠÙ Ø¥Ù„Ù‰:** {intent}\n\n"
                print(f"ğŸ”„ Intent changed: {intent_classification.get('intent')} â†’ {intent}")
                
            elif validation_result['validation_result'] == 'expanded':
                # Multiple intents detected
                yield f"ğŸ¯ **ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©:** {', '.join(validation_result['recommended_intents'])}\n\n"
                print(f"ğŸ¯ Multi-intent detected: {validation_result['recommended_intents']}")
                
            elif validation_result['validation_result'] == 'confirmed':
                # Original classification was correct
                yield "âœ… **ØªÙ… ØªØ£ÙƒÙŠØ¯ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ØµÙ„ÙŠ**\n\n"
                print(f"âœ… Intent confirmed: {intent}")

            # Update intent and workflow based on validation
            recommended_intents = validation_result['recommended_intents']
            complexity_level = validation_result['complexity_level']

            # ğŸ“š STEP 3: ENHANCED LEGAL RESEARCH (RAG + AI)
            print("ğŸ“š Step 3: Researching legal precedents with RAG...")

            # ğŸ¯ ADAPTIVE WORKFLOW EXECUTION
            research_content = ""

            if len(recommended_intents) > 1:
                # Multi-intent processing
                for intent_type in recommended_intents:
                    yield f"\n## ğŸ“š {intent_type.replace('_', ' ').title()}\n\n"
                    
                    # Research specific to this intent
                    research_chunks = []
                    async for chunk in self._enhanced_legal_research_with_rag(f"{query} - {intent_type}", context_summary):
                        research_chunks.append(chunk)
                        yield chunk
                    
                    research_content += ''.join(research_chunks) + "\n\n"
            else:
                # Single intent processing
                intent = recommended_intents[0]
                
                if intent == 'penalty_explanation':
                    yield "âš–ï¸ **Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª ÙˆØ§Ù„Ø¬Ø²Ø§Ø¡Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**\n\n"
                elif intent == 'rights_inquiry':
                    yield "ğŸ” **Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…ÙƒÙÙˆÙ„Ø©:**\n\n"
                elif intent == 'procedure_guide':
                    yield "ğŸ“‹ **Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:**\n\n"
                elif intent == 'legal_dispute':
                    yield "âš–ï¸ **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø²Ø§Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:**\n\n"
                else:
                    yield "ğŸ’¡ **Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**\n\n"

                # Single research pass
                research_chunks = []
                async for chunk in self._enhanced_legal_research_with_rag(query, context_summary):
                    research_chunks.append(chunk)
                    yield chunk
                
                research_content = ''.join(research_chunks)

            yield "\n\n"

            
            
            # ğŸ—ï¸ STEP 4: FOCUSED RESPONSE GENERATION
            print("ğŸ“ Step 3: Generating focused legal response...")

            # Generate laser-focused prompt based on intent
            focused_prompt = self._generate_focused_prompt(recommended_intents[0], query, complexity_level)

            # Override document drafter prompt temporarily
            original_prompt = self.agents[LegalAgentType.DOCUMENT_DRAFTER].system_prompts[LegalAgentType.DOCUMENT_DRAFTER]
            self.agents[LegalAgentType.DOCUMENT_DRAFTER].system_prompts[LegalAgentType.DOCUMENT_DRAFTER] = focused_prompt

            # Generate focused response
            draft_chunks = []
            async for chunk in self.agents[LegalAgentType.DOCUMENT_DRAFTER].process_streaming(query, research_content):
                draft_chunks.append(chunk)
                yield chunk

            # Restore original prompt
            self.agents[LegalAgentType.DOCUMENT_DRAFTER].system_prompts[LegalAgentType.DOCUMENT_DRAFTER] = original_prompt
            
            draft_content = ''.join(draft_chunks)
            yield "\n\n"

            # ğŸ¯ STEP 4.5: INTELLIGENT CONTENT MERGING (for multi-intent)
            if len(recommended_intents) > 1:
                print("ğŸ”„ Step 4.5: Merging multi-intent content...")
                yield "ğŸ”„ **Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ø¨Ø°ÙƒØ§Ø¡...**\n\n"
                
                # Prepare content for merging
                intent_outputs = {}
                for i, intent_type in enumerate(recommended_intents):
                    intent_outputs[intent_type] = f"Intent {i+1}: {draft_content}"
                
                # Use content merger for deduplication and professional formatting
                try:
                    final_merged_content = self.content_merger.merge_multi_intent_outputs(
                        intent_outputs=intent_outputs,
                        original_query=query,
                        complexity_level=complexity_level
                    )
                    
                    yield "âœ… **Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø¯Ù…Ø¬:**\n\n"
                    yield final_merged_content
                    yield "\n\n"
                    
                except Exception as e:
                    print(f"âš ï¸ Content merging failed: {e}")
                    yield "âš ï¸ **ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ**\n\n"

            # ğŸ” STEP 5: CITATION VALIDATION (NEW!)
            print("ğŸ” Step 4: Validating citations and legal references...")
            yield "ğŸ” **Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**\n\n"
            
            # Combine all content for comprehensive citation validation
            all_content_for_validation = f"""
    Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©:

    Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:
    {fact_content}

    Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:
    {research_content}

    Ø§Ù„Ù…Ø³ÙˆØ¯Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:
    {draft_content}

    Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© Ø£Ø¹Ù„Ø§Ù‡ ÙˆØªÙ‚ÙŠÙŠÙ… Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠÙ‡Ø§.
    """
            
            # Stream citation validation results
            async for chunk in self.agents[LegalAgentType.CITATION_VALIDATOR].process_streaming(
                all_content_for_validation, context_summary
            ):
                yield chunk
            
            # ğŸ”§ STEP 6: RESTORE ORIGINAL PROMPTS
            for agent_type, original_prompt in original_prompts.items():
                if agent_type in self.agents:
                    self.agents[agent_type].system_prompts = original_prompt
            
            print(f"âœ… Smart multi-agent streaming with citation validation completed. Intent: {intent}")
            
        except Exception as e:
            print(f"âŒ Smart multi-agent streaming failed: {e}")
            import traceback
            traceback.print_exc()
            
            # Restore prompts even if there's an error
            try:
                for agent_type, original_prompt in original_prompts.items():
                    if agent_type in self.agents:
                        self.agents[agent_type].system_prompts = original_prompt
            except:
                pass
            
            yield f"ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ Ø¨Ù†Ø¸Ø§Ù… Ù…Ø¨Ø³Ø· Ù†Ø¸Ø±Ø§Ù‹ Ù„Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ Ù…Ø¤Ù‚Øª.\n\n{query}\n\nÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ùˆ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªÙ‚Ø¯Ù…."
    async def _execute_agent_step(
        self,
        agent_type: LegalAgentType,
        step_number: int,
        step_name: str,
        input_data: str,
        context: Optional[str] = None
    ) -> AgentStep:
        """Execute individual agent step with timing and error handling"""
        
        step_start = time.time()
        
        try:
            output, citations, confidence = await self.agents[agent_type].process(input_data, context)
            
            processing_time = int((time.time() - step_start) * 1000)
            
            return AgentStep(
                agent_type=agent_type,
                step_number=step_number,
                step_name=step_name,
                input_data=input_data[:200] + "..." if len(input_data) > 200 else input_data,
                output_data=output,
                citations=citations,
                confidence_score=confidence,
                processing_time_ms=processing_time,
                timestamp=datetime.now().isoformat(),
                sources_verified=False
            )
            
        except Exception as e:
            return AgentStep(
                agent_type=agent_type,
                step_number=step_number,
                step_name=step_name,
                input_data=input_data[:200] + "..." if len(input_data) > 200 else input_data,
                output_data=f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}",
                citations=[],
                confidence_score=0.1,
                processing_time_ms=int((time.time() - step_start) * 1000),
                timestamp=datetime.now().isoformat(),
                sources_verified=False
            )
    
    def _compile_final_answer(self, reasoning_steps: List[AgentStep], enable_trust_trail: bool) -> str:
        """Compile final answer with optional trust trail"""
        
        # Get the main legal advice from the drafting step
        draft_step = next((step for step in reasoning_steps if step.agent_type == LegalAgentType.DOCUMENT_DRAFTER), None)
        main_answer = draft_step.output_data if draft_step else "ØªØ¹Ø°Ø± Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©."
        
        if not enable_trust_trail:
            return main_answer
        
        # Add trust trail for transparency
        trust_trail = "\n\n" + "="*60 + "\n"
        trust_trail += "ğŸ” **Ø³Ù„Ø³Ù„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ** (Trust Trail)\n"
        trust_trail += "="*60 + "\n\n"
        
        for step in reasoning_steps:
            trust_trail += f"**{step.step_number}. {step.step_name}**\n"
            trust_trail += f"â±ï¸ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {step.processing_time_ms}ms\n"
            trust_trail += f"ğŸ“Š Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: {step.confidence_score:.1%}\n"
            
            if step.citations:
                trust_trail += f"ğŸ“š Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹: {', '.join(step.citations[:3])}" 
                if len(step.citations) > 3:
                    trust_trail += f" Ùˆ {len(step.citations) - 3} Ù…Ø±Ø§Ø¬Ø¹ Ø£Ø®Ø±Ù‰"
                trust_trail += "\n"
            
            trust_trail += f"ğŸ’­ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {step.output_data[:150]}...\n\n"
        
        return main_answer + trust_trail
    
    def _summarize_conversation_context(self, context: List[Dict]) -> str:
        """Summarize conversation context for agents"""
        if not context or len(context) < 2:
            return ""
        
        # Get last few exchanges
        recent_context = context[-4:] if len(context) > 4 else context
        
        summary = "Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n"
        for msg in recent_context:
            role = "Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…" if msg.get("role") == "user" else "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯"
            content = msg.get("content", "")[:100]
            summary += f"- {role}: {content}...\n"
        
        return summary
    
    async def _fallback_response(self, query: str, start_time: float) -> LegalReasoningResult:
        """Fallback to simple response if multi-agent fails"""
        
        simple_response = f"""ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ Ø¨Ù†Ø¸Ø§Ù… Ù…Ø¨Ø³Ø· Ù†Ø¸Ø±Ø§Ù‹ Ù„Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ Ù…Ø¤Ù‚Øª.

{query}

ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ùˆ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªÙ‚Ø¯Ù…."""

        fallback_step = AgentStep(
            agent_type=LegalAgentType.FACT_ANALYZER,
            step_number=1,
            step_name="Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¨Ø³Ø·Ø©",
            input_data=query,
            output_data=simple_response,
            citations=[],
            confidence_score=0.3,
            processing_time_ms=int((time.time() - start_time) * 1000),
            timestamp=datetime.now().isoformat(),
            sources_verified=False
        )
        
        return LegalReasoningResult(
            query=query,
            final_answer=simple_response,
            reasoning_steps=[fallback_step],
            total_processing_time_ms=int((time.time() - start_time) * 1000),
            overall_confidence=0.3,
            trust_trail_enabled=False,
            citations_summary=[]
        )
    


    

class EnhancedRAGEngine:
    """
    ğŸš€ ENHANCED RAG ENGINE WITH MULTI-AGENT INTEGRATION
    
    This integrates with your existing rag_engine.py while adding multi-agent capabilities
    """
    
    def __init__(self):
        # Use the imported OpenAI client
        self.openai_client = openai_client
        self.orchestrator = MultiAgentLegalOrchestrator(self.openai_client)
        
        # Flag to enable/disable multi-agent processing
        self.multi_agent_enabled = True
    
    async def ask_question_with_multi_agent(
        self, 
        query: str, 
        conversation_context: Optional[List[Dict]] = None,
        enable_trust_trail: bool = False
    ) -> AsyncIterator[str]:
        """
        ğŸ¯ NEW METHOD: Multi-agent legal reasoning with streaming
        """
        
        if not self.multi_agent_enabled:
            # Fallback to existing single-agent (would need to import from rag_engine)
            try:
                from rag_engine import ask_question_with_context
                response = await ask_question_with_context(query, conversation_context or [])
                
                # Stream the response in chunks
                chunk_size = 50
                for i in range(0, len(response), chunk_size):
                    chunk = response[i:i + chunk_size]
                    yield chunk
                    await asyncio.sleep(0.03)
                return
            except ImportError:
                # If can't import, use simple fallback
                yield "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ØºÙŠØ± Ù…ØªØ§Ø­ Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹."
                return
        
        try:
            # Process with REAL streaming multi-agent system
            async for chunk in self.orchestrator.process_legal_query_streaming(
                query=query,
                enable_trust_trail=enable_trust_trail,
                conversation_context=conversation_context
            ):
                yield chunk  # Real-time streaming from agents
            
            # Send trust trail data if enabled
            if enable_trust_trail:
                trust_trail_data = {
                    "type": "trust_trail",
                    "reasoning_steps": [],  # Would need to be implemented
                    "citations_summary": []
                }
                yield f"\n\ndata: {json.dumps(trust_trail_data)}\n\n"
                
        except Exception as e:
            print(f"âŒ Multi-agent processing failed, using fallback: {e}")
            
            # Simple fallback response
            fallback_response = f"""ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ:

{query}

ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ø°Ø§ ÙƒÙ†Øª Ø¨Ø­Ø§Ø¬Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹."""
            
            chunk_size = 50
            for i in range(0, len(fallback_response), chunk_size):
                chunk = fallback_response[i:i + chunk_size]
                yield chunk
                await asyncio.sleep(0.03)

async def test_nuclear_system():
    """Test function to verify nuclear system works - GUARANTEED NO BLOAT"""
    try:
        # Create OpenAI client directly
        import os
        from openai import AsyncOpenAI
        from dotenv import load_dotenv
        
        load_dotenv(".env")
        
        # Use the same client setup as your rag_engine
        OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
        AI_PROVIDER = os.getenv("AI_PROVIDER", "openai")
        
        if AI_PROVIDER == "openai" and OPENAI_API_KEY:
            openai_client = AsyncOpenAI(
                api_key=OPENAI_API_KEY,
                timeout=60.0,
                max_retries=2
            )
        elif DEEPSEEK_API_KEY:
            openai_client = AsyncOpenAI(
                api_key=DEEPSEEK_API_KEY,
                base_url="https://api.deepseek.com/v1",
                timeout=60.0,
                max_retries=2
            )
        else:
            raise ValueError("âŒ No API key available")
        
        from nuclear_orchestrator import NuclearLegalOrchestrator
        nuclear_orchestrator = NuclearLegalOrchestrator(openai_client)
        
        query = "Ù…ÙˆØ¸Ù ØªÙ… ÙØµÙ„Ù‡ Ø¨Ø¯ÙˆÙ† Ù…Ø¨Ø±Ø±ØŒ Ù…Ø§ Ø­Ù‚ÙˆÙ‚Ù‡ØŸ"
        print(f"ğŸš€ Testing NUCLEAR system with query: {query}")
        print("="*60)
        
        response_chunks = []
        async for chunk in nuclear_orchestrator.nuclear_process_query(
            query=query,
            conversation_context=None
        ):
            response_chunks.append(chunk)
            print(chunk, end="", flush=True)
        
        print("\n" + "="*60)
        print(f"âœ… NUCLEAR Test completed. Total chunks: {len(response_chunks)}")
        
        # Show nuclear metrics
        metrics = nuclear_orchestrator.get_nuclear_metrics()
        print(f"ğŸ“Š NUCLEAR METRICS:")
        print(f"   Word compliance: {metrics['word_limit_compliance']:.1%}")
        print(f"   CTA compliance: {metrics['cta_compliance']:.1%}")
        print(f"   Processing time: {metrics['average_processing_time_ms']}ms")
        print(f"   Overall score: {metrics['overall_compliance_score']:.1%}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Nuclear test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    # Run nuclear test when file is executed directly
    import asyncio
    asyncio.run(test_nuclear_system())