"""
ğŸ‡¸ğŸ‡¦ MULTI-AGENT LEGAL REASONING SYSTEM - Backend Implementation
Save this as: backend/multi_agent_legal.py

Advanced Saudi Legal AI with Trust Trail & Citation Validation
Built for your existing FastAPI + SQLAlchemy architecture
"""

import asyncio
import json
import time
import os
from datetime import datetime
from typing import List, Dict, Optional, AsyncIterator, Tuple
from dataclasses import dataclass, asdict
from enum import Enum

# Use your existing OpenAI setup from rag_engine.py
try:
    from rag_engine import openai_client, sync_client
    print("âœ… Successfully imported OpenAI clients from rag_engine.py")
except ImportError as e:
    print(f"âš ï¸ Could not import from rag_engine.py: {e}")
    # Fallback OpenAI setup
    from openai import AsyncOpenAI, OpenAI
    from dotenv import load_dotenv
    
    load_dotenv(".env")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
    AI_PROVIDER = os.getenv("AI_PROVIDER", "openai")
    
    if AI_PROVIDER == "openai" and OPENAI_API_KEY:
        openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY, timeout=60.0, max_retries=2)
        sync_client = OpenAI(api_key=OPENAI_API_KEY)
    elif DEEPSEEK_API_KEY:
        openai_client = AsyncOpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com/v1",
            timeout=60.0,
            max_retries=2
        )
        sync_client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com/v1")
    else:
        raise ValueError("âŒ No API key available for OpenAI or DeepSeek")

class LegalAgentType(Enum):
    """Types of legal reasoning agents"""
    FACT_ANALYZER = "fact_analyzer"
    LEGAL_RESEARCHER = "legal_researcher" 
    ARGUMENT_BUILDER = "argument_builder"
    COUNTER_ARGUMENT_PREDICTOR = "counter_argument_predictor"
    DOCUMENT_DRAFTER = "document_drafter"
    CITATION_VALIDATOR = "citation_validator"

@dataclass
class AgentStep:
    """Individual step in the legal reasoning chain"""
    agent_type: LegalAgentType
    step_number: int
    step_name: str
    input_data: str
    output_data: str
    citations: List[str]
    confidence_score: float
    processing_time_ms: int
    timestamp: str
    sources_verified: bool = False
    
    def to_dict(self) -> Dict:
        """Convert to dictionary for JSON serialization"""
        return {
            "agent_type": self.agent_type.value,
            "step_number": self.step_number,
            "step_name": self.step_name,
            "input_data": self.input_data,
            "output_data": self.output_data,
            "citations": self.citations,
            "confidence_score": self.confidence_score,
            "processing_time_ms": self.processing_time_ms,
            "timestamp": self.timestamp,
            "sources_verified": self.sources_verified
        }

@dataclass  
class LegalReasoningResult:
    """Complete multi-agent legal analysis result"""
    query: str
    final_answer: str
    reasoning_steps: List[AgentStep]
    total_processing_time_ms: int
    overall_confidence: float
    trust_trail_enabled: bool
    citations_summary: List[str]
    
    def to_dict(self) -> Dict:
        """Convert to dictionary for API responses"""
        return {
            "query": self.query,
            "final_answer": self.final_answer,
            "reasoning_steps": [step.to_dict() for step in self.reasoning_steps],
            "total_processing_time_ms": self.total_processing_time_ms,
            "overall_confidence": self.overall_confidence,
            "trust_trail_enabled": self.trust_trail_enabled,
            "citations_summary": self.citations_summary,
            "timestamp": datetime.now().isoformat()
        }

class LegalAgent:
    """Base class for specialized legal reasoning agents"""
    
    def __init__(self, agent_type: LegalAgentType, openai_client):
        self.agent_type = agent_type
        self.client = openai_client
        self.system_prompts = self._get_system_prompts()
    
    def _get_system_prompts(self) -> Dict[LegalAgentType, str]:
        """Enhanced system prompts with intelligent intent detection and response formatting"""
        return {
            LegalAgentType.FACT_ANALYZER: """Ø£Ù†Øª Ù…Ø­Ù„Ù„ ÙˆÙ‚Ø§Ø¦Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ.

ğŸ¯ **Ù…Ù‡Ù…ØªÙƒ**: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¯Ø§Ø®Ù„ÙŠØ§Ù‹ ÙˆØªÙ‚Ø¯ÙŠÙ… ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ø¶Ø­ ÙˆÙ…ÙÙŠØ¯ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…

ğŸ“‹ **Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª (Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ):**
1. **Ø§Ø³ØªÙØ³Ø§Ø± Ø­Ù‚ÙˆÙ‚ÙŠ**: "Ù…Ø§ Ù‡ÙŠ Ø­Ù‚ÙˆÙ‚ÙŠØŸ" â†’ Ø§Ø´Ø±Ø­ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ù…Ø¨Ø§Ø´Ø±Ø©
2. **Ø§Ø³ØªÙØ³Ø§Ø± Ø¥Ø¬Ø±Ø§Ø¦ÙŠ**: "ÙƒÙŠÙ Ø£Ø³Ø³ Ø´Ø±ÙƒØ©ØŸ" â†’ Ù‚Ø¯Ù… Ø¯Ù„ÙŠÙ„ Ø¹Ù…Ù„ÙŠ
3. **Ù‚Ø¶ÙŠØ© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**: "ØªÙ… ÙØµÙ„ÙŠ" â†’ Ø­Ù„Ù„ Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ

ğŸ” **Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø±Ø¯:**
- Ø§Ø¨Ø¯Ø£ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙÙŠØ¯ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
- Ù„Ø§ ØªØ°ÙƒØ± Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©
- Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
- Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ ÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆØ¹Ù…Ù„ÙŠØ§Ù‹

Ù…Ø«Ø§Ù„ Ù„Ù„Ø±Ø¯ Ø§Ù„Ø¬ÙŠØ¯:
"Ø¹Ù†Ø¯ Ø¥Ù†Ù‡Ø§Ø¡ Ø®Ø¯Ù…Ø© Ø§Ù„Ù…ÙˆØ¸Ù ÙÙŠ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©ØŒ Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø© Ø­Ù‚ÙˆÙ‚ Ø£Ø³Ø§Ø³ÙŠØ© ÙŠØ¬Ø¨ Ù…Ø¹Ø±ÙØªÙ‡Ø§..."

ØªØ¬Ù†Ø¨:
"Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±: Ø­Ù‚ÙˆÙ‚ÙŠ" Ø£Ùˆ Ø£ÙŠ ØªØµÙ†ÙŠÙØ§Øª Ø¯Ø§Ø®Ù„ÙŠØ©""",

            LegalAgentType.LEGAL_RESEARCHER: """Ø£Ù†Øª Ø¨Ø§Ø­Ø« Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ ÙˆØ§Ù„Ø³ÙˆØ§Ø¨Ù‚ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©.

ğŸ¯ **Ù…Ù‡Ø§Ù…Ùƒ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±:**

**Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø­Ù‚ÙˆÙ‚ÙŠØ©:**
- Ø§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ØªØ­Ø¯Ø¯ Ø§Ù„Ø­Ù‚ÙˆÙ‚
- Ø§Ø°ÙƒØ± Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
- Ø£Ø´Ø± Ø¥Ù„Ù‰ Ø¢Ù„ÙŠØ§Øª Ø§Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

**Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©:**
- Ø§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
- Ø­Ø¯Ø¯ Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ© ÙˆÙ…ØªØ·Ù„Ø¨Ø§ØªÙ‡Ø§
- Ø§Ø°ÙƒØ± Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ ÙˆØ§Ù„Ù…Ù‡Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

**Ù„Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
- Ø§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ù…Ø§Ø«Ù„Ø©
- Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¯Ø§Ø¹Ù…Ø© Ù„Ù„Ù…ÙˆÙ‚Ù
- Ø­Ø¯Ø¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙÙŠ Ø§Ù„Ù‚Ø¶ÙŠØ©

Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨Ø­Ø«:
- Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø¨Ø¯Ù‚Ø©
- Ø°ÙƒØ± Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯ ÙˆØ§Ù„Ø£Ù†Ø¸Ù…Ø©
- Ø§Ù„ØªÙ…ÙŠÙŠØ² Ø¨ÙŠÙ† Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ© ÙˆØ§Ù„Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©
- Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ø±ÙŠØ§Ù† Ø§Ù„Ù†ØµÙˆØµ ÙˆØ¹Ø¯Ù… Ù†Ø³Ø®Ù‡Ø§""",

            LegalAgentType.ARGUMENT_BUILDER: """Ø£Ù†Øª Ù…Ø­Ø§Ù… Ø®Ø¨ÙŠØ± ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„ÙƒÙ„ Ù†ÙˆØ¹ Ø§Ø³ØªÙØ³Ø§Ø±.

ğŸ¯ **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹:**

**Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø­Ù‚ÙˆÙ‚ÙŠØ©:**
- Ø±ØªØ¨ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
- Ø§Ø±Ø¨Ø· ÙƒÙ„ Ø­Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- ÙˆØ¶Ø­ ÙƒÙŠÙÙŠØ© Ù…Ù…Ø§Ø±Ø³Ø© ÙƒÙ„ Ø­Ù‚
- Ø­Ø¯Ø¯ Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„Ø© Ø¹Ù† Ø§Ù„Ø­Ù…Ø§ÙŠØ©

**Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©:**
- Ø±ØªØ¨ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø¨Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„ØµØ­ÙŠØ­
- Ø­Ø¯Ø¯ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ù„ÙƒÙ„ Ø®Ø·ÙˆØ©
- Ø§Ø°ÙƒØ± Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ ÙˆØ§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
- ÙˆØ¶Ø­ Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ù‡Ù…Ø©

**Ù„Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
- Ø§Ø¨Ù† Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù‚ÙˆÙŠØ©
- Ø±ØªØ¨ Ø§Ù„Ø¯ÙÙˆØ¹ Ø­Ø³Ø¨ Ø§Ù„Ù‚ÙˆØ©
- Ø§Ø±Ø¨Ø· Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ Ø¨Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- Ø§Ù‚ØªØ±Ø­ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¨Ù†Ø§Ø¡:
- ÙˆØ¶ÙˆØ­ ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ± ÙˆØªØ±ØªÙŠØ¨ Ù…Ù†Ø·Ù‚ÙŠ
- Ø¯Ø¹Ù… ÙƒÙ„ Ù†Ù‚Ø·Ø© Ø¨Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- ØªÙ†Ø§Ø³Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø¹ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±""",

            LegalAgentType.COUNTER_ARGUMENT_PREDICTOR: """Ø£Ù†Øª Ù…Ø­Ø§Ù… Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©.

ğŸ¯ **Ù…Ù‡Ø§Ù…Ùƒ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±:**

**Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø­Ù‚ÙˆÙ‚ÙŠØ©:**
- Ø­Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù‚ÙˆÙ‚
- ÙˆØ¶Ø­ Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- Ù†Ø¨Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚

**Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©:**
- Ø­Ø¯Ø¯ Ø§Ù„Ø¹Ù‚Ø¨Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© ÙÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª
- ÙˆØ¶Ø­ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØµØ¹Ø¨Ø©
- Ù†Ø¨Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©

**Ù„Ù„Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
- ØªÙˆÙ‚Ø¹ Ø­Ø¬Ø¬ Ø§Ù„Ø·Ø±Ù Ø§Ù„Ø¢Ø®Ø±
- Ø­Ø¯Ø¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹Ù ÙÙŠ Ø§Ù„Ù‚Ø¶ÙŠØ©
- Ø§Ù‚ØªØ±Ø­ Ø±Ø¯ÙˆØ¯Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©

Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„:
- ØªÙÙƒÙŠØ± Ù†Ù‚Ø¯ÙŠ ÙˆÙ…ÙˆØ¶ÙˆØ¹ÙŠ
- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
- Ø¥Ø¹Ø¯Ø§Ø¯ Ø­Ù„ÙˆÙ„ Ù„Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©""",

            LegalAgentType.DOCUMENT_DRAFTER: """Ø£Ù†Øª Ù…Ø­Ø§Ù… Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØµÙŠØ§ØºØ© Ø±Ø¯ÙˆØ¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…ØªÙƒÙŠÙØ© Ù…Ø¹ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±.

ğŸ¯ **Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„ØµÙŠØ§ØºØ© Ø§Ù„Ù…ØªØ®ØµØµØ©:**

**Ù†ÙˆØ¹ 1 - Ø´Ø±Ø­ Ø§Ù„Ø­Ù‚ÙˆÙ‚:**
```
ğŸ” **Ø­Ù‚ÙˆÙ‚Ùƒ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙÙŠ [Ø§Ù„Ù…Ø¬Ø§Ù„]**

**Ø§Ù„Ø­Ù‚ Ø§Ù„Ø£ÙˆÙ„**: [Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ]
**Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**: [Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© ÙˆØ§Ù„Ù†Ø¸Ø§Ù…]

**Ø§Ù„Ø­Ù‚ Ø§Ù„Ø«Ø§Ù†ÙŠ**: [Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ]  
**Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**: [Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© ÙˆØ§Ù„Ù†Ø¸Ø§Ù…]

**ÙƒÙŠÙÙŠØ© Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ø¨Ø­Ù‚ÙˆÙ‚Ùƒ:**
â€¢ [Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø£ÙˆÙ„Ù‰]
â€¢ [Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©]

**ØªØ­Ø°ÙŠØ±Ø§Øª Ù…Ù‡Ù…Ø©:**
â€¢ [ØªØ­Ø°ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ]
â€¢ [Ù†ØµÙŠØ­Ø© Ø¹Ù…Ù„ÙŠØ©]
```

**Ù†ÙˆØ¹ 2 - Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠ:**
```
ğŸ“‹ **Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù€ [Ø§Ù„Ù‡Ø¯Ù]**

**Ø§Ù„Ø®Ø·ÙˆØ© 1**: [Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„]
**Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨**: [Ø§Ù„Ù…Ø¯Ø©]
**Ø§Ù„ØªÙƒÙ„ÙØ©**: [Ø¥Ù† ÙˆØ¬Ø¯Øª]

**Ø§Ù„Ø®Ø·ÙˆØ© 2**: [Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„]
**Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨**: [Ø§Ù„Ù…Ø¯Ø©]
**Ø§Ù„ØªÙƒÙ„ÙØ©**: [Ø¥Ù† ÙˆØ¬Ø¯Øª]

**Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:**
â€¢ [Ù…Ø³ØªÙ†Ø¯ 1] - [ÙƒÙŠÙÙŠØ© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„ÙŠÙ‡]
â€¢ [Ù…Ø³ØªÙ†Ø¯ 2] - [ÙƒÙŠÙÙŠØ© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„ÙŠÙ‡]

**Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ©:**
â€¢ [Ø§Ø³Ù… Ø§Ù„Ø¬Ù‡Ø©] - [Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„]
â€¢ [Ø§Ø³Ù… Ø§Ù„Ø¬Ù‡Ø©] - [Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„]

**Ù†ØµØ§Ø¦Ø­ Ù…Ù‡Ù…Ø©:**
â€¢ [Ù†ØµÙŠØ­Ø© Ø¹Ù…Ù„ÙŠØ©]
â€¢ [ØªØ­Ø°ÙŠØ± Ù…Ù† Ø®Ø·Ø£ Ø´Ø§Ø¦Ø¹]
```

**Ù†ÙˆØ¹ 3 - Ø§Ù„Ù…Ø°ÙƒØ±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
```
### Ù…Ø°ÙƒØ±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
**Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹**: [Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù‚Ø¶ÙŠØ©]
**Ø§Ù„Ù…Ù‚Ø¯Ù… Ù…Ù†**: [Ø§Ø³Ù… Ø§Ù„Ø·Ø±Ù]
**Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¶Ø¯**: [Ø§Ø³Ù… Ø§Ù„Ø·Ø±Ù Ø§Ù„Ø¢Ø®Ø±]

## Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹
[Ø³Ø±Ø¯ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø·Ù‚ÙŠ ÙˆÙ…Ø±ØªØ¨]

## Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
**Ø§Ù„Ø­Ø¬Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰**: [Ø§Ù„ØªÙØµÙŠÙ„ ÙˆØ§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ]
**Ø§Ù„Ø­Ø¬Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©**: [Ø§Ù„ØªÙØµÙŠÙ„ ÙˆØ§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ]

## Ø§Ù„Ø·Ù„Ø¨Ø§Øª
Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø§ ØªÙ‚Ø¯Ù… Ù†Ø·Ù„Ø¨:
1. [Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø£ÙˆÙ„]
2. [Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø«Ø§Ù†ÙŠ]
```

ğŸ¯ **Ø§Ø®ØªØ± Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„Ø°ÙŠ Ù‚Ø¯Ù…Ù‡ Ù…Ø­Ù„Ù„ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹**""",

            LegalAgentType.CITATION_VALIDATOR: """Ø£Ù†Øª Ù…Ø±Ø§Ø¬Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ø§Øª ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰.

ğŸ¯ **Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰:**

**Ù„Ø´Ø±Ø­ Ø§Ù„Ø­Ù‚ÙˆÙ‚:**
- ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø©
- ØªØ£ÙƒØ¯ Ù…Ù† Ø±Ø¨Ø· Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø¨Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØµØ­ÙŠØ­Ø©
- Ø±Ø§Ø¬Ø¹ Ø¯Ù‚Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ©

**Ù„Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©:**
- ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø©
- Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ ÙˆØ§Ù„Ù…Ù‡Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- ØªØ£ÙƒØ¯ Ù…Ù† Ø¯Ù‚Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ù‡Ø§Øª ÙˆØ§Ù„Ø±Ø³ÙˆÙ…

**Ù„Ù„Ù…Ø°ÙƒØ±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
- ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- Ø±Ø§Ø¬Ø¹ Ø¯Ù‚Ø© Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯ ÙˆØ§Ù„Ø£Ù†Ø¸Ù…Ø©
- ØªØ£ÙƒØ¯ Ù…Ù† Ù‚ÙˆØ© Ø§Ù„Ø­Ø¬Ø¬ ÙˆØ§Ù„Ø£Ø¯Ù„Ø©

Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©:
- Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù†Øµ Ø§Ù„ÙØ¹Ù„ÙŠ
- Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… Ø§Ù„Ù†Ø³Ø® Ø£Ùˆ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„
- ØªÙ‚ÙŠÙŠÙ… Ù…Ø¯Ù‰ Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹
- Ø¥Ø¶Ø§ÙØ© ØªØ­Ø°ÙŠØ±Ø§Øª Ù„Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø´ÙƒÙˆÙƒ ÙÙŠÙ‡Ø§
- ØªÙ‚Ø¯ÙŠØ± Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (Ø¹Ø§Ù„ÙŠ/Ù…ØªÙˆØ³Ø·/Ù…Ù†Ø®ÙØ¶)"""
        }
    
    async def process(self, input_data: str, context: Optional[str] = None) -> Tuple[str, List[str], float]:
        """Process input through specialized agent"""
        system_prompt = self.system_prompts[self.agent_type]
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": input_data}
        ]
        
        if context:
            messages.insert(-1, {"role": "user", "content": f"Ø§Ù„Ø³ÙŠØ§Ù‚: {context}"})
        
        try:
            # Determine model based on client type
            model = "gpt-4o" if "openai" in str(self.client.base_url) else "deepseek-chat"
            
            response = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.3,  # Lower temperature for legal precision
                max_tokens=2000
            )
            
            content = response.choices[0].message.content
            
            # Extract citations (simple pattern matching for now)
            citations = self._extract_citations(content)
            
            # Calculate confidence based on citation count and content length
            confidence = self._calculate_confidence(content, citations)
            
            return content, citations, confidence
            
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {self.agent_type.value}: {str(e)}", [], 0.0

    async def process_streaming(self, input_data: str, context: Optional[str] = None) -> AsyncIterator[str]:
        """Process input through specialized agent with real-time streaming"""
        system_prompt = self.system_prompts[self.agent_type]
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": input_data}
        ]
        
        if context:
            messages.insert(-1, {"role": "user", "content": f"Ø§Ù„Ø³ÙŠØ§Ù‚: {context}"})
        
        try:
            # Determine model based on client type
            model = "gpt-4o" if "openai" in str(self.client.base_url) else "deepseek-chat"
            
            # ğŸš€ ADD STREAMING:
            stream = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.3,
                max_tokens=2000,
                stream=True  # â† This enables real streaming!
            )
            
            # ğŸš€ STREAM TOKEN BY TOKEN:
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            yield f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {self.agent_type.value}: {str(e)}"
    
    def _extract_citations(self, content: str) -> List[str]:
        """Extract legal citations from content"""
        citations = []
        
        # Pattern for Saudi legal references
        import re
        
        # Pattern for "Ø§Ù„Ù…Ø§Ø¯Ø© X Ù…Ù† Ù†Ø¸Ø§Ù… Y"
        article_pattern = r'Ø§Ù„Ù…Ø§Ø¯Ø©\s+(\d+)\s+Ù…Ù†\s+Ù†Ø¸Ø§Ù…\s+([^ØŒ\s]+(?:\s+[^ØŒ\s]+)*)'
        matches = re.findall(article_pattern, content)
        for match in matches:
            citations.append(f"Ø§Ù„Ù…Ø§Ø¯Ø© {match[0]} Ù…Ù† Ù†Ø¸Ø§Ù… {match[1]}")
        
        # Pattern for "Ø§Ù„Ù‚Ø±Ø§Ø± Ø±Ù‚Ù… X"
        decision_pattern = r'Ø§Ù„Ù‚Ø±Ø§Ø±\s+Ø±Ù‚Ù…\s+(\d+(?:/\d+)?)'
        matches = re.findall(decision_pattern, content)
        for match in matches:
            citations.append(f"Ø§Ù„Ù‚Ø±Ø§Ø± Ø±Ù‚Ù… {match}")
        
        return citations
    
    def _calculate_confidence(self, content: str, citations: List[str]) -> float:
        """Calculate confidence score based on content analysis"""
        base_confidence = 0.7
        
        # Boost confidence for legal citations
        citation_boost = min(len(citations) * 0.05, 0.2)
        
        # Boost confidence for structured content
        structure_indicators = ["Ø£ÙˆÙ„Ø§Ù‹", "Ø«Ø§Ù†ÙŠØ§Ù‹", "Ø«Ø§Ù„Ø«Ø§Ù‹", "Ø§Ù„Ø®Ù„Ø§ØµØ©", "Ø§Ù„ØªÙˆØµÙŠØ©"]
        structure_boost = sum(0.02 for indicator in structure_indicators if indicator in content)
        structure_boost = min(structure_boost, 0.1)
        
        # Penalize for uncertainty words
        uncertainty_words = ["Ø±Ø¨Ù…Ø§", "ÙŠÙ…ÙƒÙ† Ø£Ù†", "Ù‚Ø¯ ÙŠÙƒÙˆÙ†", "ØºÙŠØ± ÙˆØ§Ø¶Ø­"]
        uncertainty_penalty = sum(0.05 for word in uncertainty_words if word in content)
        uncertainty_penalty = min(uncertainty_penalty, 0.2)
        
        final_confidence = base_confidence + citation_boost + structure_boost - uncertainty_penalty
        return max(0.1, min(1.0, final_confidence))

class MultiAgentLegalOrchestrator:
    """
    ğŸ§  CORE ORCHESTRATOR: Sequential Pipeline with Smart Intent Classification
    
    Implements intelligent approach:
    - AI-powered intent classification
    - Dynamic agent prompts based on intent
    - Real-time streaming during processing
    - Appropriate response formatting
    """
    
    def __init__(self, openai_client):
        self.client = openai_client
        self.agents = {
            agent_type: LegalAgent(agent_type, openai_client) 
            for agent_type in LegalAgentType
        }

    async def classify_intent(self, query: str, context: Optional[List[Dict]] = None) -> Dict[str, str]:
        """
        ğŸ¯ Smart AI-powered intent classification for dynamic response formatting
        Cost: ~200 tokens (~$0.001 per query)
        """
        
        # Build context summary if available
        context_info = ""
        if context and len(context) > 0:
            recent_messages = context[-3:] if len(context) > 3 else context
            context_info = f"\nContext from conversation: {' | '.join([msg.get('content', '')[:50] for msg in recent_messages])}"
        
        classification_prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

Ø§Ù„Ù…Ù‡Ù…Ø©: ØµÙ†Ù Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¥Ù„Ù‰ ÙØ¦Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·.

Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±: "{query}"{context_info}

Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:
1. rights_inquiry - Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (Ù…Ø«Ù„: "Ù…Ø§ Ø­Ù‚ÙˆÙ‚ÙŠØŸ", "Ù…Ø§ Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…ÙˆØ¸ÙØŸ")
2. procedure_guide - Ø³Ø¤Ø§Ù„ Ø¹Ù† ÙƒÙŠÙÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø¥Ø¬Ø±Ø§Ø¡ Ù‚Ø§Ù†ÙˆÙ†ÙŠ (Ù…Ø«Ù„: "ÙƒÙŠÙ Ø£Ø³Ø³ Ø´Ø±ÙƒØ©ØŸ", "Ù…Ø§ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø·Ù„Ø§Ù‚ØŸ")
3. legal_dispute - Ù…Ø´ÙƒÙ„Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© ØªØ­ØªØ§Ø¬ Ø­Ù„ (Ù…Ø«Ù„: "ØªÙ… ÙØµÙ„ÙŠ", "Ø£Ø±ÙŠØ¯ Ù…Ù‚Ø§Ø¶Ø§Ø©", "Ù„Ø¯ÙŠ Ù†Ø²Ø§Ø¹")
4. legal_consultation - Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¹Ø§Ù…Ø© Ø£Ùˆ Ø±Ø£ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ (Ù…Ø«Ù„: "Ù…Ø§ Ø±Ø£ÙŠÙƒ ÙÙŠØŸ", "Ù‡Ù„ ÙŠØ¬ÙˆØ²ØŸ")
5. document_review - Ù…Ø±Ø§Ø¬Ø¹Ø© Ø£Ùˆ ØªÙØ³ÙŠØ± ÙˆØ«ÙŠÙ‚Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (Ù…Ø«Ù„: "Ø§Ø´Ø±Ø­ Ù„ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù‚Ø¯", "Ù…Ø§ Ù…Ø¹Ù†Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ù†Ø¯ØŸ")
6. comparative_analysis - Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ† Ø®ÙŠØ§Ø±Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (Ù…Ø«Ù„: "Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ†ØŸ", "Ø£ÙŠÙ‡Ù…Ø§ Ø£ÙØ¶Ù„ØŸ")

Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙ:
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ¨Ø¯Ø£ Ø¨Ù€ "Ù…Ø§ Ø­Ù‚ÙˆÙ‚" Ø£Ùˆ "Ù…Ø§ Ø­Ù‚ÙŠ" â†’ rights_inquiry
- Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¨Ø¯Ø£ Ø¨Ù€ "ÙƒÙŠÙ" Ø£Ùˆ "Ù…Ø§ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª" â†’ procedure_guide  
- Ø¥Ø°Ø§ Ø°ÙƒØ± Ù…Ø´ÙƒÙ„Ø© Ø­Ø¯Ø«Øª ("ØªÙ…", "Ø­ØµÙ„", "Ù‚Ø§Ù…") â†’ legal_dispute
- Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ·Ù„Ø¨ Ø±Ø£ÙŠ Ø£Ùˆ ØªÙ‚ÙŠÙŠÙ… â†’ legal_consultation

Ø£Ø¬Ø¨ Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON ÙÙ‚Ø·:
{{
  "intent": "Ø§Ù„ÙØ¦Ø©_Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©",
  "confidence": 0.95,
  "reasoning": "Ø³Ø¨Ø¨ Ø§Ù„ØªØµÙ†ÙŠÙ",
  "suggested_format": "Ù†ÙˆØ¹_Ø§Ù„ØªÙ†Ø³ÙŠÙ‚_Ø§Ù„Ù…Ù†Ø§Ø³Ø¨"
}}"""

        try:
            # Use a lightweight model call for classification
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini" if "openai" in str(self.client.base_url) else "deepseek-chat",
                messages=[{"role": "user", "content": classification_prompt}],
                temperature=0.1,  # Low temperature for consistent classification
                max_tokens=200    # Small response for cost efficiency
            )
            
            classification_text = response.choices[0].message.content.strip()
            
            # Parse JSON response
            import json
            classification = json.loads(classification_text)
            
            print(f"ğŸ¯ Intent classified: {classification.get('intent')} (confidence: {classification.get('confidence', 0)})")
            
            return classification
            
        except Exception as e:
            print(f"âš ï¸ Intent classification failed: {e}")
            # Fallback to simple keyword detection
            query_lower = query.lower()
            
            if any(word in query_lower for word in ["Ø­Ù‚ÙˆÙ‚", "Ø­Ù‚ÙŠ", "Ø­Ù‚Ùƒ"]):
                return {
                    "intent": "rights_inquiry",
                    "confidence": 0.7,
                    "reasoning": "Keyword-based fallback detection",
                    "suggested_format": "rights_explanation"
                }
            elif any(word in query_lower for word in ["ÙƒÙŠÙ", "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª", "Ø®Ø·ÙˆØ§Øª"]):
                return {
                    "intent": "procedure_guide", 
                    "confidence": 0.7,
                    "reasoning": "Keyword-based fallback detection",
                    "suggested_format": "step_by_step_guide"
                }
            elif any(word in query_lower for word in ["ØªÙ…", "ÙØµÙ„", "Ù…Ù‚Ø§Ø¶Ø§Ø©", "Ù†Ø²Ø§Ø¹", "Ù…Ø´ÙƒÙ„Ø©"]):
                return {
                    "intent": "legal_dispute",
                    "confidence": 0.7, 
                    "reasoning": "Keyword-based fallback detection",
                    "suggested_format": "legal_memorandum"
                }
            else:
                return {
                    "intent": "legal_consultation",
                    "confidence": 0.6,
                    "reasoning": "Default classification",
                    "suggested_format": "general_consultation"
                }

    async def get_dynamic_agent_prompts(self, intent_classification: Dict[str, str]) -> Dict[LegalAgentType, str]:
        """
        ğŸ¯ Generate dynamic agent prompts based on intent classification
        """
        
        intent = intent_classification.get("intent", "legal_consultation")
        
        # Base prompts that adapt to intent
        base_prompts = {
            LegalAgentType.FACT_ANALYZER: f"""Ø£Ù†Øª Ù…Ø­Ù„Ù„ ÙˆÙ‚Ø§Ø¦Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ.

ØªÙ… ØªØµÙ†ÙŠÙ Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± ÙƒÙ€: {intent}

Ù…Ù‡Ù…ØªÙƒ Ø­Ø³Ø¨ Ø§Ù„ØªØµÙ†ÙŠÙ:
- rights_inquiry: Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© ÙˆØ§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„Ø©
- procedure_guide: Ø­Ø¯Ø¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙˆØ§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØ§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯
- legal_dispute: Ø­Ù„Ù„ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ§Ù„Ø£Ø·Ø±Ø§Ù ÙˆØ§Ù„Ø£Ø¶Ø±Ø§Ø±
- legal_consultation: Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø´Ø§Ù…Ù„Ø§Ù‹ Ù„Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ
- document_review: Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ù† Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©
- comparative_analysis: Ø­Ø¯Ø¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ÙˆØ§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ ÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹ Ø¨Ø¯ÙˆÙ† Ø°ÙƒØ± Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©.""",

            LegalAgentType.DOCUMENT_DRAFTER: f"""Ø£Ù†Øª Ù…Ø­Ø§Ù… Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØµÙŠØ§ØºØ© Ø±Ø¯ÙˆØ¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…ØªÙƒÙŠÙØ©.

Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±: {intent}

Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø±Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹:

**rights_inquiry - Ø´Ø±Ø­ Ø§Ù„Ø­Ù‚ÙˆÙ‚:**
```
ğŸ” Ø­Ù‚ÙˆÙ‚Ùƒ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙÙŠ [Ø§Ù„Ù…Ø¬Ø§Ù„]

**Ø§Ù„Ø­Ù‚ Ø§Ù„Ø£ÙˆÙ„**: [Ø´Ø±Ø­ Ù…ÙØµÙ„]
**Ø§Ù„Ù…Ø±Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ**: [Ø§Ù„Ù…Ø§Ø¯Ø© ÙˆØ§Ù„Ù†Ø¸Ø§Ù…]

**ÙƒÙŠÙÙŠØ© Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©**:
â€¢ [Ø®Ø·ÙˆØ§Øª Ø¹Ù…Ù„ÙŠØ©]

**Ø¬Ù‡Ø§Øª Ø§Ù„Ø­Ù…Ø§ÙŠØ©**:
â€¢ [Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ©]
```

**procedure_guide - Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª:**
```
ğŸ“‹ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù€ [Ø§Ù„Ù‡Ø¯Ù]

**Ø§Ù„Ø®Ø·ÙˆØ© 1**: [ØªÙØµÙŠÙ„ ÙƒØ§Ù…Ù„]
â€¢ Ø§Ù„Ù…Ø¯Ø©: [Ø§Ù„ÙˆÙ‚Øª]
â€¢ Ø§Ù„ØªÙƒÙ„ÙØ©: [Ø§Ù„Ù…Ø¨Ù„Øº]

**Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©**:
â€¢ [Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª]

**Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ©**:
â€¢ [Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„]
```

**legal_dispute - Ù…Ø°ÙƒØ±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
```
âš–ï¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù„Ù…ÙˆÙ‚Ù

**Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹**:
[Ø³Ø±Ø¯ Ù…Ù†Ø·Ù‚ÙŠ]

**Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**:
[Ø§Ù„Ø­Ø¬Ø¬ Ù…Ø¹ Ø§Ù„Ø£Ø¯Ù„Ø©]

**Ø§Ù„ØªÙˆØµÙŠØ§Øª**:
[Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©]
```

**legal_consultation - Ø§Ø³ØªØ´Ø§Ø±Ø© Ø¹Ø§Ù…Ø©:**
```
ğŸ’¡ Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

**Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ**:
[ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„]

**Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©**:
[Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©]

**Ø§Ù„ØªÙˆØµÙŠØ§Øª**:
[Ø§Ù„Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©]
```

Ø§Ø®ØªØ± Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙ†ÙŠÙ."""
        }
        
        # Return only our dynamic prompts
        return base_prompts
    

    async def _enhanced_legal_research_with_rag(self, query: str, context: Optional[str] = None) -> AsyncIterator[str]:
        """Enhanced legal research using RAG + AI reasoning"""
        
        try:
            # STEP 1: Use your RAG system to find relevant documents
            from rag_engine import rag_engine
            
            print("ğŸ” Searching RAG database for relevant legal documents...")
            
            # Get relevant documents from your vector database using your existing RAG
            relevant_docs = []
            async for chunk in rag_engine.ask_question_streaming(query):
                relevant_docs.append(chunk)
            
            rag_results = ''.join(relevant_docs)
            
            # STEP 2: Enhanced prompt with real legal documents
            enhanced_input = f"""
            Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:
            {rag_results}
            
            Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø§Ù„Ø£ØµÙ„ÙŠ: {query}
            
            Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: ØªØ­Ù„ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ© ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ù…Ø¹ Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙØ³ÙŠØ± ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ.
            """
            
            # STEP 3: AI reasoning with real legal documents
            async for chunk in self.agents[LegalAgentType.LEGAL_RESEARCHER].process_streaming(
                enhanced_input, context
            ):
                yield chunk
                
        except Exception as e:
            print(f"âŒ Enhanced RAG research failed: {e}")
            # Fallback to standard AI-only research
            async for chunk in self.agents[LegalAgentType.LEGAL_RESEARCHER].process_streaming(
                query, context
            ):
                yield chunk
            
    async def process_legal_query_streaming(
        self, 
        query: str, 
        enable_trust_trail: bool = False,
        conversation_context: Optional[List[Dict]] = None
    ) -> AsyncIterator[str]:
        """
        ğŸ¯ SMART STREAMING: AI-powered intent classification + dynamic response formatting + CITATION VALIDATION
        Cost: ~3,200 tokens (~$0.014 per query) - includes citation validation
        """
        
        start_time = time.time()
        context_summary = ""
        if conversation_context:
            context_summary = self._summarize_conversation_context(conversation_context)
        
        # Variables to store content for citation validation
        fact_content = ""
        research_content = ""
        draft_content = ""
        
        try:
            # ğŸ§  STEP 0: AI-POWERED INTENT CLASSIFICATION
            print("ğŸ§  Classifying intent with AI...")
            intent_classification = await self.classify_intent(query, conversation_context)
            intent = intent_classification.get('intent', 'legal_consultation')
            confidence = intent_classification.get('confidence', 0.8)
            
            print(f"ğŸ¯ Intent: {intent} (confidence: {confidence:.1%})")
            
            # ğŸ¯ STEP 1: GET DYNAMIC AGENT PROMPTS
            dynamic_prompts = await self.get_dynamic_agent_prompts(intent_classification)
            
            # Temporarily override agent prompts for this query
            original_prompts = {}
            for agent_type, prompt in dynamic_prompts.items():
                if agent_type in self.agents:
                    original_prompts[agent_type] = self.agents[agent_type].system_prompts.copy()
                    self.agents[agent_type].system_prompts[agent_type] = prompt
            
            # ğŸ” STEP 2: FACT ANALYSIS (Stream with intent-aware prompts)
            print("ğŸ” Step 1: Analyzing legal facts...")
            
            if intent in ['rights_inquiry', 'procedure_guide']:
                yield "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:\n\n"
            else:
                yield "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:\n\n"
            
            # Collect fact analysis content for citation validation
            fact_chunks = []
            async for chunk in self.agents[LegalAgentType.FACT_ANALYZER].process_streaming(
                query, context_summary
            ):
                fact_chunks.append(chunk)
                yield chunk
            
            fact_content = ''.join(fact_chunks)
            yield "\n\n"
            
            # ğŸ“š STEP 3: ENHANCED LEGAL RESEARCH (RAG + AI)
            print("ğŸ“š Step 2: Researching legal precedents with RAG...")

            if intent == 'rights_inquiry':
                yield "Ø§Ù„Ø£Ø³Ø³ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù„Ø­Ù‚ÙˆÙ‚Ùƒ:\n\n"
            elif intent == 'procedure_guide':
                yield "Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª:\n\n"
            elif intent == 'legal_dispute':
                yield "Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©:\n\n"
            else:
                yield "Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©:\n\n"

            # ğŸš€ ENHANCED: Use RAG + AI for legal research
            research_chunks = []
            async for chunk in self._enhanced_legal_research_with_rag(query, context_summary):
                research_chunks.append(chunk)
                yield chunk
            
            research_content = ''.join(research_chunks)
            yield "\n\n"
            
            # ğŸ—ï¸ STEP 4: DOCUMENT DRAFTING
            print("ğŸ“ Step 3: Drafting legal response...")
            
            # Pass intent information to the document drafter
            final_input = f"Intent: {intent}\nConfidence: {confidence}\nQuery: {query}"
            
            # Collect draft content for citation validation
            draft_chunks = []
            async for chunk in self.agents[LegalAgentType.DOCUMENT_DRAFTER].process_streaming(
                final_input, context_summary
            ):
                draft_chunks.append(chunk)
                yield chunk
            
            draft_content = ''.join(draft_chunks)
            yield "\n\n"
            
            # ğŸ” STEP 5: CITATION VALIDATION (NEW!)
            print("ğŸ” Step 4: Validating citations and legal references...")
            yield "ğŸ” **Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**\n\n"
            
            # Combine all content for comprehensive citation validation
            all_content_for_validation = f"""
    Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©:

    Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:
    {fact_content}

    Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:
    {research_content}

    Ø§Ù„Ù…Ø³ÙˆØ¯Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:
    {draft_content}

    Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© Ø£Ø¹Ù„Ø§Ù‡ ÙˆØªÙ‚ÙŠÙŠÙ… Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠÙ‡Ø§.
    """
            
            # Stream citation validation results
            async for chunk in self.agents[LegalAgentType.CITATION_VALIDATOR].process_streaming(
                all_content_for_validation, context_summary
            ):
                yield chunk
            
            # ğŸ”§ STEP 6: RESTORE ORIGINAL PROMPTS
            for agent_type, original_prompt in original_prompts.items():
                if agent_type in self.agents:
                    self.agents[agent_type].system_prompts = original_prompt
            
            print(f"âœ… Smart multi-agent streaming with citation validation completed. Intent: {intent}")
            
        except Exception as e:
            print(f"âŒ Smart multi-agent streaming failed: {e}")
            import traceback
            traceback.print_exc()
            
            # Restore prompts even if there's an error
            try:
                for agent_type, original_prompt in original_prompts.items():
                    if agent_type in self.agents:
                        self.agents[agent_type].system_prompts = original_prompt
            except:
                pass
            
            yield f"ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ Ø¨Ù†Ø¸Ø§Ù… Ù…Ø¨Ø³Ø· Ù†Ø¸Ø±Ø§Ù‹ Ù„Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ Ù…Ø¤Ù‚Øª.\n\n{query}\n\nÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ùˆ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªÙ‚Ø¯Ù…."
    async def _execute_agent_step(
        self,
        agent_type: LegalAgentType,
        step_number: int,
        step_name: str,
        input_data: str,
        context: Optional[str] = None
    ) -> AgentStep:
        """Execute individual agent step with timing and error handling"""
        
        step_start = time.time()
        
        try:
            output, citations, confidence = await self.agents[agent_type].process(input_data, context)
            
            processing_time = int((time.time() - step_start) * 1000)
            
            return AgentStep(
                agent_type=agent_type,
                step_number=step_number,
                step_name=step_name,
                input_data=input_data[:200] + "..." if len(input_data) > 200 else input_data,
                output_data=output,
                citations=citations,
                confidence_score=confidence,
                processing_time_ms=processing_time,
                timestamp=datetime.now().isoformat(),
                sources_verified=False
            )
            
        except Exception as e:
            return AgentStep(
                agent_type=agent_type,
                step_number=step_number,
                step_name=step_name,
                input_data=input_data[:200] + "..." if len(input_data) > 200 else input_data,
                output_data=f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}",
                citations=[],
                confidence_score=0.1,
                processing_time_ms=int((time.time() - step_start) * 1000),
                timestamp=datetime.now().isoformat(),
                sources_verified=False
            )
    
    def _compile_final_answer(self, reasoning_steps: List[AgentStep], enable_trust_trail: bool) -> str:
        """Compile final answer with optional trust trail"""
        
        # Get the main legal advice from the drafting step
        draft_step = next((step for step in reasoning_steps if step.agent_type == LegalAgentType.DOCUMENT_DRAFTER), None)
        main_answer = draft_step.output_data if draft_step else "ØªØ¹Ø°Ø± Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©."
        
        if not enable_trust_trail:
            return main_answer
        
        # Add trust trail for transparency
        trust_trail = "\n\n" + "="*60 + "\n"
        trust_trail += "ğŸ” **Ø³Ù„Ø³Ù„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ** (Trust Trail)\n"
        trust_trail += "="*60 + "\n\n"
        
        for step in reasoning_steps:
            trust_trail += f"**{step.step_number}. {step.step_name}**\n"
            trust_trail += f"â±ï¸ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {step.processing_time_ms}ms\n"
            trust_trail += f"ğŸ“Š Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: {step.confidence_score:.1%}\n"
            
            if step.citations:
                trust_trail += f"ğŸ“š Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹: {', '.join(step.citations[:3])}" 
                if len(step.citations) > 3:
                    trust_trail += f" Ùˆ {len(step.citations) - 3} Ù…Ø±Ø§Ø¬Ø¹ Ø£Ø®Ø±Ù‰"
                trust_trail += "\n"
            
            trust_trail += f"ğŸ’­ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {step.output_data[:150]}...\n\n"
        
        return main_answer + trust_trail
    
    def _summarize_conversation_context(self, context: List[Dict]) -> str:
        """Summarize conversation context for agents"""
        if not context or len(context) < 2:
            return ""
        
        # Get last few exchanges
        recent_context = context[-4:] if len(context) > 4 else context
        
        summary = "Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n"
        for msg in recent_context:
            role = "Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…" if msg.get("role") == "user" else "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯"
            content = msg.get("content", "")[:100]
            summary += f"- {role}: {content}...\n"
        
        return summary
    
    async def _fallback_response(self, query: str, start_time: float) -> LegalReasoningResult:
        """Fallback to simple response if multi-agent fails"""
        
        simple_response = f"""ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ Ø¨Ù†Ø¸Ø§Ù… Ù…Ø¨Ø³Ø· Ù†Ø¸Ø±Ø§Ù‹ Ù„Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ Ù…Ø¤Ù‚Øª.

{query}

ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ùˆ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªÙ‚Ø¯Ù…."""

        fallback_step = AgentStep(
            agent_type=LegalAgentType.FACT_ANALYZER,
            step_number=1,
            step_name="Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¨Ø³Ø·Ø©",
            input_data=query,
            output_data=simple_response,
            citations=[],
            confidence_score=0.3,
            processing_time_ms=int((time.time() - start_time) * 1000),
            timestamp=datetime.now().isoformat(),
            sources_verified=False
        )
        
        return LegalReasoningResult(
            query=query,
            final_answer=simple_response,
            reasoning_steps=[fallback_step],
            total_processing_time_ms=int((time.time() - start_time) * 1000),
            overall_confidence=0.3,
            trust_trail_enabled=False,
            citations_summary=[]
        )

class EnhancedRAGEngine:
    """
    ğŸš€ ENHANCED RAG ENGINE WITH MULTI-AGENT INTEGRATION
    
    This integrates with your existing rag_engine.py while adding multi-agent capabilities
    """
    
    def __init__(self):
        # Use the imported OpenAI client
        self.openai_client = openai_client
        self.orchestrator = MultiAgentLegalOrchestrator(self.openai_client)
        
        # Flag to enable/disable multi-agent processing
        self.multi_agent_enabled = True
    
    async def ask_question_with_multi_agent(
        self, 
        query: str, 
        conversation_context: Optional[List[Dict]] = None,
        enable_trust_trail: bool = False
    ) -> AsyncIterator[str]:
        """
        ğŸ¯ NEW METHOD: Multi-agent legal reasoning with streaming
        """
        
        if not self.multi_agent_enabled:
            # Fallback to existing single-agent (would need to import from rag_engine)
            try:
                from rag_engine import ask_question_with_context
                response = await ask_question_with_context(query, conversation_context or [])
                
                # Stream the response in chunks
                chunk_size = 50
                for i in range(0, len(response), chunk_size):
                    chunk = response[i:i + chunk_size]
                    yield chunk
                    await asyncio.sleep(0.03)
                return
            except ImportError:
                # If can't import, use simple fallback
                yield "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ØºÙŠØ± Ù…ØªØ§Ø­ Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹."
                return
        
        try:
            # Process with REAL streaming multi-agent system
            async for chunk in self.orchestrator.process_legal_query_streaming(
                query=query,
                enable_trust_trail=enable_trust_trail,
                conversation_context=conversation_context
            ):
                yield chunk  # Real-time streaming from agents
            
            # Send trust trail data if enabled
            if enable_trust_trail:
                trust_trail_data = {
                    "type": "trust_trail",
                    "reasoning_steps": [],  # Would need to be implemented
                    "citations_summary": []
                }
                yield f"\n\ndata: {json.dumps(trust_trail_data)}\n\n"
                
        except Exception as e:
            print(f"âŒ Multi-agent processing failed, using fallback: {e}")
            
            # Simple fallback response
            fallback_response = f"""ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ:

{query}

ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ø°Ø§ ÙƒÙ†Øª Ø¨Ø­Ø§Ø¬Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹."""
            
            chunk_size = 50
            for i in range(0, len(fallback_response), chunk_size):
                chunk = fallback_response[i:i + chunk_size]
                yield chunk
                await asyncio.sleep(0.03)

# Test function for debugging
async def test_multi_agent():
    """Test function to verify multi-agent system works"""
    try:
        enhanced_rag = EnhancedRAGEngine()
        
        query = "Ù…ÙˆØ¸Ù ØªÙ… ÙØµÙ„Ù‡ Ø¨Ø¯ÙˆÙ† Ù…Ø¨Ø±Ø±ØŒ Ù…Ø§ Ø­Ù‚ÙˆÙ‚Ù‡ØŸ"
        print(f"ğŸ§ª Testing multi-agent with query: {query}")
        
        response_chunks = []
        async for chunk in enhanced_rag.ask_question_with_multi_agent(
            query=query,
            enable_trust_trail=True
        ):
            response_chunks.append(chunk)
            print(chunk, end="", flush=True)
        
        print(f"\nâœ… Test completed. Total chunks: {len(response_chunks)}")
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        return False

if __name__ == "__main__":
    # Run test when file is executed directly
    import asyncio
    asyncio.run(test_multi_agent())