"""
Enterprise Saudi Legal Document Crawler
Bulletproof quality validation with duplicate prevention
Built for 500+ documents with zero incomplete files
"""

import asyncio
import hashlib
import re
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Set, Tuple, Any
from dataclasses import dataclass, asdict
from pathlib import Path
import logging

import aiohttp
import asyncio
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import unicodedata

# Import our services
from app.services.document_service import DocumentService

logger = logging.getLogger(__name__)


@dataclass
class CrawlerConfig:
    """Crawler configuration with quality controls"""
    max_documents: int = 50  # Start small, scale up
    batch_size: int = 10     # Process in small batches
    request_delay: float = 2.0  # Be respectful to servers
    min_content_length: int = 200
    min_arabic_ratio: float = 0.6
    min_quality_score: float = 7.0
    max_retries: int = 3
    timeout: int = 30


@dataclass
class DocumentQuality:
    """Document quality metrics"""
    content_length: int
    arabic_ratio: float
    legal_keywords_count: int
    structure_score: float
    duplicate_score: float
    overall_score: float
    
    def is_acceptable(self, config: CrawlerConfig) -> bool:
        """Check if document meets quality standards"""
        return (
            self.content_length >= config.min_content_length and
            self.arabic_ratio >= config.min_arabic_ratio and
            self.overall_score >= config.min_quality_score
        )


@dataclass
class CrawledDocument:
    """Crawled document with validation data"""
    url: str
    title: str
    content: str
    content_hash: str
    source_domain: str
    extraction_timestamp: datetime
    quality: DocumentQuality
    metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage"""
        return {
            'url': self.url,
            'title': self.title,
            'content': self.content,
            'content_hash': self.content_hash,
            'source_domain': self.source_domain,
            'extraction_timestamp': self.extraction_timestamp.isoformat(),
            'quality': asdict(self.quality),
            'metadata': self.metadata
        }


class ContentValidator:
    """Enterprise-grade content validation"""
    
    LEGAL_KEYWORDS = [
        'ÿßŸÑŸÖÿßÿØÿ©', 'ÿßŸÑŸÅÿµŸÑ', 'ÿßŸÑÿ®ÿßÿ®', 'ÿßŸÑÿ®ŸÜÿØ', 'ÿßŸÑŸÅŸÇÿ±ÿ©', 'ÿßŸÑŸÜÿ∏ÿßŸÖ', 'ÿßŸÑŸÑÿßÿ¶ÿ≠ÿ©',
        'ÿßŸÑŸÖÿ±ÿ≥ŸàŸÖ', 'ÿßŸÑŸÖŸÑŸÉŸä', 'Ÿàÿ≤ÿßÿ±ÿ©', 'ÿßŸÑÿπÿØŸÑ', 'ÿßŸÑŸÖÿ≠ŸÉŸÖÿ©', 'ÿßŸÑŸÇÿ∂ÿßÿ°', 'ÿßŸÑÿØÿπŸàŸâ',
        'ÿßŸÑÿ≠ŸÉŸÖ', 'ÿßŸÑÿ™ÿ¥ÿ±Ÿäÿπ', 'ÿßŸÑŸÇÿßŸÜŸàŸÜ', 'ÿßŸÑÿ£ŸÜÿ∏ŸÖÿ©', 'ÿßŸÑÿ™ŸÉÿßŸÑŸäŸÅ', 'ÿßŸÑÿ±ÿ≥ŸàŸÖ'
    ]
    
    ARTICLE_PATTERNS = [
        r'ÿßŸÑŸÖÿßÿØÿ©\s+[ÿßŸÑÿ£ŸàŸÑŸâ|ÿßŸÑÿ´ÿßŸÜŸäÿ©|ÿßŸÑÿ´ÿßŸÑÿ´ÿ©|\d+]',
        r'ÿßŸÑŸÅÿµŸÑ\s+[ÿßŸÑÿ£ŸàŸÑ|ÿßŸÑÿ´ÿßŸÜŸä|ÿßŸÑÿ´ÿßŸÑÿ´|\d+]',
        r'ÿßŸÑÿ®ÿßÿ®\s+[ÿßŸÑÿ£ŸàŸÑ|ÿßŸÑÿ´ÿßŸÜŸä|ÿßŸÑÿ´ÿßŸÑÿ´|\d+]'
    ]
    
    @classmethod
    def calculate_arabic_ratio(cls, text: str) -> float:
        """Calculate percentage of Arabic characters"""
        if not text:
            return 0.0
        
        arabic_chars = sum(1 for char in text if '\u0600' <= char <= '\u06FF')
        total_chars = len([c for c in text if c.isalpha()])
        
        return arabic_chars / total_chars if total_chars > 0 else 0.0
    
    @classmethod
    def count_legal_keywords(cls, text: str) -> int:
        """Count legal terminology occurrences"""
        text_lower = text.lower()
        return sum(1 for keyword in cls.LEGAL_KEYWORDS if keyword in text_lower)
    
    @classmethod
    def calculate_structure_score(cls, text: str) -> float:
        """Score document structure based on legal formatting"""
        score = 0.0
        
        # Check for article patterns
        for pattern in cls.ARTICLE_PATTERNS:
            if re.search(pattern, text):
                score += 2.0
        
        # Check for numbering
        if re.search(r'\d+\.', text):
            score += 1.0
        
        # Check for legal structure
        if 'ÿ™ÿπÿ±ŸäŸÅÿßÿ™' in text or 'ŸÜÿ∑ÿßŸÇ ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ' in text:
            score += 2.0
        
        return min(score, 10.0)  # Cap at 10
    
    @classmethod
    def validate_content(cls, original_content: str, extracted_content: str) -> bool:
        """Validate extracted content against original"""
        # Hash comparison
        original_hash = hashlib.sha256(original_content.encode()).hexdigest()
        extracted_hash = hashlib.sha256(extracted_content.encode()).hexdigest()
        
        if original_hash == extracted_hash:
            return True
        
        # Length comparison (allow 5% variance)
        length_ratio = len(extracted_content) / len(original_content)
        if 0.95 <= length_ratio <= 1.05:
            return True
        
        # Key phrase preservation check
        key_phrases = re.findall(r'ÿßŸÑŸÖÿßÿØÿ©\s+\w+', original_content)
        preserved_phrases = sum(1 for phrase in key_phrases if phrase in extracted_content)
        preservation_ratio = preserved_phrases / len(key_phrases) if key_phrases else 1.0
        
        return preservation_ratio >= 0.9
    
    @classmethod
    def calculate_quality_score(cls, content: str, metadata: Dict[str, Any]) -> DocumentQuality:
        """Calculate comprehensive quality score"""
        content_length = len(content)
        arabic_ratio = cls.calculate_arabic_ratio(content)
        legal_keywords = cls.count_legal_keywords(content)
        structure_score = cls.calculate_structure_score(content)
        
        # Overall score calculation
        length_score = min(content_length / 500, 10.0)  # 500 chars = max score
        arabic_score = arabic_ratio * 10
        keywords_score = min(legal_keywords / 5, 10.0)  # 5 keywords = max score
        
        overall_score = (length_score + arabic_score + keywords_score + structure_score) / 4
        
        return DocumentQuality(
            content_length=content_length,
            arabic_ratio=arabic_ratio,
            legal_keywords_count=legal_keywords,
            structure_score=structure_score,
            duplicate_score=0.0,  # Will be calculated later
            overall_score=overall_score
        )


class DuplicateDetector:
    """Advanced duplicate detection system"""
    
    def __init__(self):
        self.seen_hashes: Set[str] = set()
        self.seen_titles: Set[str] = set()
        self.processed_documents: List[CrawledDocument] = []
    
    def is_duplicate_hash(self, content_hash: str) -> bool:
        """Check for exact content duplicates"""
        return content_hash in self.seen_hashes
    
    def is_duplicate_title(self, title: str) -> bool:
        """Check for title duplicates"""
        normalized_title = self.normalize_title(title)
        return normalized_title in self.seen_titles
    
    def normalize_title(self, title: str) -> str:
        """Normalize title for comparison"""
        # Remove diacritics and extra spaces
        normalized = unicodedata.normalize('NFD', title)
        normalized = re.sub(r'[\u064B-\u0652]', '', normalized)  # Remove diacritics
        normalized = re.sub(r'\s+', ' ', normalized).strip().lower()
        return normalized
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """Calculate text similarity (simple word overlap)"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def find_similar_documents(self, document: CrawledDocument, threshold: float = 0.8) -> List[CrawledDocument]:
        """Find documents with high similarity"""
        similar = []
        
        for existing_doc in self.processed_documents:
            similarity = self.calculate_similarity(document.content, existing_doc.content)
            if similarity >= threshold:
                similar.append(existing_doc)
        
        return similar
    
    def add_document(self, document: CrawledDocument) -> bool:
        """Add document to tracking, return False if duplicate"""
        # Check exact duplicates
        if self.is_duplicate_hash(document.content_hash):
            logger.warning(f"Exact duplicate found: {document.title}")
            return False
        
        if self.is_duplicate_title(document.title):
            logger.warning(f"Title duplicate found: {document.title}")
            return False
        
        # Check similarity
        similar_docs = self.find_similar_documents(document)
        if similar_docs:
            logger.warning(f"Similar document found for: {document.title}")
            # Could implement smart merging here
        
        # Add to tracking
        self.seen_hashes.add(document.content_hash)
        self.seen_titles.add(self.normalize_title(document.title))
        self.processed_documents.append(document)
        
        return True


class ProgressTracker:
    """Progress tracking with checkpoint system"""
    
    def __init__(self, checkpoint_file: str = "data/crawler_progress.json"):
        self.checkpoint_file = Path(checkpoint_file)
        self.progress = self.load_progress()
    
    def load_progress(self) -> Dict[str, Any]:
        """Load progress from checkpoint file"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Error loading progress: {e}")
        
        return {
            'processed_urls': [],
            'failed_urls': [],
            'last_checkpoint': None,
            'total_processed': 0,
            'total_successful': 0,
            'quality_stats': {}
        }
    
    def save_progress(self):
        """Save current progress to checkpoint"""
        try:
            self.checkpoint_file.parent.mkdir(exist_ok=True)
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"Error saving progress: {e}")
    
    def add_processed_url(self, url: str, success: bool = True):
        """Track processed URL"""
        if success:
            self.progress['processed_urls'].append(url)
            self.progress['total_successful'] += 1
        else:
            self.progress['failed_urls'].append(url)
        
        self.progress['total_processed'] += 1
        
        # Save checkpoint every 10 documents
        if self.progress['total_processed'] % 10 == 0:
            self.progress['last_checkpoint'] = datetime.now().isoformat()
            self.save_progress()
            logger.info(f"Checkpoint saved: {self.progress['total_processed']} documents processed")
    
    def is_processed(self, url: str) -> bool:
        """Check if URL was already processed"""
        return url in self.progress['processed_urls'] or url in self.progress['failed_urls']
    
    def get_stats(self) -> Dict[str, Any]:
        """Get progress statistics"""
        return {
            'total_processed': self.progress['total_processed'],
            'successful': self.progress['total_successful'],
            'failed': len(self.progress['failed_urls']),
            'success_rate': self.progress['total_successful'] / max(self.progress['total_processed'], 1),
            'last_checkpoint': self.progress['last_checkpoint']
        }


class SaudiLegalCrawler:
    """Enterprise Saudi Legal Document Crawler"""
    
    def __init__(self, config: CrawlerConfig, document_service: DocumentService):
        self.config = config
        self.document_service = document_service
        self.validator = ContentValidator()
        self.duplicate_detector = DuplicateDetector()
        self.progress_tracker = ProgressTracker()
        
        self.session: Optional[aiohttp.ClientSession] = None
        self.stats = {
            'started_at': None,
            'total_urls_found': 0,
            'total_processed': 0,
            'successful_extractions': 0,
            'quality_rejections': 0,
            'duplicates_found': 0,
            'errors': 0
        }
        
        logger.info(f"SaudiLegalCrawler initialized with config: {asdict(config)}")
    
    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=self.config.timeout),
            headers={
                'User-Agent': 'SaudiLegalBot/1.0 (Legal Research Tool)'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    async def discover_document_urls(self, base_url: str = "https://laws.moj.gov.sa") -> List[str]:
        """Discover legal document URLs from MOJ website"""
        urls = []
        
        # Target pages for document discovery
        discovery_urls = [
            f"{base_url}/ar/legislations-regulations",
            f"{base_url}/ar/Laws/Pages/default.aspx",
            f"{base_url}/ar/LawsAndRegulations"
        ]
        
        logger.info(f"Starting URL discovery from {len(discovery_urls)} pages")
        
        for discovery_url in discovery_urls:
            try:
                page_urls = await self.extract_urls_from_page(discovery_url)
                urls.extend(page_urls)
                logger.info(f"Found {len(page_urls)} URLs from {discovery_url}")
                
                # Respectful delay
                await asyncio.sleep(self.config.request_delay)
                
            except Exception as e:
                logger.error(f"Error discovering URLs from {discovery_url}: {e}")
        
        # Remove duplicates and filter
        unique_urls = list(set(urls))
        filtered_urls = self.filter_legal_urls(unique_urls)
        
        logger.info(f"Discovery complete: {len(filtered_urls)} legal document URLs found")
        self.stats['total_urls_found'] = len(filtered_urls)
        
        return filtered_urls[:self.config.max_documents]
    
    async def extract_urls_from_page(self, url: str) -> List[str]:
        """Extract document URLs from a discovery page"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    logger.warning(f"Failed to fetch {url}: status {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # Extract all links
                links = []
                for a_tag in soup.find_all('a', href=True):
                    href = a_tag.get('href')
                    if href:
                        full_url = urljoin(url, href)
                        links.append(full_url)
                
                return links
                
        except Exception as e:
            logger.error(f"Error extracting URLs from {url}: {e}")
            return []
    
    def filter_legal_urls(self, urls: List[str]) -> List[str]:
        """Filter URLs to keep only legal documents"""
        legal_patterns = [
            r'/Laws/',
            r'/Regulations/',
            r'/legislation',
            r'ŸÜÿ∏ÿßŸÖ',
            r'ŸÇÿßŸÜŸàŸÜ',
            r'ŸÖÿ±ÿ≥ŸàŸÖ',
            r'ŸÑÿßÿ¶ÿ≠ÿ©'
        ]
        
        filtered = []
        for url in urls:
            # Must be from government domain
            if not any(domain in url for domain in ['moj.gov.sa', 'laws.gov.sa']):
                continue
            
            # Must match legal patterns
            if any(re.search(pattern, url, re.IGNORECASE) for pattern in legal_patterns):
                filtered.append(url)
        
        return filtered
    
    async def crawl_documents(self) -> Dict[str, Any]:
        """Main crawling orchestration"""
        self.stats['started_at'] = datetime.now()
        
        logger.info("üöÄ Starting Saudi Legal Document Crawler")
        logger.info(f"üìã Target: {self.config.max_documents} documents")
        logger.info(f"üîç Quality threshold: {self.config.min_quality_score}/10")
        
        try:
            # Phase 1: Discover URLs
            urls = await self.discover_document_urls()
            if not urls:
                raise Exception("No legal document URLs found")
            
            # Phase 2: Process in batches
            successful_documents = []
            
            for i in range(0, len(urls), self.config.batch_size):
                batch_urls = urls[i:i + self.config.batch_size]
                batch_num = (i // self.config.batch_size) + 1
                
                logger.info(f"üì¶ Processing batch {batch_num}: {len(batch_urls)} URLs")
                
                batch_results = await self.process_batch(batch_urls)
                successful_documents.extend(batch_results)
                
                # Progress report
                self.log_progress_report(batch_num, len(urls) // self.config.batch_size + 1)
                
                # Respectful delay between batches
                if i + self.config.batch_size < len(urls):
                    await asyncio.sleep(self.config.request_delay * 2)
            
            # Phase 3: Final quality check and storage
            final_results = await self.finalize_documents(successful_documents)
            
            # Phase 4: Generate final report
            return self.generate_final_report(final_results)
            
        except Exception as e:
            logger.error(f"Crawling failed: {e}")
            raise
    
    async def process_batch(self, urls: List[str]) -> List[CrawledDocument]:
        """Process a batch of URLs"""
        tasks = []
        
        for url in urls:
            # Skip if already processed
            if self.progress_tracker.is_processed(url):
                logger.info(f"Skipping already processed URL: {url}")
                continue
            
            task = self.process_single_document(url)
            tasks.append(task)
        
        # Process concurrently but respectfully
        results = []
        for task in tasks:
            try:
                result = await task
                if result:
                    results.append(result)
                # Delay between requests
                await asyncio.sleep(self.config.request_delay)
            except Exception as e:
                logger.error(f"Batch processing error: {e}")
                self.stats['errors'] += 1
        
        return results
    
    async def process_single_document(self, url: str) -> Optional[CrawledDocument]:
        """Process a single document with full validation"""
        try:
            logger.info(f"üîÑ Processing: {url}")
            
            # Extract content
            document = await self.extract_document_content(url)
            if not document:
                self.progress_tracker.add_processed_url(url, success=False)
                return None
            
            # Quality validation
            if not document.quality.is_acceptable(self.config):
                logger.warning(f"‚ùå Quality rejected: {document.title} (score: {document.quality.overall_score:.1f})")
                self.stats['quality_rejections'] += 1
                self.progress_tracker.add_processed_url(url, success=False)
                return None
            
            # Duplicate check
            if not self.duplicate_detector.add_document(document):
                logger.warning(f"‚ùå Duplicate rejected: {document.title}")
                self.stats['duplicates_found'] += 1
                self.progress_tracker.add_processed_url(url, success=False)
                return None
            
            # Success
            logger.info(f"‚úÖ Accepted: {document.title} (score: {document.quality.overall_score:.1f})")
            self.stats['successful_extractions'] += 1
            self.progress_tracker.add_processed_url(url, success=True)
            
            return document
            
        except Exception as e:
            logger.error(f"‚ùå Error processing {url}: {e}")
            self.stats['errors'] += 1
            self.progress_tracker.add_processed_url(url, success=False)
            return None
    
    async def extract_document_content(self, url: str) -> Optional[CrawledDocument]:
        """Extract and validate document content"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    logger.warning(f"Failed to fetch {url}: status {response.status}")
                    return None
                
                html = await response.text()
                
                # Parse content
                soup = BeautifulSoup(html, 'html.parser')
                
                # Extract title
                title = self.extract_title(soup, url)
                if not title:
                    logger.warning(f"No title found for {url}")
                    return None
                
                # Extract main content
                content = self.extract_main_content(soup)
                if not content:
                    logger.warning(f"No content found for {url}")
                    return None
                
                # Generate content hash
                content_hash = hashlib.sha256(content.encode()).hexdigest()
                
                # Calculate quality
                metadata = {
                    'source_url': url,
                    'domain': urlparse(url).netloc,
                    'extraction_method': 'beautifulsoup',
                    'html_length': len(html)
                }
                
                quality = self.validator.calculate_quality_score(content, metadata)
                
                return CrawledDocument(
                    url=url,
                    title=title,
                    content=content,
                    content_hash=content_hash,
                    source_domain=urlparse(url).netloc,
                    extraction_timestamp=datetime.now(),
                    quality=quality,
                    metadata=metadata
                )
                
        except Exception as e:
            logger.error(f"Content extraction error for {url}: {e}")
            return None
    
    def extract_title(self, soup: BeautifulSoup, url: str) -> Optional[str]:
        """Extract document title"""
        # Try multiple selectors
        title_selectors = [
            'h1',
            '.page-title',
            '.document-title',
            'title',
            '.main-heading'
        ]
        
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element and element.get_text(strip=True):
                return element.get_text(strip=True)
        
        # Fallback to URL-based title
        return urlparse(url).path.split('/')[-1] or "Untitled Document"
    
    def extract_main_content(self, soup: BeautifulSoup) -> Optional[str]:
        """Extract main document content"""
        # Remove unwanted elements
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()
        
        # Try content selectors
        content_selectors = [
            '.document-content',
            '.main-content',
            '.content',
            'main',
            'article',
            '.law-text'
        ]
        
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                content = element.get_text(strip=True)
                if len(content) > 100:  # Minimum content check
                    return content
        
        # Fallback: get all text
        content = soup.get_text(strip=True)
        return content if len(content) > 100 else None
    
    def log_progress_report(self, current_batch: int, total_batches: int):
        """Log progress report"""
        progress_pct = (current_batch / total_batches) * 100
        
        logger.info(f"üìä Progress Report - Batch {current_batch}/{total_batches} ({progress_pct:.1f}%)")
        logger.info(f"   ‚úÖ Successful: {self.stats['successful_extractions']}")
        logger.info(f"   ‚ùå Quality rejected: {self.stats['quality_rejections']}")
        logger.info(f"   üîÑ Duplicates: {self.stats['duplicates_found']}")
        logger.info(f"   ‚ö†Ô∏è Errors: {self.stats['errors']}")
    
    async def finalize_documents(self, documents: List[CrawledDocument]) -> Dict[str, Any]:
        """Store documents in database"""
        logger.info(f"üíæ Storing {len(documents)} validated documents in database...")
        
        # Convert to document service format
        doc_dicts = []
        for doc in documents:
            doc_dict = {
                'id': f"crawled_{hashlib.md5(doc.url.encode()).hexdigest()[:8]}",
                'title': doc.title,
                'content': doc.content,
                'metadata': {
                    **doc.metadata,
                    'quality_score': doc.quality.overall_score,
                    'arabic_ratio': doc.quality.arabic_ratio,
                    'content_hash': doc.content_hash,
                    'extraction_timestamp': doc.extraction_timestamp.isoformat()
                }
            }
            doc_dicts.append(doc_dict)
        
        # Store in database
        result = await self.document_service.add_documents_batch(doc_dicts)
        
        logger.info(f"üìä Storage result: {result['successful']} stored, {result['errors']} errors")
        
        return {
            'documents_processed': len(documents),
            'storage_result': result,
            'final_documents': doc_dicts
        }
    
    def generate_final_report(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive final report"""
        duration = datetime.now() - self.stats['started_at']
        
        report = {
            'crawler_summary': {
                'started_at': self.stats['started_at'].isoformat(),
                'duration_minutes': duration.total_seconds() / 60,
                'total_urls_discovered': self.stats['total_urls_found'],
                'documents_processed': results['documents_processed'],
                'success_rate': (self.stats['successful_extractions'] / max(self.stats['total_urls_found'], 1)) * 100
            },
            'quality_metrics': {
                'successful_extractions': self.stats['successful_extractions'],
                'quality_rejections': self.stats['quality_rejections'],
                'duplicates_found': self.stats['duplicates_found'],
                'errors': self.stats['errors'],
                'quality_acceptance_rate': (self.stats['successful_extractions'] / max(self.stats['successful_extractions'] + self.stats['quality_rejections'], 1)) * 100
            },
            'storage_results': results['storage_result'],
            'configuration': asdict(self.config),
            'next_steps': {
                'documents_in_database': results['storage_result']['successful'],
                'ready_for_rag': True,
                'recommended_next_batch_size': min(self.config.max_documents * 2, 100)
            }
        }
        
        # Save final report
        self.save_crawler_report(report)
        
        return report
    
    def save_crawler_report(self, report: Dict[str, Any]):
        """Save crawler report to file"""
        try:
            report_file = Path(f"data/crawler_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            report_file.parent.mkdir(exist_ok=True)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"üìÑ Crawler report saved: {report_file}")
            
        except Exception as e:
            logger.error(f"Error saving crawler report: {e}")