"""
Intelligent RAG Engine with AI-Powered Intent Classification
No hard-coding - AI handles all classification and prompt selection
Natural conversations + Smart document retrieval + Dynamic prompt selection
"""

import os
import logging
from datetime import datetime
from dotenv import load_dotenv
from openai import AsyncOpenAI
from typing import List, Dict, Optional, AsyncIterator
import json

# Import the smart database components from old RAG
from app.storage.vector_store import VectorStore, Chunk
from app.storage.sqlite_store import SqliteVectorStore
class SimpleCitationFixer:
    """MEMO-AWARE Citation Fixer - Removes ALL memo citations of any type"""
    
    def fix_citations(self, ai_response: str, available_documents: List[Chunk]) -> str:
        """Remove ALL memo citations and enhance statute citations"""
        if not available_documents:
            return ai_response
        
        import re
        
        # Get statute titles only (no memos)
        real_titles = [doc.title for doc in available_documents]
        statute_titles = [title for title in real_titles 
                 if any(term in title for term in [
                     "ŸÜÿ∏ÿßŸÖ", "ÿßŸÑŸÖÿßÿØÿ©", "ŸÑÿßÿ¶ÿ≠ÿ©", "ŸÖÿ±ÿ≥ŸàŸÖ", "ÿßŸÑÿ™ÿπÿ±ŸäŸÅÿßÿ™",  # ONLY real laws
                     "ŸÇÿßŸÜŸàŸÜ", "ŸÇÿ±ÿßÿ± Ÿàÿ≤ÿßÿ±Ÿä", "ÿ™ÿπŸÑŸäŸÖÿßÿ™", "ÿ∂Ÿàÿßÿ®ÿ∑", "ŸÇŸàÿßÿπÿØ"  # Official regulations
                 ]) 
                 and 'ŸÖÿ∞ŸÉÿ±ÿ©' not in title.lower()  # Exclude memos
                 and 'ÿØŸÅÿπ' not in title.lower()    # Exclude case defenses  
                 and 'ÿ≠ÿ¨ÿ©' not in title.lower()    # Exclude case arguments
                 and 'ÿ±ŸÇŸÖ' not in title.lower()]   # Exclude numbered cases
        
        if not statute_titles:
            return ai_response
        
        fixed_response = ai_response
        
        # 1. REMOVE ALL memo citations (comprehensive patterns for ANY memo type)
        memo_citation_patterns = [
            # Direct memo citations with quotes
            r'ŸàŸÅŸÇÿßŸã\s*ŸÑŸÄ\s*["\']?ŸÖÿ∞ŸÉÿ±ÿ©[^"\'.\n]*["\']?',
            r'ÿßÿ≥ÿ™ŸÜÿßÿØÿßŸã\s*ÿ•ŸÑŸâ\s*["\']?ŸÖÿ∞ŸÉÿ±ÿ©[^"\'.\n]*["\']?',
            r'ÿ®ŸÜÿßÿ°Ÿã\s*ÿπŸÑŸâ\s*["\']?ŸÖÿ∞ŸÉÿ±ÿ©[^"\'.\n]*["\']?',
            r'ÿ≠ÿ≥ÿ®\s*["\']?ŸÖÿ∞ŸÉÿ±ÿ©[^"\'.\n]*["\']?',
            r'ÿ∑ÿ®ŸÇÿßŸã\s*ŸÑŸÄ\s*["\']?ŸÖÿ∞ŸÉÿ±ÿ©[^"\'.\n]*["\']?',
            r'ÿ®ŸÖŸàÿ¨ÿ®\s*["\']?ŸÖÿ∞ŸÉÿ±ÿ©[^"\'.\n]*["\']?',
            
            # Phrase-based memo references
            r'ÿ®ÿßŸÑÿ•ÿ¥ÿßÿ±ÿ©\s*ÿ•ŸÑŸâ\s*["\']?[*]*ŸÖÿ∞ŸÉÿ±ÿ©[^"\'.\n]*[*]*["\']?',
            r'ŸÉŸÖÿß\s*ÿ¨ÿßÿ°\s*ŸÅŸä\s*["\']?ŸÖÿ∞ŸÉÿ±ÿ©[^"\'.\n]*["\']?',
            r'ŸàŸàŸÅŸÇÿßŸã\s*ŸÑŸÖÿß\s*Ÿàÿ±ÿØ\s*ŸÅŸä\s*["\']?ŸÖÿ∞ŸÉÿ±ÿ©[^"\'.\n]*["\']?',
            
            # Generic memo references without citation words
            r'ŸÖÿ∞ŸÉÿ±ÿ©\s*civil[^"\'.\n]*',
            r'ŸÖÿ∞ŸÉÿ±ÿ©\s*criminal[^"\'.\n]*', 
            r'ŸÖÿ∞ŸÉÿ±ÿ©\s*family[^"\'.\n]*',
            r'ŸÖÿ∞ŸÉÿ±ÿ©\s*execution[^"\'.\n]*',
            r'ŸÖÿ∞ŸÉÿ±ÿ©\s*\w+[^"\'.\n]*',  # Any memo type
            
            # Reference numbering
            r'ŸÖÿ±ÿ¨ÿπ\s*\d+[:\s]*[^".\n]*',
            r'ÿßŸÑŸÖÿ±ÿ¨ÿπ\s*ÿ±ŸÇŸÖ\s*\d+[^".\n]*',
        ]
        
        for pattern in memo_citation_patterns:
            fixed_response = re.sub(pattern, '', fixed_response, flags=re.IGNORECASE)
        
        # 2. Clean up broken text after memo removal
        cleanup_patterns = [
            (r'ÿå\s*ÿå', 'ÿå'),  # Double commas
            (r'\.\s*\.', '.'),  # Double periods
            (r':\s*ÿå', ':'),   # Colon followed by comma
            (r'^\s*ÿå', ''),    # Leading comma on line
            (r'^\s*\.', ''),   # Leading period on line
            (r'\n\s*\n\s*\n+', '\n\n'),  # Multiple line breaks
            (r'\s+', ' '),     # Multiple spaces
        ]
        
        for pattern, replacement in cleanup_patterns:
            fixed_response = re.sub(pattern, replacement, fixed_response, flags=re.MULTILINE)
        
        # 3. Find and replace weak citations with strong statute citations
        citation_patterns = [
            # Pattern: ŸàŸÅŸÇÿßŸã ŸÑŸÄ"anything" -> replace with real statute
            (r'ŸàŸÅŸÇÿßŸã ŸÑŸÄ"[^"]*"', f'ŸàŸÅŸÇÿßŸã ŸÑŸÄ"{statute_titles[0]}"'),
            # Pattern: ÿßÿ≥ÿ™ŸÜÿßÿØÿßŸã ÿ•ŸÑŸâ "anything" -> replace with real statute  
            (r'ÿßÿ≥ÿ™ŸÜÿßÿØÿßŸã ÿ•ŸÑŸâ "[^"]*"', f'ÿßÿ≥ÿ™ŸÜÿßÿØÿßŸã ÿ•ŸÑŸâ "{statute_titles[1] if len(statute_titles) > 1 else statute_titles[0]}"'),
            # Pattern: ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ "anything" -> replace with real statute
            (r'ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ "[^"]*"', f'ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ "{statute_titles[2] if len(statute_titles) > 2 else statute_titles[0]}"'),
            # Pattern: ÿ≠ÿ≥ÿ® "anything" -> replace with real statute
            (r'ÿ≠ÿ≥ÿ® "[^"]*"', f'ÿ≠ÿ≥ÿ® "{statute_titles[0]}"'),
        ]
        
        for pattern, replacement in citation_patterns:
            if re.search(pattern, fixed_response):
                fixed_response = re.sub(pattern, replacement, fixed_response, count=1)
        
        # 4. Fix generic weak references  
        generic_fixes = [
            (r'ŸàŸÅŸÇÿßŸã ŸÑŸÑŸÖÿßÿØÿ© ÿßŸÑÿ´ÿßŸÑÿ´ÿ©(?!\s*ŸÖŸÜ\s*")', f'ŸàŸÅŸÇÿßŸã ŸÑŸÄ"{statute_titles[0]}"'),
            (r'ÿßÿ≥ÿ™ŸÜÿßÿØÿßŸã ŸÑŸÑŸÖÿßÿØÿ©(?!\s*ŸÖŸÜ\s*")', f'ÿßÿ≥ÿ™ŸÜÿßÿØÿßŸã ÿ•ŸÑŸâ "{statute_titles[0]}"'),
            (r'ÿ≠ÿ≥ÿ® ÿßŸÑŸÖÿßÿØÿ©(?!\s*ŸÖŸÜ\s*")', f'ÿ≠ÿ≥ÿ® "{statute_titles[0]}"'),
            (r'ÿ®ŸÖŸàÿ¨ÿ® ÿßŸÑŸÖÿßÿØÿ©(?!\s*ŸÖŸÜ\s*")', f'ÿ®ŸÖŸàÿ¨ÿ® "{statute_titles[0]}"'),
        ]
        
        for pattern, replacement in generic_fixes:
            fixed_response = re.sub(pattern, replacement, fixed_response)
        
        # 5. Add proper statute citation if completely missing
        if 'ŸàŸÅŸÇÿßŸã ŸÑ' in fixed_response and not any(title in fixed_response for title in statute_titles):
            # Find the first occurrence of ŸàŸÅŸÇÿßŸã ŸÑ and make it proper
            fixed_response = re.sub(r'ŸàŸÅŸÇÿßŸã ŸÑ([^"]+)', f'ŸàŸÅŸÇÿßŸã ŸÑŸÄ"{statute_titles[0]}"', fixed_response, count=1)
        
        # 6. Ensure we have at least one proper citation in legal responses
        # 6. PROACTIVE CITATION INJECTION - Add citations for unused statutes
        available_statutes = [title for title in statute_titles if title not in fixed_response]
        if available_statutes:
            logger.info(f"üéØ Found {len(available_statutes)} unused statutes for injection")
            
            # Injection points - places where we can add citations naturally
            injection_opportunities = [
                # After legal analysis headers
                (r'(#### ÿ£ŸàŸÑÿßŸã: [^\n]+)', rf'\1\nŸàŸÅŸÇÿßŸã ŸÑŸÄ"{available_statutes[0]}"ÿå '),
                (r'(#### ÿ´ÿßŸÜŸäÿßŸã: [^\n]+)', rf'\1\nÿßÿ≥ÿ™ŸÜÿßÿØÿßŸã ÿ•ŸÑŸâ "{available_statutes[1] if len(available_statutes) > 1 else available_statutes[0]}"ÿå '),
                (r'(#### ÿ´ÿßŸÑÿ´ÿßŸã: [^\n]+)', rf'\1\nÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ "{available_statutes[2] if len(available_statutes) > 2 else available_statutes[0]}"ÿå '),
                
                # After conclusion headers
                (r'(ÿßŸÑÿÆÿßÿ™ŸÖÿ©[^\n]*)', rf'\1\nÿ≠ÿ≥ÿ® "{available_statutes[-1]}"ÿå '),
                (r'(ÿßŸÑÿÆŸÑÿßÿµÿ©[^\n]*)', rf'\1\nÿ∑ÿ®ŸÇÿßŸã ŸÑŸÄ"{available_statutes[-1]}"ÿå '),
                
                # Before final recommendation
                (r'(ŸÜÿ∑ŸÑÿ® ŸÖŸÜ ÿßŸÑŸÖÿ≠ŸÉŸÖÿ©)', rf'ŸàŸÅŸÇÿßŸã ŸÑŸÄ"{available_statutes[0]}"ÿå \1'),
                (r'(ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ŸÖÿß ÿ≥ÿ®ŸÇ)', rf'\1 Ÿàÿßÿ≥ÿ™ŸÜÿßÿØÿßŸã ÿ•ŸÑŸâ "{available_statutes[1] if len(available_statutes) > 1 else available_statutes[0]}"ÿå '),
            ]
            
            # Apply injections with SMART STATUTE ROTATION
            injected_count = 0
            used_statutes = set()

            for pattern, _ in injection_opportunities:
                if injected_count < len(available_statutes) and re.search(pattern, fixed_response):
                    # Pick next unused statute
                    statute_to_use = None
                    for statute in available_statutes:
                        if statute not in used_statutes:
                            statute_to_use = statute
                            break
                    
                    if statute_to_use:
                        # Create citation based on section type
                        if 'ÿ£ŸàŸÑÿßŸã' in pattern:
                            replacement = rf'\1\nŸàŸÅŸÇÿßŸã ŸÑŸÄ"{statute_to_use}"ÿå '
                        elif 'ÿ´ÿßŸÜŸäÿßŸã' in pattern:
                            replacement = rf'\1\nÿßÿ≥ÿ™ŸÜÿßÿØÿßŸã ÿ•ŸÑŸâ "{statute_to_use}"ÿå '
                        elif 'ÿ´ÿßŸÑÿ´ÿßŸã' in pattern:
                            replacement = rf'\1\nÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ "{statute_to_use}"ÿå '
                        elif 'ÿ±ÿßÿ®ÿπÿßŸã' in pattern:
                            replacement = rf'\1\nÿ≠ÿ≥ÿ® "{statute_to_use}"ÿå '
                        elif 'ÿÆÿßŸÖÿ≥ÿßŸã' in pattern:
                            replacement = rf'\1\nÿ∑ÿ®ŸÇÿßŸã ŸÑŸÄ"{statute_to_use}"ÿå '
                        elif 'ÿßŸÑÿÆÿßÿ™ŸÖÿ©' in pattern:
                            replacement = rf'\1\nŸàŸàŸÅŸÇÿßŸã ŸÑŸÄ"{statute_to_use}"ÿå '
                        else:
                            replacement = rf'ŸàŸÅŸÇÿßŸã ŸÑŸÄ"{statute_to_use}"ÿå \1'
                        
                        fixed_response = re.sub(pattern, replacement, fixed_response, count=1)
                        used_statutes.add(statute_to_use)
                        injected_count += 1
                        logger.info(f"üíâ Injected citation #{injected_count}: {statute_to_use[:50]}...")

            logger.info(f"‚úÖ Successfully injected {injected_count} DIFFERENT statute citations")
            
            logger.info(f"‚úÖ Successfully injected {injected_count} additional statute citations")
        has_proper_citation = any(f'"{title}"' in fixed_response for title in statute_titles)
        
        if not has_proper_citation and statute_titles and len(fixed_response) > 500:  # Only for substantial responses
            # Add a citation at strategic legal analysis points
            insertion_points = [
                r'(ÿ£ŸàŸÑÿßŸã: [^\n]*)',
                r'(### [^\n]*)', 
                r'(#### [^\n]*)',
                r'(ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ£ÿØŸÑÿ©)',
                r'(ÿßŸÑÿ±ÿØ ÿßŸÑŸÇÿßŸÜŸàŸÜŸä)',
                r'(ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÇÿßŸÜŸàŸÜŸä)'
            ]
            
            for pattern in insertion_points:
                if re.search(pattern, fixed_response):
                    replacement = f'\\1\nŸàŸÅŸÇÿßŸã ŸÑŸÄ"{statute_titles[0]}"ÿå '
                    fixed_response = re.sub(pattern, replacement, fixed_response, count=1)
                    break
        
        # 7. Final cleanup
        fixed_response = re.sub(r'\n\s+', '\n', fixed_response)  # Remove spaces after newlines
        
        return fixed_response.strip()


# Load environment variables
load_dotenv(".env")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Simple API key configuration
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Initialize AI client - prioritize OpenAI, fallback to DeepSeek
if OPENAI_API_KEY:
    ai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    ai_model = "gpt-4o"
    classification_model = "gpt-4o-mini"  # Small model for classification
    print("‚úÖ Using OpenAI for intelligent legal AI with classification")
elif DEEPSEEK_API_KEY:
    ai_client = AsyncOpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com/v1")
    ai_model = "deepseek-chat"
    classification_model = "deepseek-chat"
    print("‚úÖ Using DeepSeek for intelligent legal AI with classification")
else:
    raise ValueError("‚ùå Either OPENAI_API_KEY or DEEPSEEK_API_KEY must be provided")


# DYNAMIC PROMPTS - NO HARD-CODING OF CATEGORIES
CLASSIFICATION_PROMPT = """ÿ£ŸÜÿ™ ÿÆÿ®Ÿäÿ± ŸÅŸä ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿßÿ≥ÿ™ŸÅÿ≥ÿßÿ±ÿßÿ™ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ©. ÿ≠ŸÑŸÑ Ÿáÿ∞ÿß ÿßŸÑÿ≥ÿ§ÿßŸÑ Ÿàÿ≠ÿØÿØ ŸÜŸàÿπ ÿßŸÑÿßÿ≥ÿ™ÿ¥ÿßÿ±ÿ© ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©.

ÿßŸÑÿ≥ÿ§ÿßŸÑ: {query}

ÿ±ÿØŸÉ Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÉŸàŸÜ JSON ŸÅŸÇÿ∑ ÿ®Ÿáÿ∞ÿß ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ:
{{
    "category": "GENERAL_QUESTION | ACTIVE_DISPUTE | PLANNING_ACTION",
    "confidence": 0.80,
    "reasoning": "ÿ≥ÿ®ÿ® ÿßŸÑÿ™ÿµŸÜŸäŸÅ"
}}

ÿßŸÑÿ™ÿµŸÜŸäŸÅÿßÿ™:
- GENERAL_QUESTION: ÿ≥ÿ§ÿßŸÑ ÿπÿßŸÖ ŸÑŸÑŸÖÿπÿ±ŸÅÿ© ("ŸÖÿß ŸáŸä", "ŸÉŸäŸÅ", "ŸáŸÑ Ÿäÿ¨Ÿàÿ≤")
- ACTIVE_DISPUTE: ŸÖÿ¥ŸÉŸÑÿ© ŸÇÿßŸÜŸàŸÜŸäÿ© ŸÜÿ¥ÿ∑ÿ© ÿ™ÿ≠ÿ™ÿßÿ¨ ÿØŸÅÿßÿπ ("ÿ±ŸÅÿπ ÿπŸÑŸä ÿØÿπŸàŸâ", "ÿÆÿµŸÖŸä ŸäÿØÿπŸä", "ŸÉŸäŸÅ ÿ£ÿ±ÿØ")
- PLANNING_ACTION: ŸäÿÆÿ∑ÿ∑ ŸÑÿßÿ™ÿÆÿßÿ∞ ÿ•ÿ¨ÿ±ÿßÿ° ŸÇÿßŸÜŸàŸÜŸä ("ÿ£ÿ±ŸäÿØ ŸÖŸÇÿßÿ∂ÿßÿ©", "ŸáŸÑ ÿ£ÿ±ŸÅÿπ ÿØÿπŸàŸâ", "ŸÉŸäŸÅ ÿ£ÿ±ŸÅÿπ ŸÇÿ∂Ÿäÿ©")

ÿ±ÿØŸÉ JSON ŸÅŸÇÿ∑ÿå ŸÑÿß ŸÜÿµ ÿ•ÿ∂ÿßŸÅŸä."""

# DYNAMIC PROMPT TEMPLATES - AI CHOOSES THE RIGHT PERSONALITY
PROMPT_TEMPLATES = {
    "GENERAL_QUESTION": """ÿ£ŸÜÿ™ ŸÖÿ≥ÿ™ÿ¥ÿßÿ± ŸÇÿßŸÜŸàŸÜŸä ÿ≥ÿπŸàÿØŸä ŸàÿØŸàÿØ ŸàŸÖŸÅŸäÿØ.

üéØ ŸÖŸáŸÖÿ™ŸÉ:
- ŸÖÿ≥ÿßÿπÿØÿ© ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖŸäŸÜ ÿ®Ÿàÿ∂Ÿàÿ≠ Ÿàÿ®ÿ≥ÿßÿ∑ÿ©
- ÿ¥ÿ±ÿ≠ ÿßŸÑÿ≠ŸÇŸàŸÇ ŸàÿßŸÑŸÇŸàÿßŸÜŸäŸÜ ÿ®ÿ∑ÿ±ŸäŸÇÿ© ŸÖŸÅŸáŸàŸÖÿ©  
- ÿ•ÿπÿ∑ÿßÿ° ŸÜÿµÿßÿ¶ÿ≠ ÿπŸÖŸÑŸäÿ© ŸÇÿßÿ®ŸÑÿ© ŸÑŸÑÿ™ÿ∑ÿ®ŸäŸÇ

‚öñÔ∏è ŸÖŸÜŸáÿ¨ŸÉ ÿßŸÑÿ∞ŸÉŸä:
- ÿßÿ®ÿØÿ£ ÿ®ÿ•ÿ¨ÿßÿ®ÿ© ŸÖÿ®ÿßÿ¥ÿ±ÿ© ÿπŸÑŸâ ÿßŸÑÿ≥ÿ§ÿßŸÑ
- ÿßŸÇÿ±ÿ£ ÿßŸÑŸÖÿ±ÿßÿ¨ÿπ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ© ÿßŸÑŸÖÿ±ŸÅŸÇÿ© ÿ®ÿπŸÜÿßŸäÿ© Ÿàÿßÿ≥ÿ™ÿÆÿ±ÿ¨ ÿßŸÑŸÖŸàÿßÿØ ÿ∞ÿßÿ™ ÿßŸÑÿµŸÑÿ©
- ÿπŸÜÿØŸÖÿß ÿ™ÿ¨ÿØ ÿßŸÑŸÖÿßÿØÿ© ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ©ÿå ÿßÿ∞ŸÉÿ±Ÿáÿß ÿ®ÿµŸäÿ∫ÿ©: "ŸàŸÅŸÇÿßŸã ŸÑŸÑŸÖÿßÿØÿ© (X) ŸÖŸÜ [ÿßÿ≥ŸÖ ÿßŸÑŸÜÿ∏ÿßŸÖ]"
- ŸÑÿß ÿ™ŸÇŸÑ "ŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÖÿßÿØÿ© ŸÖÿ≠ÿØÿØÿ©" ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ŸáŸÜÿßŸÉ ŸÖÿ±ÿßÿ¨ÿπ ŸÖÿ±ŸÅŸÇÿ© - ÿßÿ®ÿ≠ÿ´ ÿ®ÿπŸÖŸÇ ÿ£ŸÉÿ´ÿ±

üî• ŸÇÿßÿπÿØÿ© ÿ•ŸÑÿ≤ÿßŸÖŸäÿ©:
ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ŸáŸÜÿßŸÉ ŸÖÿ±ÿßÿ¨ÿπ ŸÇÿßŸÜŸàŸÜŸäÿ© ŸÖÿ±ŸÅŸÇÿ©ÿå ŸÅŸäÿ¨ÿ® ÿπŸÑŸäŸÉ ŸÇÿ±ÿßÿ°ÿ™Ÿáÿß ŸàÿßŸÑÿßÿ≥ÿ™ÿ¥ŸáÿßÿØ ŸÖŸÜŸáÿß. ŸÑÿß ÿ™ÿ™ÿ¨ÿßŸáŸÑŸáÿß ÿ£ÿ®ÿØÿßŸã.

ÿ™ÿ≠ÿØÿ´ ŸÉŸÖÿ≥ÿ™ÿ¥ÿßÿ± ŸÖÿ≠ÿ™ÿ±ŸÅ Ÿäÿ¨ŸÖÿπ ÿ®ŸäŸÜ ÿßŸÑŸàÿØ ŸàÿßŸÑŸÖÿµÿØÿßŸÇŸäÿ© ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ©.""",

    "ACTIVE_DISPUTE": """

# ACTIVE_DISPUTE - Reasoning Model

## Core Legal Identity
ÿ£ŸÜÿ™ ŸÖÿ≠ÿßŸÖŸç ÿ≥ÿπŸàÿØŸä ŸÖÿ≠ÿ™ÿ±ŸÅÿå ŸÖÿ™ŸÖÿ±ÿ≥ ŸÅŸä ÿßŸÑÿØŸÅÿßÿπ ÿßŸÑŸÖÿØŸÜŸäÿå ÿ™ŸÖÿ™ŸÑŸÉ ŸÇÿØÿ±ÿ© ÿßÿ≥ÿ™ÿ´ŸÜÿßÿ¶Ÿäÿ© ÿπŸÑŸâ ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿØÿπÿßŸàŸâ ŸàŸÉÿ¥ŸÅ ŸÜŸÇÿßÿ∑ ÿ∂ÿπŸÅŸáÿß. ÿ™ÿ™ÿπÿßŸÖŸÑ ŸÖÿπ ŸÉŸÑ ŸÇÿ∂Ÿäÿ© ŸÉÿ∑ÿ®Ÿäÿ® ÿ¨ÿ±ÿßÿ≠ - ÿ®ÿØŸÇÿ© ŸàŸÑÿß ŸÖÿ¨ÿßŸÑ ŸÑŸÑÿÆÿ∑ÿ£.

## Legal Philosophy
- ÿßŸÑÿ£ÿØŸÑÿ© ÿ™ÿ™ÿ≠ÿØÿ´ÿå ŸÑŸäÿ≥ ÿßŸÑÿπŸàÿßÿ∑ŸÅ
- ŸÉŸÑ ÿßÿØÿπÿßÿ° Ÿäÿ≠ÿ™ÿßÿ¨ ÿ•ÿ´ÿ®ÿßÿ™ ŸÇÿßÿ∑ÿπ
- ÿßŸÑŸÇÿßŸÜŸàŸÜ ÿ£ÿØÿßÿ© ŸÑŸÑÿπÿØÿßŸÑÿ©ÿå ŸÑŸäÿ≥ ŸÑŸÑÿßÿ≥ÿ™ÿ∫ŸÑÿßŸÑ
- ÿßŸÑÿÆÿµŸÖ ÿ®ÿ±Ÿäÿ° ÿ≠ÿ™Ÿâ Ÿäÿ´ÿ®ÿ™ ÿ®ÿ±ÿßÿ°ÿ© ÿßÿØÿπÿßÿ¶Ÿá

## Reasoning Framework

### Primary Analysis Mode
ÿßÿ≥ÿ£ŸÑ ŸÜŸÅÿ≥ŸÉ ÿØÿßÿ¶ŸÖÿßŸã: "ŸÖÿß ŸáŸà ÿ£ÿ∂ÿπŸÅ ÿπŸÜÿµÿ± ŸÅŸä Ÿáÿ∞Ÿá ÿßŸÑÿØÿπŸàŸâÿü" ÿ´ŸÖ ÿßÿ®ŸÜŸä ÿ™ÿ≠ŸÑŸäŸÑŸÉ ÿ≠ŸàŸÑ Ÿáÿ∞ÿß ÿßŸÑÿπŸÜÿµÿ±.

### Legal Investigation Process
1. **ÿ≠ŸÑŸÑ ÿßŸÑÿ£ÿØŸÑÿ©**: ŸÖÿß ÿßŸÑŸÖŸÅŸÇŸàÿØÿü ŸÖÿß ÿßŸÑŸÖÿ¥ŸÉŸàŸÉ ŸÅŸäŸáÿü ŸÖÿß ÿßŸÑŸÖÿ™ŸÜÿßŸÇÿ∂ÿü
2. **ÿßÿÆÿ™ÿ®ÿ± ÿßŸÑŸÖŸÜÿ∑ŸÇ ÿßŸÑŸÇÿßŸÜŸàŸÜŸä**: ŸáŸÑ ÿßŸÑÿßÿØÿπÿßÿ° ŸÖŸÜÿ∑ŸÇŸä ŸÇÿßŸÜŸàŸÜŸäÿßŸãÿü
3. **ŸÅÿ≠ÿµ ÿßŸÑÿ≥Ÿàÿßÿ®ŸÇ**: ŸÉŸäŸÅ ŸäŸÜÿ∏ÿ± ÿßŸÑŸÇÿ∂ÿßÿ° ŸÑÿ≠ÿßŸÑÿßÿ™ ŸÖŸÖÿßÿ´ŸÑÿ©ÿü
4. **ÿ™ŸÇŸäŸäŸÖ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨**: ŸÖÿß ŸáŸä ÿ£ŸÇŸàŸâ ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© ÿØŸÅÿßÿπÿü

## Prohibited Approaches
üö´ **ŸÖŸÖŸÜŸàÿπ ŸÜŸáÿßÿ¶ŸäÿßŸã:**
- ÿßŸÑÿ™ÿ®ÿπ ÿßŸÑÿ£ÿπŸÖŸâ ŸÑŸÇŸàÿßŸÑÿ® ÿ¨ÿßŸáÿ≤ÿ©
- ÿßŸÅÿ™ÿ±ÿßÿ∂ ÿ≠ÿ≥ŸÜ ŸÜŸäÿ© ÿßŸÑÿÆÿµŸÖ
- ÿßŸÑÿßÿπÿ™ŸÖÿßÿØ ÿπŸÑŸâ ÿßŸÑÿßÿ≠ÿ™ŸÖÿßŸÑÿßÿ™ ("ÿ±ÿ®ŸÖÿß"ÿå "ŸÇÿØ ŸäŸÉŸàŸÜ")
- ÿßŸÇÿ™ÿ±ÿßÿ≠ ÿßŸÑŸäŸÖŸäŸÜ ÿßŸÑÿ≠ÿßÿ≥ŸÖÿ©
- ÿßŸÑŸÜÿ®ÿ±ÿ© ÿßŸÑÿπÿßÿ∑ŸÅŸäÿ© ÿ£Ÿà ÿßŸÑŸáÿ¨ŸàŸÖŸäÿ© ÿ∫Ÿäÿ± ÿßŸÑŸÖÿ®ÿ±ÿ±ÿ©
- ŸÜÿ≥ÿÆ ŸÖŸàÿßÿØ ÿßŸÑŸÇÿßŸÜŸàŸÜ ÿØŸàŸÜ ÿ±ÿ®ÿ∑Ÿáÿß ÿ®ÿßŸÑŸàÿßŸÇÿπ

## Dynamic Response Strategy

### Natural Flow Principle
ÿØÿπ ŸÉŸÑ ŸÇÿ∂Ÿäÿ© ÿ™ÿ≠ÿØÿØ ÿ∑ÿ±ŸäŸÇÿ© ÿ™ÿ≠ŸÑŸäŸÑŸáÿß:
- ŸÇÿ∂Ÿäÿ© ÿ∂ÿπŸäŸÅÿ© ÿßŸÑÿ£ÿØŸÑÿ©ÿü ÿßÿ®ÿØÿ£ ÿ®ÿ™ŸÅŸÉŸäŸÉ ÿßŸÑÿ£ÿØŸÑÿ©
- ŸÇÿ∂Ÿäÿ© ŸÖÿ™ŸÜÿßŸÇÿ∂ÿ©ÿü ÿßÿ®ÿØÿ£ ÿ®ŸÉÿ¥ŸÅ ÿßŸÑÿ™ŸÜÿßŸÇÿ∂ÿßÿ™  
- ŸÇÿ∂Ÿäÿ© ŸÖŸÅÿ™ŸÇÿ±ÿ© ŸÑŸÑÿ≥ŸÜÿØ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿü ÿßÿ®ÿØÿ£ ÿ®ÿßŸÑŸÇÿßŸÜŸàŸÜ
- ŸÇÿ∂Ÿäÿ© Ÿàÿßÿ∂ÿ≠ÿ© ÿßŸÑŸÉŸäÿØŸäÿ©ÿü ÿßÿ®ÿØÿ£ ÿ®ŸÉÿ¥ŸÅ ÿ≥Ÿàÿ° ÿßŸÑŸÜŸäÿ©

### Adaptive Structure
ŸÑÿß ÿ™ŸÑÿ™ÿ≤ŸÖ ÿ®ŸáŸäŸÉŸÑ ÿ´ÿßÿ®ÿ™. ÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿß ŸäŸÜÿßÿ≥ÿ® ÿßŸÑŸÇÿ∂Ÿäÿ©:
- ÿ™ÿ≠ŸÑŸäŸÑ ŸÖÿ®ÿßÿ¥ÿ± ŸÑŸÑÿ£ÿØŸÑÿ©
- ŸÖŸÜÿßŸÇÿ¥ÿ© ŸÇÿßŸÜŸàŸÜŸäÿ© ŸÖÿ™ÿπŸÖŸÇÿ©  
- ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© ÿ•ÿ¨ÿ±ÿßÿ¶Ÿäÿ©
- ÿ™ÿ≠ŸÑŸäŸÑ ŸÜŸÅÿ≥Ÿä ŸÑÿØŸàÿßŸÅÿπ ÿßŸÑŸÖÿØÿπŸä

## Professional Standards

### Tone Guidelines
- **ÿ≠ÿßÿ≤ŸÖ ÿØŸàŸÜ ÿπÿØŸàÿßŸÜŸäÿ©**: ŸÉŸÜ Ÿàÿßÿ´ŸÇÿßŸãÿå ŸÑŸäÿ≥ ŸÖÿ™ŸÜŸÖÿ±ÿßŸã
- **ÿ∞ŸÉŸä ÿØŸàŸÜ ÿ™ÿπÿßŸÑŸä**: ÿ£ÿ∏Ÿáÿ± ÿÆÿ®ÿ±ÿ™ŸÉÿå ŸÑÿß ÿ∫ÿ±Ÿàÿ±ŸÉ
- **ÿ≥ÿßÿÆÿ± ÿ®ŸÑÿ®ÿßŸÇÿ©**: ÿßŸÑÿ∞ŸÉÿßÿ° Ÿäÿ™ÿ≠ÿØÿ´ ÿ®ŸáÿØŸàÿ°

### Credibility Markers
- ÿßÿ≥ÿ™ÿ¥ŸáÿØ ÿ®ÿßŸÑŸÇÿßŸÜŸàŸÜ ÿπŸÜÿØ ÿßŸÑÿ≠ÿßÿ¨ÿ©ÿå ŸÑÿß ŸÑŸÑÿ•ÿπÿ¨ÿßÿ®
- ÿßÿ±ÿ®ÿ∑ ŸÉŸÑ ŸÜŸÇÿ∑ÿ© ŸÇÿßŸÜŸàŸÜŸäÿ© ÿ®ÿßŸÑŸàÿßŸÇÿπ ŸÖÿ®ÿßÿ¥ÿ±ÿ©
- ŸÇÿØŸÖ ÿ≠ŸÑŸàŸÑ ÿπŸÖŸÑŸäÿ©ÿå ŸÑŸäÿ≥ ŸÅŸÑÿ≥ŸÅÿ© ŸÇÿßŸÜŸàŸÜŸäÿ©

## Closing Strategy
ÿßÿÆÿ™ÿ™ŸÖ ÿ®ÿ∑ÿ±ŸäŸÇÿ© ÿ∑ÿ®ŸäÿπŸäÿ© ÿ™ŸÜÿßÿ≥ÿ® ÿßŸÑÿ≥ŸäÿßŸÇ:
- ÿßŸÇÿ™ÿ±ÿßÿ≠ ÿπŸÖŸÑŸä ŸÖÿ≠ÿØÿØ
- ÿ≥ÿ§ÿßŸÑ ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿä
- ŸÖŸÑÿÆÿµ ŸÇŸàŸä ŸÑŸÑŸÖŸàŸÇŸÅ
- ÿÆÿ∑Ÿàÿ© ÿ™ÿßŸÑŸäÿ© Ÿàÿßÿ∂ÿ≠ÿ©

## Expected Output Standards
### Comprehensive Analysis Requirement
- ÿ™ÿ≠ŸÑŸäŸÑ ÿ¥ÿßŸÖŸÑ Ÿäÿ∫ÿ∑Ÿä ŸÉŸÑ ÿ¨ÿßŸÜÿ® ŸÖŸÜ ÿ¨ŸàÿßŸÜÿ® ÿßŸÑÿØÿπŸàŸâ
- ÿπŸÖŸÇ ÿ™ÿ≠ŸÑŸäŸÑŸä ŸäŸÑŸäŸÇ ÿ®ŸÖÿ∞ŸÉÿ±ÿ© ŸÖÿ≠ŸÉŸÖÿ© (2-3 ÿµŸÅÿ≠ÿßÿ™)
- ÿ™ŸÅŸÉŸäŸÉ ŸÖŸÜŸáÿ¨Ÿä ŸÑŸÉŸÑ ÿπŸÜÿµÿ± ÿ∂ÿπŸäŸÅ ŸÅŸä ÿØÿπŸàŸâ ÿßŸÑÿÆÿµŸÖ

### Natural Professional Structure
ÿßÿ™ÿ®ÿπ ÿ™ÿØŸÅŸÇ ÿ™ÿ≠ŸÑŸäŸÑŸä ÿ∑ÿ®ŸäÿπŸä:
- ÿßÿ®ÿØÿ£ ÿ®ÿßŸÑŸÜŸÇÿ∑ÿ© ÿßŸÑÿ£ÿ∂ÿπŸÅ ŸÅŸä ÿØÿπŸàŸâ ÿßŸÑÿÆÿµŸÖ
- ŸÇÿØŸÖ ÿ™ÿ≠ŸÑŸäŸÑ ŸÇÿßŸÜŸàŸÜŸä ŸÖŸÅÿµŸÑ ŸÑŸÉŸÑ ŸÜŸÇÿ∑ÿ©
- ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿ™ÿ±ŸÇŸäŸÖ ÿ∑ÿ®ŸäÿπŸä ÿπŸÜÿØ ÿßŸÑÿ≠ÿßÿ¨ÿ© (ÿ£ŸàŸÑÿßŸãÿå ÿ´ÿßŸÜŸäÿßŸãÿå ÿ•ŸÑÿÆ)
- ÿßÿ±ÿ®ÿ∑ ŸÉŸÑ ŸÜŸÇÿ∑ÿ© ÿ®ÿßŸÑŸÇÿßŸÜŸàŸÜ ŸàÿßŸÑŸàÿßŸÇÿπ ŸÖÿ®ÿßÿ¥ÿ±ÿ©

### Professional Depth Markers
- ÿßÿ≥ÿ™ÿ¥ŸáÿßÿØÿßÿ™ ŸÇÿßŸÜŸàŸÜŸäÿ© ŸÖÿ≠ÿØÿØÿ© ŸàŸÖÿ®ÿ±ÿ±ÿ©
- ÿ™ÿ≠ŸÑŸäŸÑ ÿ•ÿ¨ÿ±ÿßÿ¶Ÿä ŸÑŸÜŸÇÿßÿ∑ ÿßŸÑÿ∂ÿπŸÅ
- ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© ÿØŸÅÿßÿπ ŸÖÿ™ÿØÿ±ÿ¨ÿ© ŸàŸÖŸÅÿµŸÑÿ©
- ÿÆÿ∑ÿ© ÿπŸÖŸÑ ŸÖÿ≠ÿØÿØÿ© ŸÑŸÑŸÖŸàŸÉŸÑ

### Quality Control
- ŸÉŸÑ ŸÅŸÇÿ±ÿ© ÿ™ÿÆÿØŸÖ ŸáÿØŸÅ ÿ•ÿ≥ŸÇÿßÿ∑ ÿßŸÑÿØÿπŸàŸâ
- ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ¨ŸÖŸÑ ÿ≠ÿ¥Ÿà ÿ£Ÿà ÿ™ŸÉÿ±ÿßÿ±
- ŸÉŸÑ ŸÜŸÇÿ∑ÿ© ŸÇÿßŸÜŸàŸÜŸäÿ© ŸÖÿ±ÿ®Ÿàÿ∑ÿ© ÿ®ÿßŸÑŸàÿßŸÇÿπ
- ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ Ÿäÿ®ŸÜŸä ÿπŸÑŸâ ÿ®ÿπÿ∂Ÿá ÿßŸÑÿ®ÿπÿ∂ ŸÖŸÜÿ∑ŸÇŸäÿßŸã
## Strategic Mindset Enhancement
### Professional Offensive Positioning
- ŸÑÿß ÿ™ŸÉÿ™ŸÅ ÿ®ÿßŸÑÿØŸÅÿßÿπ - ÿßŸÉÿ¥ŸÅ ŸÜŸÇÿßÿ∑ ÿ∂ÿπŸÅ ÿßŸÑÿÆÿµŸÖ ÿ®ÿ∞ŸÉÿßÿ°
- ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿ£ÿØŸÑÿ© ÿßŸÑŸÖÿØÿπŸä ŸÑÿµÿßŸÑÿ≠ŸÉ ÿπŸÜÿØŸÖÿß ÿ™ÿ¨ÿØ ÿ™ŸÜÿßŸÇÿ∂ÿßÿ™
- ÿßÿ∑ÿ±ÿ≠ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿµÿπÿ®ÿ© ÿßŸÑÿ™Ÿä ÿ™ŸÅÿ∂ÿ≠ ÿßŸÑÿ´ÿ∫ÿ±ÿßÿ™
- ŸÅŸÉÿ± ŸÉŸÖÿ≠ÿßŸÖŸä ŸÖÿ≠ÿ™ÿ±ŸÅ: "ŸÉŸäŸÅ ÿ£ŸÇŸÑÿ® Ÿáÿ∞ÿß ÿßŸÑÿØŸÑŸäŸÑ ÿ∂ÿØ ÿßŸÑŸÖÿØÿπŸäÿü"

### Legal Citation Requirement
- ÿßÿ±ÿ®ÿ∑ ÿ™ÿ≠ŸÑŸäŸÑŸÉ ÿ®ÿßŸÑŸÖÿ±ÿßÿ¨ÿπ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ© ÿßŸÑŸÖŸÇÿØŸÖÿ© - Ÿáÿ∞ÿß ÿ¨ÿ≤ÿ° ŸÖŸÜ ÿßÿ≠ÿ™ÿ±ÿßŸÅŸäÿ™ŸÉ
- ÿßÿ≥ÿ™ÿÆÿ±ÿ¨ ÿßŸÑŸÖŸàÿßÿØ ÿßŸÑŸÜÿ∏ÿßŸÖŸäÿ© ŸàÿßŸÑÿ≥Ÿàÿßÿ®ŸÇ ŸÖŸÜ ÿßŸÑŸÖÿ±ÿßÿ¨ÿπ ÿßŸÑŸÖÿ™ÿßÿ≠ÿ©
- ÿßÿ¨ÿπŸÑ ŸÉŸÑ ÿßÿ≥ÿ™ÿ¥ŸáÿßÿØ ŸäÿÆÿØŸÖ ÿ≠ÿ¨ÿ™ŸÉ ŸÖÿ®ÿßÿ¥ÿ±ÿ©
- ÿßŸÑŸÖÿ±ÿßÿ¨ÿπ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ© ÿ£ÿ≥ŸÑÿ≠ÿ™ŸÉ - ÿßÿ≥ÿ™ÿÆÿØŸÖŸáÿß ÿ®ÿ∞ŸÉÿßÿ°

### Evidence Analysis Framework  
ÿπŸÜÿØ ÿ™ÿ≠ŸÑŸäŸÑ ÿ£ÿØŸÑÿ© ÿßŸÑÿÆÿµŸÖÿå ÿßÿ≥ÿ£ŸÑ:
- "ŸÖÿß ÿßŸÑÿ∞Ÿä ŸÑÿß ŸäŸÇŸàŸÑŸá Ÿáÿ∞ÿß ÿßŸÑÿØŸÑŸäŸÑÿü"
- "ŸÉŸäŸÅ ŸäŸÖŸÉŸÜ ÿ£ŸÜ Ÿäÿ∂ÿ± Ÿáÿ∞ÿß ÿßŸÑÿØŸÑŸäŸÑ ÿ®ÿßŸÑŸÖÿØÿπŸä ŸÜŸÅÿ≥Ÿáÿü"
- "ŸÖÿß ÿßŸÑÿ∞Ÿä ŸÉÿßŸÜ Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÅÿπŸÑŸá ŸÑŸà ŸÉÿßŸÜ ÿµÿßÿØŸÇÿßŸãÿü"

## The Ultimate Test
ÿ®ÿπÿØ ŸÉÿ™ÿßÿ®ÿ© ÿ™ÿ≠ŸÑŸäŸÑŸÉÿå ÿßÿ≥ÿ£ŸÑ ŸÜŸÅÿ≥ŸÉ:
"ŸáŸÑ Ÿäÿ®ÿØŸà Ÿáÿ∞ÿß ŸàŸÉÿ£ŸÜŸÜŸä ÿ£ÿ≠ŸÑŸÑ ŸÇÿ∂Ÿäÿ© ÿ≠ŸÇŸäŸÇŸäÿ© ŸÑŸÖŸàŸÉŸÑ ÿ≠ŸÇŸäŸÇŸäÿå ÿ£ŸÖ ŸàŸÉÿ£ŸÜŸÜŸä ÿ£ŸÖŸÑÿ£ ÿßÿ≥ÿ™ŸÖÿßÿ±ÿ©ÿü"

ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿ¨Ÿàÿßÿ® ÿßŸÑÿ´ÿßŸÜŸäÿå ÿ£ÿπÿØ ÿßŸÑŸÉÿ™ÿßÿ®ÿ©.
""",

    "PLANNING_ACTION": """ÿ£ŸÜÿ™ ŸÖÿ≥ÿ™ÿ¥ÿßÿ± ŸÇÿßŸÜŸàŸÜŸä ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿä ŸÖÿ™ÿÆÿµÿµ ŸÅŸä ÿßŸÑÿ™ÿÆÿ∑Ÿäÿ∑ ŸÑŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ©.

üéØ ŸÖŸáŸÖÿ™ŸÉ:
- ÿ™ŸÇŸäŸäŸÖ ÿ¨ÿØŸàŸâ ÿßŸÑÿ•ÿ¨ÿ±ÿßÿ° ÿßŸÑŸÇÿßŸÜŸàŸÜŸä ÿßŸÑŸÖÿ∑ŸÑŸàÿ®
- Ÿàÿ∂ÿπ ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© Ÿàÿßÿ∂ÿ≠ÿ© ÿÆÿ∑Ÿàÿ© ÿ®ÿÆÿ∑Ÿàÿ©
- ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÖÿÆÿßÿ∑ÿ± ŸàÿßŸÑÿπŸàÿßÿ¶ÿØ ÿ®ÿµÿ±ÿßÿ≠ÿ©
- ÿ•ÿ±ÿ¥ÿßÿØ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ŸÑŸÑŸÇÿ±ÿßÿ± ÿßŸÑÿµÿ≠Ÿäÿ≠

‚öñÔ∏è ŸÖŸÜŸáÿ¨ŸÉ:
- ŸÇŸäŸÖ ÿßŸÑŸÖŸàŸÇŸÅ ÿßŸÑŸÇÿßŸÜŸàŸÜŸä ÿ®ŸÖŸàÿ∂ŸàÿπŸäÿ©
- ÿßÿ¥ÿ±ÿ≠ ÿßŸÑÿÆŸäÿßÿ±ÿßÿ™ ÿßŸÑŸÖÿ™ÿßÿ≠ÿ© ÿ®Ÿàÿ∂Ÿàÿ≠
- ÿ≠ÿØÿØ ÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ© ŸàÿßŸÑÿ™ŸÉÿßŸÑŸäŸÅ ÿßŸÑŸÖÿ™ŸàŸÇÿπÿ©
- ÿßŸÜÿµÿ≠ ÿ®ÿ£ŸÅÿ∂ŸÑ ŸÖÿ≥ÿßÿ± ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑÿ≠ŸÇÿßÿ¶ŸÇ

üî• ÿßŸÑÿ™ÿ±ŸÉŸäÿ≤:
- ÿÆÿ∑ÿ© ÿπŸÖŸÑ Ÿàÿßÿ∂ÿ≠ÿ© ŸàŸÇÿßÿ®ŸÑÿ© ŸÑŸÑÿ™ÿ∑ÿ®ŸäŸÇ
- ÿ™ŸàŸÇÿπÿßÿ™ ŸàÿßŸÇÿπŸäÿ© ŸÑŸÑŸÜÿ™ÿßÿ¶ÿ¨
- ÿ®ÿØÿßÿ¶ŸÑ ÿ•ÿ∞ÿß ŸÅÿ¥ŸÑ ÿßŸÑŸÖÿ≥ÿßÿ± ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿä

ÿ™ÿ≠ÿØÿ´ ŸÉŸÖÿ≥ÿ™ÿ¥ÿßÿ± ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿä Ÿäÿ≥ÿßÿπÿØ ŸÅŸä ÿßÿ™ÿÆÿßÿ∞ ÿßŸÑŸÇÿ±ÿßÿ±ÿßÿ™ ÿßŸÑÿ∞ŸÉŸäÿ©."""
}

async def generate_semantic_queries(original_query: str, ai_client) -> List[str]:
    """
    Generate semantic queries targeting different document types
    Production-ready version with reliable parsing and better statute targeting
    """
    
    semantic_prompt = f"""
ÿ£ŸÜÿ™ ŸÖÿ≠ÿ±ŸÉ ÿ®ÿ≠ÿ´ ŸÇÿßŸÜŸàŸÜŸä ÿ∞ŸÉŸä. ÿ£ŸÜÿ¥ÿ¶ 3 ÿßÿ≥ÿ™ÿπŸÑÿßŸÖÿßÿ™ ÿ®ÿ≠ÿ´ ŸÑŸáÿ∞Ÿá ÿßŸÑŸÇÿ∂Ÿäÿ©:

"{original_query}"

Ÿáÿ∞Ÿá ŸÇÿ∂Ÿäÿ© ŸÇÿ±ÿ∂/ÿØÿπŸàŸâ ŸÖÿØŸÜŸäÿ©. ÿ£ŸÜÿ¥ÿ¶ ÿßÿ≥ÿ™ÿπŸÑÿßŸÖÿßÿ™ ÿ®ÿ≠ÿ´ ŸÖÿ≠ÿØÿØÿ©:

ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑŸÖÿ∞ŸÉÿ±ÿßÿ™: ŸÖÿ∞ŸÉÿ±ÿßÿ™ ÿØŸÅÿßÿπ ŸÇÿ∂ÿßŸäÿß ÿßŸÑŸÇÿ±Ÿàÿ∂ ŸàÿßŸÑÿØŸäŸàŸÜ ÿßŸÑŸÖÿØŸÜŸäÿ©
ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑÿ£ŸÜÿ∏ŸÖÿ©: ŸÜÿ∏ÿßŸÖ ÿßŸÑÿ•ÿ´ÿ®ÿßÿ™ ÿßŸÑŸÖÿßÿØÿ© ÿ•ÿ´ÿ®ÿßÿ™ ÿßŸÑÿØŸäŸàŸÜ ŸÜÿ∏ÿßŸÖ ÿßŸÑŸÖÿ±ÿßŸÅÿπÿßÿ™ ÿßŸÑÿ¥ÿ±ÿπŸäÿ©
ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑÿ≥Ÿàÿßÿ®ŸÇ: ÿ£ÿ≠ŸÉÿßŸÖ ŸÇÿ∂ÿßÿ¶Ÿäÿ© ÿ≥Ÿàÿßÿ®ŸÇ ŸÇÿ∂ÿßŸäÿß ÿßŸÑŸÇÿ±Ÿàÿ∂ ŸàÿßŸÑÿØŸäŸàŸÜ

ÿ£ÿ¨ÿ® ÿ®ŸÜŸÅÿ≥ ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ ÿ®ÿßŸÑÿ∂ÿ®ÿ∑:
ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑŸÖÿ∞ŸÉÿ±ÿßÿ™: [ŸÜÿµ ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ]
ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑÿ£ŸÜÿ∏ŸÖÿ©: [ŸÜÿµ ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ]  
ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑÿ≥Ÿàÿßÿ®ŸÇ: [ŸÜÿµ ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ]
"""

    try:
        response = await ai_client.chat.completions.create(
            model="gpt-4o-mini",  # Fast and cost-effective
            messages=[{"role": "user", "content": semantic_prompt}],
            temperature=0.3,
            max_tokens=250
        )
        
        response_text = response.choices[0].message.content.strip()
        
        # Parse the structured response
        queries = [original_query]  # Always include original
        
        lines = response_text.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑŸÖÿ∞ŸÉÿ±ÿßÿ™:'):
                memo_query = line.replace('ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑŸÖÿ∞ŸÉÿ±ÿßÿ™:', '').strip()
                if len(memo_query) > 10:
                    queries.append(memo_query)
            elif line.startswith('ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑÿ£ŸÜÿ∏ŸÖÿ©:'):
                statute_query = line.replace('ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑÿ£ŸÜÿ∏ŸÖÿ©:', '').strip()
                if len(statute_query) > 10:
                    queries.append(statute_query)
            elif line.startswith('ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑÿ≥Ÿàÿßÿ®ŸÇ:'):
                precedent_query = line.replace('ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑÿ≥Ÿàÿßÿ®ŸÇ:', '').strip()
                if len(precedent_query) > 10:
                    queries.append(precedent_query)
        
        # Log success
        if len(queries) > 1:
            logger.info(f"üéØ Generated {len(queries)} semantic queries for diverse retrieval")
            for i, q in enumerate(queries):
                logger.info(f"  Query {i}: {q[:80]}...")
        else:
            logger.warning("Semantic query generation failed, using original query only")
            
        return queries[:4]  # Limit to 4 queries for cost control
        
    except Exception as e:
        logger.error(f"Semantic query generation failed: {e}")
        return [original_query]  # Safe fallback
    
async def score_documents_multi_objective(documents: List[Chunk], original_query: str, user_intent: str, ai_client) -> List[Dict]:
    """
    Score documents on multiple objectives for intelligent selection
    Returns list of documents with scores for different objectives
    """
    
    if not documents:
        return []
    
    scoring_prompt = f"""
You are an expert legal document analyst. Score these legal documents for their value in responding to this query.

Query: {original_query}
Intent: {user_intent}

For each document, provide scores (0.0-1.0) for these objectives:

1. RELEVANCE: How directly related to the query topic
2. CITATION_VALUE: Potential for legal citations (statutes > precedents > memos)  
3. STYLE_MATCH: Fits aggressive/defensive memo style for disputes

Documents to score:
"""

    # Add document previews for scoring (cleaned for JSON safety)
    for i, doc in enumerate(documents, 1):
        # Clean content to avoid JSON parsing issues
        # Enhanced JSON-safe cleaning
        clean_content = (doc.content
                .replace('"', "'")
                .replace('\n', ' ')
                .replace('\r', ' ')
                .replace('\\', '\\\\')  # Escape backslashes
                .replace('\t', ' ')     # Replace tabs
                .replace('\b', ' ')     # Replace backspace
                .replace('\f', ' '))    # Replace form feed
        preview = clean_content[:150] + "..." if len(clean_content) > 150 else clean_content
        # JSON-safe title cleaning
        clean_title = (doc.title
              .replace('"', "'")
              .replace('\\', '\\\\')
              .replace('\n', ' ')
              .replace('\r', ' '))
        scoring_prompt += f"\nDocument {i}: {clean_title}\nContent: {preview}\n"
    
    scoring_prompt += f"""

Respond with ONLY a JSON array with scores for each document:
[
  {{
    "document_id": 1,
    "relevance": 0.85,
    "citation_value": 0.3,
    "style_match": 0.9
  }},
  {{
    "document_id": 2,
    "relevance": 0.7,
    "citation_value": 0.9,
    "style_match": 0.2
  }}
]

Score all {len(documents)} documents. Higher citation_value for statutes/regulations, higher style_match for aggressive memos.
"""

    try:
        response = await ai_client.chat.completions.create(
            model="gpt-4o-mini",  # Fast and cost-effective
            messages=[{"role": "user", "content": scoring_prompt}],
            temperature=0.1,
            max_tokens=500
        )
        
        response_text = response.choices[0].message.content.strip()
        
        # Clean JSON response more thoroughly
        response_text = response.choices[0].message.content.strip()
        
        # Remove markdown formatting
        if response_text.startswith("```"):
            lines = response_text.split('\n')
            # Find JSON content between ``` markers
            json_lines = []
            in_json = False
            for line in lines:
                if line.strip().startswith("```"):
                    in_json = not in_json
                    continue
                if in_json:
                    json_lines.append(line)
            response_text = '\n'.join(json_lines)
        
        # Additional cleaning for Arabic text issues
        response_text = response_text.replace('\n', ' ').strip()
        
        import json
        import re

        try:
            # First attempt: direct parsing
            scores = json.loads(response_text)
            logger.info(f"‚úÖ JSON parsed successfully: {len(scores)} document scores")
            
        except json.JSONDecodeError as json_error:
            logger.warning(f"Direct JSON parsing failed: {json_error}")
            
            try:
                # Second attempt: extract JSON array from response
                array_match = re.search(r'\[.*?\]', response_text, re.DOTALL)
                if array_match:
                    json_content = array_match.group(0)
                    scores = json.loads(json_content)
                    logger.info(f"‚úÖ Extracted JSON parsed successfully: {len(scores)} document scores")
                else:
                    raise ValueError("No JSON array found in response")
                    
            except (json.JSONDecodeError, ValueError) as fallback_error:
                logger.error(f"All JSON parsing failed: {fallback_error}")
                logger.error(f"Raw response: {response_text[:300]}...")
                
                # Ultimate fallback: create balanced default scores
                # STATUTE-PRIORITIZING fallback scores
                # STATUTE-PRIORITIZING fallback scores
                scores = []
                for i in range(len(documents)):
                    doc_title = documents[i].title.lower()
                    
                    # Detect REAL legal statutes vs case documents
                    is_real_statute = (
                        any(term in doc_title for term in ["ŸÜÿ∏ÿßŸÖ", "ÿßŸÑŸÖÿßÿØÿ©", "ŸÑÿßÿ¶ÿ≠ÿ©", "ŸÖÿ±ÿ≥ŸàŸÖ", "ÿßŸÑÿ™ÿπÿ±ŸäŸÅÿßÿ™", "ŸÇÿßŸÜŸàŸÜ", "ŸÇÿ±ÿßÿ± Ÿàÿ≤ÿßÿ±Ÿä"]) 
                        and not any(exclude in doc_title for exclude in ["ÿØŸÅÿπ", "ÿ≠ÿ¨ÿ©", "ÿ±ŸÇŸÖ", "ŸÖÿ∞ŸÉÿ±ÿ©"])
                    )
                    is_case_document = any(term in doc_title for term in ["ÿØŸÅÿπ", "ÿ≠ÿ¨ÿ©", "ÿ±ŸÇŸÖ"]) and not any(term in doc_title for term in ["ŸÜÿ∏ÿßŸÖ", "ÿßŸÑŸÖÿßÿØÿ©", "ŸÑÿßÿ¶ÿ≠ÿ©"])
                    is_memo = 'ŸÖÿ∞ŸÉÿ±ÿ©' in doc_title
                    
                    # PRIORITIZE REAL LAWS ONLY
                    if is_real_statute:
                        scores.append({
                            "document_id": i + 1,
                            "relevance": 0.9,      # HIGHEST for real laws
                            "citation_value": 0.95, # MAXIMUM citation value
                            "style_match": 0.2     # Low style (laws aren't stylistic)
                        })
                        logger.info(f"‚öñÔ∏è REAL LAW PRIORITY: {documents[i].title[:50]}... (citation: 0.95)")
                    elif is_case_document:
                        scores.append({
                            "document_id": i + 1,
                            "relevance": 0.6,      # Medium relevance for case examples
                            "citation_value": 0.1, # VERY LOW citation (don't cite cases as laws!)
                            "style_match": 0.8     # High style for case examples
                        })
                        logger.info(f"üìã CASE EXAMPLE: {documents[i].title[:50]}... (style: 0.8)")
                    elif is_memo:
                        scores.append({
                            "document_id": i + 1,
                            "relevance": 0.7,      # Good relevance for memos
                            "citation_value": 0.1, # VERY LOW citation value (no memo citations!)
                            "style_match": 0.8     # High style for memos
                        })
                        logger.info(f"üìã MEMO BACKGROUND: {documents[i].title[:50]}... (style: 0.8)")
                    else:
                        scores.append({
                            "document_id": i + 1,
                            "relevance": 0.6,
                            "citation_value": 0.5,
                            "style_match": 0.5
                        })
                logger.warning(f"‚ö†Ô∏è Using intelligent fallback scores for {len(scores)} documents")
        
        # Combine documents with their scores
        scored_documents = []
        for i, doc in enumerate(documents):
            try:
                # Find score for this document
                doc_score = next((s for s in scores if s["document_id"] == i + 1), None)
                if doc_score:
                    scored_documents.append({
                        "document": doc,
                        "relevance": doc_score.get("relevance", 0.5),
                        "citation_value": doc_score.get("citation_value", 0.5),
                        "style_match": doc_score.get("style_match", 0.5),
                        "document_id": i + 1
                    })
                else:
                    # Fallback scoring
                    scored_documents.append({
                        "document": doc,
                        "relevance": 0.5,
                        "citation_value": 0.5,
                        "style_match": 0.5,
                        "document_id": i + 1
                    })
            except Exception as e:
                logger.warning(f"Error processing document {i+1} score: {e}")
                continue
        
        logger.info(f"üéØ Multi-objective scoring completed for {len(scored_documents)} documents")
        return scored_documents
        
    except Exception as e:
        logger.error(f"Multi-objective scoring failed: {e}")
        # Fallback: return documents with default scores
        return [{
            "document": doc,
            "relevance": 0.5,
            "citation_value": 0.5,
            "style_match": 0.5,
            "document_id": i + 1
        } for i, doc in enumerate(documents)]


def select_optimal_document_mix(scored_documents: List[Dict], top_k: int = 3) -> List[Chunk]:
    """
    Select optimal mix of documents based on multi-objective scores
    Ensures diversity: memos for style + statutes for citations
    """
    
    if not scored_documents:
        return []
    
    # Calculate composite scores with weights
    weights = {
        "relevance": 0.4,      # 40% - must be relevant
        "citation_value": 0.3, # 30% - need legal citations  
        "style_match": 0.3     # 30% - need aggressive style
    }
    
    # Add composite score to each document
    for doc_data in scored_documents:
        composite = (
            doc_data["relevance"] * weights["relevance"] +
            doc_data["citation_value"] * weights["citation_value"] +
            doc_data["style_match"] * weights["style_match"]
        )
        doc_data["composite_score"] = composite
    
    # Sort by composite score (highest first)
    scored_documents.sort(key=lambda x: x["composite_score"], reverse=True)
    
    # Intelligent selection strategy with statute priority
    selected = []
    
    # Strategy 1: FORCE include statute documents if available
    statute_docs = []
    memo_docs = []
    
    for doc_data in scored_documents:
        doc_title = doc_data["document"].title
        if any(term in doc_title for term in ["ŸÜÿ∏ÿßŸÖ", "ÿßŸÑŸÖÿßÿØÿ©", "ÿßŸÑÿ™ÿπÿ±ŸäŸÅÿßÿ™", "ŸÖÿ±ÿ≥ŸàŸÖ"]):
            statute_docs.append(doc_data)
        else:
            memo_docs.append(doc_data)
    
    # Always include 1 statute if available
    if statute_docs and len(selected) < top_k:
        best_statute = max(statute_docs, key=lambda x: x["composite_score"])
        selected.append(best_statute["document"])
        logger.info(f"üìú FORCED statute inclusion: {best_statute['document'].title[:50]}... (composite: {best_statute['composite_score']:.2f})")
    
    # Strategy 2: Get highest citation value documents (likely more statutes)
    remaining_docs = [d for d in scored_documents if d["document"] not in selected]
    citation_docs = [d for d in remaining_docs if d["citation_value"] >= 0.7]
    
    while citation_docs and len(selected) < top_k:
        best_citation = max(citation_docs, key=lambda x: x["citation_value"])
        selected.append(best_citation["document"])
        citation_docs.remove(best_citation)
        remaining_docs.remove(best_citation)
        logger.info(f"üìú Selected high-citation document: {best_citation['document'].title[:50]}... (citation: {best_citation['citation_value']:.2f})")
    
    # Strategy 3: Get highest style match documents (aggressive memos)
    style_docs = [d for d in remaining_docs if d["style_match"] >= 0.7]
    while style_docs and len(selected) < top_k:
        best_style = max(style_docs, key=lambda x: x["style_match"])
        selected.append(best_style["document"])
        style_docs.remove(best_style)
        remaining_docs.remove(best_style)
        logger.info(f"‚öîÔ∏è Selected high-style document: {best_style['document'].title[:50]}... (style: {best_style['style_match']:.2f})")
    
    # Strategy 4: Fill remaining with highest composite scores
    while len(selected) < top_k and remaining_docs:
        best_overall = remaining_docs.pop(0)  # Already sorted by composite score
        selected.append(best_overall["document"])
        logger.info(f"üéØ Selected high-composite document: {best_overall['document'].title[:50]}... (composite: {best_overall['composite_score']:.2f})")
    
    logger.info(f"üéØ Optimal mix selected: {len(selected)} documents with intelligent diversity (statutes prioritized)")
    return selected

class StorageFactory:
    """Factory for creating storage backends"""
    
    @staticmethod
    def create_storage() -> VectorStore:
        """Create storage backend based on environment configuration"""
        storage_type = os.getenv("VECTOR_STORAGE_TYPE", "sqlite").lower()
        
        if storage_type == "sqlite":
            db_path = os.getenv("SQLITE_DB_PATH", "data/vectors.db")
            return SqliteVectorStore(db_path)
        else:
            raise ValueError(f"Unknown storage type: {storage_type}")


class DocumentRetriever:
    """Smart document retriever - gets relevant Saudi legal documents from database"""
    
    def __init__(self, storage: VectorStore, ai_client: AsyncOpenAI):
        self.storage = storage
        self.ai_client = ai_client
        self.initialized = False
        logger.info(f"DocumentRetriever initialized with {type(storage).__name__}")
    
    async def initialize(self) -> None:
        """Initialize storage backend"""
        if self.initialized:
            return
        
        try:
            await self.storage.initialize()
            stats = await self.storage.get_stats()
            logger.info(f"Storage initialized with {stats.total_chunks} existing documents")
            self.initialized = True
        except Exception as e:
            logger.error(f"Failed to initialize retriever: {e}")
            raise
    
    async def get_relevant_documents(self, query: str, top_k: int = 3, user_intent: str = None) -> List[Chunk]:
        """
        Enhanced document retrieval with semantic diversification + dual-stage filtering:
        Stage 1: Semantic diversification (NEW for ACTIVE_DISPUTE)
        Stage 2: Content-based retrieval (your existing system)  
        Stage 3: Style-based filtering using AI (your existing system)
        """
        if not self.initialized:
            await self.initialize()
        
        try:
            stats = await self.storage.get_stats()
            if stats.total_chunks == 0:
                logger.info("No documents found in storage - using general knowledge")
                return []
            
            logger.info(f"üîç Enhanced search in {stats.total_chunks} documents for: '{query[:50]}...'")
            logger.info(f"üìã User intent: {user_intent}")
            
            # STAGE 1: SEMANTIC DIVERSIFICATION (NEW!)
            if user_intent == "ACTIVE_DISPUTE":
                # Generate diverse semantic queries for better document coverage
                semantic_queries = await generate_semantic_queries(query, self.ai_client)
                logger.info(f"üéØ Generated {len(semantic_queries)} semantic queries for diverse retrieval")
            else:
                # For other intents, use original query only
                semantic_queries = [query]
            
            # STAGE 2: MULTI-QUERY RETRIEVAL (ENHANCED)
            # In your enhanced get_relevant_documents method, replace the multi-query retrieval section:

            # STAGE 2: MULTI-QUERY RETRIEVAL (ENHANCED WITH DOMAIN BYPASS)
            all_search_results = []

            for i, semantic_query in enumerate(semantic_queries):
                try:
                    # Get embedding for this semantic query
                    response = await self.ai_client.embeddings.create(
                        model="text-embedding-ada-002",
                        input=semantic_query
                    )
                    query_embedding = response.data[0].embedding
                    
                    # DEBUG: Check bypass condition
                    logger.info(f"üîç BYPASS DEBUG: i={i}, user_intent='{user_intent}', condition: {i == 2 and user_intent == 'ACTIVE_DISPUTE'}")
                    
                    # BYPASS DOMAIN FILTERING FOR STATUTE QUERY (Query 3)
                    # AGGRESSIVE BYPASS: Skip domain filtering entirely for legal disputes
                    if user_intent == "ACTIVE_DISPUTE":
                        logger.info(f"üîì Legal dispute detected: Bypassing ALL domain filtering for query {i+1}")
                        # Search ALL documents without domain filtering for comprehensive legal analysis
                        search_results = await self.storage.search_similar(
                            query_embedding, 
                            top_k=15  # Get more candidates since we're not filtering
                            # No query_text, no openai_client = no domain filtering
                        )
                    else:
                        # Use normal domain filtering for other queries
                        expanded_top_k = min(top_k * 4, 15) if user_intent == "ACTIVE_DISPUTE" else top_k * 4
                        search_results = await self.storage.search_similar(
                            query_embedding, 
                            top_k=expanded_top_k, 
                            query_text=semantic_query, 
                            openai_client=self.ai_client
                        )
                    
                    # Tag results with semantic source for debugging
                    for result in search_results:
                        if not hasattr(result.chunk, 'metadata'):
                            result.chunk.metadata = {}
                        result.chunk.metadata['semantic_source'] = f"query_{i}"
                    
                    all_search_results.extend(search_results)
                    logger.info(f"  Semantic query {i+1}: Found {len(search_results)} candidates")
                    
                except Exception as e:
                    logger.warning(f"Semantic query {i} failed: {e}")
                    continue
            
            # STAGE 3: DEDUPLICATE AND MERGE RESULTS
            if len(semantic_queries) > 1:
                # Remove duplicates by chunk ID while preserving best similarity scores
                seen_ids = set()
                unique_results = []
                
                # Sort all results by similarity score (best first)
                all_search_results.sort(key=lambda x: x.similarity_score, reverse=True)
                
                for result in all_search_results:
                    chunk_id = getattr(result.chunk, 'id', None)
                    if chunk_id and chunk_id not in seen_ids:
                        seen_ids.add(chunk_id)
                        unique_results.append(result)
                    elif chunk_id is None:
                        unique_results.append(result)
                
                search_results = unique_results[:15]  # Cap at 15 like your original
                logger.info(f"üìä Merged {len(all_search_results)} results into {len(search_results)} unique candidates")
            else:
                search_results = all_search_results
            
            content_candidates = [result.chunk for result in search_results]
            
            if not content_candidates:
                logger.info("No relevant documents found - using general knowledge")
                return []
            
            logger.info(f"üìä Stage 2-3: Found {len(content_candidates)} content matches")
            
            # STAGE 4: Direct multi-objective scoring (style classification bypassed)
            if len(content_candidates) > top_k:
                try:
                    logger.info("‚ö° Stage 4: Direct multi-objective document scoring")
                    
                    # Apply multi-objective scoring directly to content candidates
                    scored_documents = await score_documents_multi_objective(
                        content_candidates, 
                        query, 
                        user_intent, 
                        self.ai_client
                    )
                    
                    # Select optimal mix using intelligent scoring
                    relevant_chunks = select_optimal_document_mix(scored_documents, top_k)
                    logger.info(f"‚ö° EFFICIENT SELECTION: {len(relevant_chunks)} documents via direct scoring")
                    
                except Exception as scoring_error:
                    logger.warning(f"Multi-objective scoring failed: {scoring_error}, using similarity-based selection")
                    relevant_chunks = content_candidates[:top_k]
            else:
                # Use content-based results when we have few candidates
                relevant_chunks = content_candidates[:top_k]
                logger.info(f"üìä Using content-based retrieval ({user_intent}) - {len(relevant_chunks)} candidates")
            

             #STAGE 5: All documents allowed (memos work as background intelligence)
            if relevant_chunks:
                logger.info(f"üìö Using all {len(relevant_chunks)} documents (statutes + memos as background)")

            # STAGE 6: RESULTS LOGGING (keeping your original format)
            if relevant_chunks:
                logger.info(f"Found {len(relevant_chunks)} relevant documents:")
                for i, chunk in enumerate(relevant_chunks):
                    # Find similarity score from original search results
                    similarity = 0.0
                    for result in search_results:
                        if result.chunk.id == chunk.id:
                            similarity = result.similarity_score
                            break
                    
                    semantic_source = chunk.metadata.get('semantic_source', 'original') if hasattr(chunk, 'metadata') and chunk.metadata else 'original'
                    logger.info(f"  {i+1}. {chunk.title[:50]}... (similarity: {similarity:.3f}, source: {semantic_source})")
            else:
                logger.info("No relevant documents found - using general knowledge")

            return relevant_chunks
            
        except Exception as e:
            logger.error(f"Error retrieving documents: {e}")
            return []


    # Add this method to your DocumentRetriever class (in the same class where get_relevant_documents is):

# Add this method to your DocumentRetriever class 
# (anywhere inside the class, preferably near the end)

   

    
class IntentClassifier:
    """AI-powered intent classifier - no hard-coding"""
    
    def __init__(self, ai_client: AsyncOpenAI, model: str):
        self.ai_client = ai_client
        self.model = model
        logger.info("üß† AI Intent Classifier initialized - zero hard-coding")
    
    async def classify_intent(self, query: str, conversation_history: List[Dict[str, str]] = None) -> Dict[str, any]:
        """Use AI to classify user intent dynamically"""
        try:
            # Build context for better classification
            context = ""
            if conversation_history:
                recent_context = conversation_history[-3:]  # Last 3 messages for context
                context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in recent_context])
                context = f"\n\nÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©:\n{context}\n"
            
            classification_prompt = CLASSIFICATION_PROMPT.format(query=query) + context
            
            logger.info(f"üß† Classifying intent for: {query[:30]}...")
            
            response = await self.ai_client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": classification_prompt}],
                max_tokens=200,
                temperature=0.1  # Low temperature for consistent classification
            )
            
            # Parse AI response
            result_text = response.choices[0].message.content.strip()
            
            # Clean up response (remove markdown if present)
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0].strip()
            
            # Parse JSON
            classification = json.loads(result_text)
            
            logger.info(f"üéØ Intent classified: {classification['category']} (confidence: {classification['confidence']:.2f})")
            
            # Validate classification
            valid_categories = ["GENERAL_QUESTION", "ACTIVE_DISPUTE", "PLANNING_ACTION"]
            if classification["category"] not in valid_categories:
                logger.warning(f"Invalid category: {classification['category']}, defaulting to GENERAL_QUESTION")
                classification["category"] = "GENERAL_QUESTION"
                classification["confidence"] = 0.5
            
            return classification
            
        except Exception as e:
            logger.error(f"Intent classification error: {e}")
            # Safe fallback
            return {
                "category": "GENERAL_QUESTION",
                "confidence": 0.5,
                "reasoning": f"Classification failed: {str(e)}"
            }


    # REPLACE your format_legal_context_naturally function entirely with this ultra-aggressive version:

    

class IntelligentLegalRAG:
    """
    Intelligent Legal RAG with AI-Powered Intent Classification
    No hard-coding - AI handles classification and prompt selection
    """
    
    def __init__(self):
        """Initialize intelligent RAG with AI classification"""
        self.ai_client = ai_client
        self.ai_model = ai_model
        
        # Add smart document retrieval
        self.storage = StorageFactory.create_storage()
        self.retriever = DocumentRetriever(
            storage=self.storage,
            ai_client=self.ai_client
        )
        
        # Add AI-powered intent classifier
        self.classifier = IntentClassifier(
            ai_client=self.ai_client,
            model=classification_model
        )
        
        logger.info("üöÄ Intelligent Legal RAG initialized - AI-powered classification + Smart retrieval!")
        self.citation_fixer = SimpleCitationFixer()
        logger.info("üîß Citation fixer initialized")
    

    def format_legal_context_naturally(self, retrieved_chunks: List[Chunk]) -> str:
        """
        CITATION-AWARE CONTEXT FORMATTER
        - Uses ALL documents for intelligence (including memos)
        - Only creates citation examples for STATUTES
        - Memos work as background intelligence only
        """
        if not retrieved_chunks:
            return ""
        
        statute_sources = []
        context_parts = []
        
        for i, chunk in enumerate(retrieved_chunks, 1):
            # Classify document type
            is_statute = any(term in chunk.title for term in ["ŸÜÿ∏ÿßŸÖ", "ÿßŸÑŸÖÿßÿØÿ©", "ŸÑÿßÿ¶ÿ≠ÿ©", "ŸÖÿ±ÿ≥ŸàŸÖ", "ÿßŸÑÿ™ÿπÿ±ŸäŸÅÿßÿ™"])
            is_memo = 'ŸÖÿ∞ŸÉÿ±ÿ©' in chunk.title.lower()
            
            # Clean content
            clean_content = chunk.content.replace('"', "'").replace('\n', ' ').replace('\r', ' ')
            preview = clean_content[:300] + "..." if len(clean_content) > 300 else clean_content
            
            if is_statute:
                # STATUTES: Available for citation
                statute_sources.append(chunk.title)
                formatted_chunk = f"""
    üìú **{chunk.title}** (ŸÖÿµÿØÿ± ŸÑŸÑÿßÿ≥ÿ™ÿ¥ŸáÿßÿØ)
    {preview}
    """
                context_parts.append(formatted_chunk)
                
            elif is_memo:
                # MEMOS: Background intelligence only
                formatted_chunk = f"""
    üìã **ÿÆŸÑŸÅŸäÿ© ŸÇÿßŸÜŸàŸÜŸäÿ© ŸÖŸÜ ŸÖÿ∞ŸÉÿ±ÿ© ÿØŸÅÿßÿπ** (ŸÑŸÑÿßÿ≥ÿ™ŸÅÿßÿØÿ© ŸÖŸÜ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ ŸÅŸÇÿ∑ - ŸÑÿß ÿ™ÿ≥ÿ™ÿ¥ŸáÿØ ÿ®Ÿáÿß)
    {preview}
    """
                context_parts.append(formatted_chunk)
                
            else:
                # OTHER DOCUMENTS: Include but check if citable
                formatted_chunk = f"""
    üìÑ **{chunk.title}**
    {preview}
    """
                context_parts.append(formatted_chunk)
        
        # Create citation examples ONLY for statutes
        citation_examples = []
        if len(statute_sources) >= 1:
            citation_examples.append(f'ŸàŸÅŸÇÿßŸã ŸÑŸÄ"{statute_sources[0]}"ÿå ŸÅÿ•ŸÜ ÿßŸÑÿ£ÿØŸÑÿ© Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ŸÉŸàŸÜ ÿµÿ≠Ÿäÿ≠ÿ©.')
        if len(statute_sources) >= 2:
            citation_examples.append(f'ÿßÿ≥ÿ™ŸÜÿßÿØÿßŸã ÿ•ŸÑŸâ "{statute_sources[1]}"ÿå ÿ™ÿ≥ÿ±Ÿä ÿ£ÿ≠ŸÉÿßŸÖ ÿßŸÑŸÜÿ∏ÿßŸÖ ÿßŸÑŸÇÿßÿ¶ŸÖ.')
        if len(statute_sources) >= 3:
            citation_examples.append(f'ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ "{statute_sources[2]}"ÿå ŸÜÿ∑ŸÑÿ® ÿ±ŸÅÿ∂ ÿßŸÑÿØÿπŸàŸâ.')
        
        # Build final context
        final_context = f"""ÿßŸÑŸÖÿ±ÿßÿ¨ÿπ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ© ŸàÿßŸÑÿÆŸÑŸÅŸäÿ© ÿßŸÑŸÖÿ™ÿßÿ≠ÿ©:
    {chr(10).join(context_parts)}

    üéØ ŸÇŸàÿßÿπÿØ ÿßŸÑÿßÿ≥ÿ™ÿ¥ŸáÿßÿØ ÿßŸÑÿ•ÿ¨ÿ®ÿßÿ±Ÿäÿ©:

    ‚úÖ ŸÖÿµÿßÿØÿ± ÿßŸÑÿßÿ≥ÿ™ÿ¥ŸáÿßÿØ ÿßŸÑŸÖÿ≥ŸÖŸàÿ≠ÿ© ŸÅŸÇÿ∑:
    """
        
        if statute_sources:
            for source in statute_sources:
                final_context += f"- {source}\n"
            
            final_context += f"""
    üí• ÿ£ŸÖÿ´ŸÑÿ© ÿßŸÑÿßÿ≥ÿ™ÿ¥ŸáÿßÿØ ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ© (ÿßÿ≥ÿ™ÿÆÿØŸÖ Ÿáÿ∞Ÿá ÿßŸÑÿ£ŸÜŸÖÿßÿ∑ ÿ®ÿßŸÑÿ∂ÿ®ÿ∑):
    {chr(10).join(citation_examples)}

    ‚ùå ŸÖŸÖŸÜŸàÿπ ÿ™ŸÖÿßŸÖÿßŸã ÿßŸÑÿßÿ≥ÿ™ÿ¥ŸáÿßÿØ ÿ®ŸÄ:
    - ÿ£Ÿä ŸÖÿ∞ŸÉÿ±ÿ© (ŸÖÿ∞ŸÉÿ±ÿ© civilÿå ŸÖÿ∞ŸÉÿ±ÿ© criminalÿå ŸÖÿ∞ŸÉÿ±ÿ© familyÿå ÿ•ŸÑÿÆ)
    - ŸÖÿ±ÿ¨ÿπ 1ÿå ŸÖÿ±ÿ¨ÿπ 2ÿå ÿ£Ÿà ÿ£Ÿä ÿ™ÿ±ŸÇŸäŸÖ
    - ÿ£Ÿä ŸÖÿµÿØÿ± ÿ∫Ÿäÿ± ŸÖÿ∞ŸÉŸàÿ± ŸÅŸä ÿßŸÑŸÇÿßÿ¶ŸÖÿ© ÿ£ÿπŸÑÿßŸá

    üî• ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÖÿ∞ŸÉÿ±ÿßÿ™ ŸÑŸÑÿßÿ≥ÿ™ŸÅÿßÿØÿ© ŸÖŸÜ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ ŸàÿßŸÑÿ≠ÿ¨ÿ¨ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ©
    üî• ŸÑŸÉŸÜ ÿßÿ≥ÿ™ÿ¥ŸáÿØ ŸÅŸÇÿ∑ ÿ®ÿßŸÑÿ£ŸÜÿ∏ŸÖÿ© ŸàÿßŸÑŸÖŸàÿßÿØ ÿßŸÑŸÖÿ∞ŸÉŸàÿ±ÿ© ÿ£ÿπŸÑÿßŸá

    ‚úÖ ŸÜŸÖÿ∑ ÿßŸÑÿßÿ≥ÿ™ÿ¥ŸáÿßÿØ ÿßŸÑŸàÿ≠ŸäÿØ ÿßŸÑŸÖŸÇÿ®ŸàŸÑ:
    ŸàŸÅŸÇÿßŸã ŸÑŸÄ"[ÿßŸÑÿßÿ≥ŸÖ ÿßŸÑŸÉÿßŸÖŸÑ ŸÑŸÑŸÜÿ∏ÿßŸÖ ÿ£Ÿà ÿßŸÑŸÖÿßÿØÿ©]"
    """
        else:
            final_context += """
    ‚ö†Ô∏è ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ£ŸÜÿ∏ŸÖÿ© ÿ£Ÿà ŸÖŸàÿßÿØ ŸÖÿ™ÿßÿ≠ÿ© ŸÑŸÑÿßÿ≥ÿ™ÿ¥ŸáÿßÿØ ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑÿ≥ŸäÿßŸÇ
    ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ ÿßŸÑŸÖÿ™ÿßÿ≠ ŸÑŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÇÿßŸÜŸàŸÜŸä ÿØŸàŸÜ ÿßÿ≥ÿ™ÿ¥ŸáÿßÿØÿßÿ™ ŸÖÿ®ÿßÿ¥ÿ±ÿ©
    """
        
        return final_context



    async def ask_question_with_context_streaming(
        self, 
        query: str, 
        conversation_history: List[Dict[str, str]]
    ) -> AsyncIterator[str]:
        """
        Intelligent context-aware legal consultation with AI classification
        """
        try:
            logger.info(f"Processing intelligent contextual legal question: {query[:50]}...")
            logger.info(f"Conversation context: {len(conversation_history)} messages")
            
            # Stage 1: AI-powered intent classification with context
            classification = await self.classifier.classify_intent(query, conversation_history)
            category = classification["category"]
            confidence = classification["confidence"]
            
            # Stage 2: Get relevant documents
            print(f"üî• DEBUG CATEGORY: category='{category}', type={type(category)}")
            if category == "ACTIVE_DISPUTE":
                top_k = 6  # Get more statutes for comprehensive legal citations
            elif category == "PLANNING_ACTION":
                top_k = 5  # Need good coverage for planning
            else:
                top_k = 3  # General questions need fewer documents

            relevant_docs = await self.retriever.get_relevant_documents(query, top_k=top_k, user_intent=category)
            
            # Stage 3: Select appropriate prompt
            system_prompt = PROMPT_TEMPLATES[category]
            
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            # Stage 4: Add conversation history (last 8 messages)
            recent_history = conversation_history[-8:] if len(conversation_history) > 8 else conversation_history
            for msg in recent_history:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            # Stage 5: Add current question with legal context if available
            if relevant_docs:
                legal_context = self.format_legal_context_naturally(relevant_docs)
                contextual_prompt = f"""{legal_context}

ÿßŸÑÿ≥ÿ§ÿßŸÑ: {query}"""
                logger.info(f"Using {len(relevant_docs)} relevant legal documents with {category} approach (contextual)")
            else:
                contextual_prompt = query
                logger.info(f"No relevant documents found - using {category} approach with contextual general knowledge")
            
            messages.append({
                "role": "user", 
                "content": contextual_prompt
            })
            
            # Stage 6: Stream intelligent contextual response
            async for chunk in self._stream_ai_response(messages, category):
                yield chunk
                
        except Exception as e:
            logger.error(f"Intelligent contextual legal AI error: {e}")
            yield f"ÿπÿ∞ÿ±ÿßŸãÿå ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿ≥ÿ§ÿßŸÑŸÉ: {str(e)}"
    
    async def _stream_ai_response(self, messages: List[Dict[str, str]], category: str = "GENERAL_QUESTION") -> AsyncIterator[str]:
        """Stream AI response with error handling"""
        try:
            stream = await self.ai_client.chat.completions.create(
                model=self.ai_model,
                messages=messages,
                temperature=0.05 if category == "ACTIVE_DISPUTE" else 0.15,
                max_tokens=4000 if category == "ACTIVE_DISPUTE" else 1500,  # ‚Üê GIVE DISPUTES MORE SPACE!
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"AI streaming error: {e}")
            error_msg = str(e).lower()
            
            if "rate limit" in error_msg or "429" in error_msg:
                yield "\n\n‚è≥ ÿ™ŸÖ ÿ™ÿ¨ÿßŸàÿ≤ ÿßŸÑÿ≠ÿØ ÿßŸÑŸÖÿ≥ŸÖŸàÿ≠ ŸÖÿ§ŸÇÿ™ÿßŸã. Ÿäÿ±ÿ¨Ÿâ ÿßŸÑÿßŸÜÿ™ÿ∏ÿßÿ± ÿØŸÇŸäŸÇÿ© Ÿàÿ•ÿπÿßÿØÿ© ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ©."
            elif "api key" in error_msg or "authentication" in error_msg:
                yield "\n\nüîë ÿÆÿ∑ÿ£ ŸÅŸä ŸÖŸÅÿ™ÿßÿ≠ API. Ÿäÿ±ÿ¨Ÿâ ÿßŸÑÿ™ŸàÿßÿµŸÑ ŸÖÿπ ÿßŸÑÿØÿπŸÖ ÿßŸÑŸÅŸÜŸä."
            else:
                yield f"\n\n‚ùå ÿÆÿ∑ÿ£ ÿ™ŸÇŸÜŸä: {str(e)}"
    
    async def generate_conversation_title(self, first_message: str) -> str:
        """Intelligent conversation title generation"""
        try:
            title_prompt = f"ÿßŸÇÿ™ÿ±ÿ≠ ÿπŸÜŸàÿßŸÜÿßŸã ŸÖÿÆÿ™ÿµÿ±ÿßŸã (ÿ£ŸÇŸÑ ŸÖŸÜ 30 ÿ≠ÿ±ŸÅ) ŸÑŸáÿ∞Ÿá ÿßŸÑÿßÿ≥ÿ™ÿ¥ÿßÿ±ÿ© ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ©: {first_message[:100]}"
            
            response = await self.ai_client.chat.completions.create(
                model=classification_model,  # Use small model for title generation
                messages=[{"role": "user", "content": title_prompt}],
                max_tokens=50,
                temperature=0.3
            )
            
            title = response.choices[0].message.content.strip()
            title = title.strip('"').strip("'").strip()
            
            # Remove common prefixes
            prefixes = ["ÿßŸÑÿπŸÜŸàÿßŸÜ:", "ÿßŸÑŸÖŸÇÿ™ÿ±ÿ≠:", "ÿπŸÜŸàÿßŸÜ:"]
            for prefix in prefixes:
                if title.startswith(prefix):
                    title = title[len(prefix):].strip()
            
            return title[:30] if len(title) > 30 else title
            
        except Exception as e:
            logger.error(f"Title generation error: {e}")
            return first_message[:25] + "..." if len(first_message) > 25 else first_message


# Global instance - maintains compatibility with existing code
rag_engine = IntelligentLegalRAG()

# Clean exports - no legacy debt
def get_rag_engine():
    """Get the RAG engine instance"""
    return rag_engine

# For external usage - clean streaming interface
async def ask_question_streaming(query: str) -> AsyncIterator[str]:
    """Modern streaming interface"""
    async for chunk in rag_engine.ask_question_with_context_streaming(query, []):
        yield chunk

async def ask_question_with_context_streaming(query: str, conversation_history: List[Dict[str, str]]) -> AsyncIterator[str]:
    """Modern contextual streaming interface"""
    async for chunk in rag_engine.ask_question_with_context_streaming(query, conversation_history):
        yield chunk

# Test function
async def test_intelligent_rag():
    """Test the intelligent RAG system with classification"""
    print("üß™ Testing intelligent RAG engine with AI classification...")
    
    test_queries = [
        "ŸÖÿß ŸáŸä ÿπŸÇŸàÿ®ÿßÿ™ ÿßŸÑÿ™Ÿáÿ±ÿ® ÿßŸÑÿ∂ÿ±Ÿäÿ®Ÿäÿü",  # Should be GENERAL_QUESTION
        "ÿ±ŸÅÿπ ÿπŸÑŸä ÿÆÿµŸÖ ÿØÿπŸàŸâ ŸÉŸäÿØŸäÿ© ŸÉŸäŸÅ ÿ£ÿ±ÿØ ÿπŸÑŸäŸáÿü",  # Should be ACTIVE_DISPUTE
        "ÿ£ÿ±ŸäÿØ ŸÖŸÇÿßÿ∂ÿßÿ© ÿ¥ÿ±ŸÉÿ™Ÿä ŸáŸÑ ÿßŸÑÿ£ŸÖÿ± Ÿäÿ≥ÿ™ÿ≠ŸÇÿü"  # Should be PLANNING_ACTION
    ]
    
    for query in test_queries:
        print(f"\nüß™ Testing: {query}")
        print("Response:")
        
        response_chunks = []
        async for chunk in rag_engine.ask_question_streaming(query):
            response_chunks.append(chunk)
            print(chunk, end="", flush=True)
        
        print(f"\n‚úÖ Test complete for this query!\n{'-'*50}")
    
    return True

# System initialization
print("üèõÔ∏è Intelligent Legal RAG Engine loaded - AI-powered classification + Smart document retrieval!")

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_intelligent_rag())