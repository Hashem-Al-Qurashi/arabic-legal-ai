"""
Intelligent RAG Engine with AI-Powered Intent Classification
No hard-coding - AI handles all classification and prompt selection
Natural conversations + Smart document retrieval + Dynamic prompt selection
"""

import os
import logging
from datetime import datetime
from dotenv import load_dotenv
from openai import AsyncOpenAI
from typing import List, Dict, Optional, AsyncIterator
import json

# Import the smart database components from old RAG
from app.storage.vector_store import VectorStore, Chunk
from app.storage.sqlite_store import SqliteVectorStore

# Load environment variables
load_dotenv(".env")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Simple API key configuration
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Initialize AI client - prioritize OpenAI, fallback to DeepSeek
if OPENAI_API_KEY:
    ai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    ai_model = "gpt-4o"
    classification_model = "gpt-4o-mini"  # Small model for classification
    print("âœ… Using OpenAI for intelligent legal AI with classification")
elif DEEPSEEK_API_KEY:
    ai_client = AsyncOpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com/v1")
    ai_model = "deepseek-chat"
    classification_model = "deepseek-chat"
    print("âœ… Using DeepSeek for intelligent legal AI with classification")
else:
    raise ValueError("âŒ Either OPENAI_API_KEY or DEEPSEEK_API_KEY must be provided")


# DYNAMIC PROMPTS - NO HARD-CODING OF CATEGORIES
CLASSIFICATION_PROMPT = """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©. Ø­Ù„Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ­Ø¯Ø¯ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

Ø±Ø¯Ùƒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† JSON ÙÙ‚Ø· Ø¨Ù‡Ø°Ø§ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚:
{{
    "category": "GENERAL_QUESTION | ACTIVE_DISPUTE | PLANNING_ACTION",
    "confidence": 0.95,
    "reasoning": "Ø³Ø¨Ø¨ Ø§Ù„ØªØµÙ†ÙŠÙ"
}}

Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª:
- GENERAL_QUESTION: Ø³Ø¤Ø§Ù„ Ø¹Ø§Ù… Ù„Ù„Ù…Ø¹Ø±ÙØ© ("Ù…Ø§ Ù‡ÙŠ", "ÙƒÙŠÙ", "Ù‡Ù„ ÙŠØ¬ÙˆØ²")
- ACTIVE_DISPUTE: Ù…Ø´ÙƒÙ„Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù†Ø´Ø·Ø© ØªØ­ØªØ§Ø¬ Ø¯ÙØ§Ø¹ ("Ø±ÙØ¹ Ø¹Ù„ÙŠ Ø¯Ø¹ÙˆÙ‰", "Ø®ØµÙ…ÙŠ ÙŠØ¯Ø¹ÙŠ", "ÙƒÙŠÙ Ø£Ø±Ø¯")
- PLANNING_ACTION: ÙŠØ®Ø·Ø· Ù„Ø§ØªØ®Ø§Ø° Ø¥Ø¬Ø±Ø§Ø¡ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ("Ø£Ø±ÙŠØ¯ Ù…Ù‚Ø§Ø¶Ø§Ø©", "Ù‡Ù„ Ø£Ø±ÙØ¹ Ø¯Ø¹ÙˆÙ‰", "ÙƒÙŠÙ Ø£Ø±ÙØ¹ Ù‚Ø¶ÙŠØ©")

Ø±Ø¯Ùƒ JSON ÙÙ‚Ø·ØŒ Ù„Ø§ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ."""

# DYNAMIC PROMPT TEMPLATES - AI CHOOSES THE RIGHT PERSONALITY
PROMPT_TEMPLATES = {
    "GENERAL_QUESTION": """Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ ÙˆØ¯ÙˆØ¯ ÙˆÙ…ÙÙŠØ¯.

ğŸ¯ Ù…Ù‡Ù…ØªÙƒ:
- Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ¨Ø³Ø§Ø·Ø©
- Ø´Ø±Ø­ Ø§Ù„Ø­Ù‚ÙˆÙ‚ ÙˆØ§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ÙÙ‡ÙˆÙ…Ø©  
- Ø¥Ø¹Ø·Ø§Ø¡ Ù†ØµØ§Ø¦Ø­ Ø¹Ù…Ù„ÙŠØ© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚
- Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù„Ù„ÙÙ‡Ù… Ø£ÙƒØ«Ø± Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©

âš–ï¸ Ù…Ù†Ù‡Ø¬Ùƒ:
- Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„
- Ø§Ø°ÙƒØ± Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ø·Ø¨ÙŠØ¹ÙŠØ©: "Ø­Ø³Ø¨ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ØŒ Ø§Ù„Ù…Ø§Ø¯Ø© 12 , Ù„Ø§Ø¨Ø¯ Ù…Ù† Ø°ÙƒØ± Ø§Ù„Ù…ØµØ¯Ø± Ø§Ø°Ø§ ÙˆØ¬Ø¯"
- Ù‚Ø¯Ù… Ø®Ø·ÙˆØ§Øª Ø¹Ù…Ù„ÙŠØ© ÙˆØ§Ø¶Ø­Ø©
- Ù„Ø§ ØªØ¹Ù‚Ø¯ Ø§Ù„Ø£Ù…ÙˆØ± Ø¨Ù„Ø§ Ø¯Ø§Ø¹

ğŸ”¥ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ©:
- Ø§Ù‚ØªØ±Ø­ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
- ÙƒÙ† Ù…Ø­Ø¯Ø¯Ø§Ù‹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø­Ø§Ù„ØªÙ‡

ğŸš« ØªØ¬Ù†Ø¨:
- Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© ÙˆØ§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ØµØ¹Ø¨Ø©
- Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø§Ù„Ù…ÙØ±Ø·Ø©  
- Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø¬Ø§Ù‡Ø²Ø©
- Ø§Ù„Ø¥Ø·Ø§Ù„Ø© Ø¨Ù„Ø§ ÙØ§Ø¦Ø¯Ø©

ØªØ­Ø¯Ø« ÙƒÙ…Ø³ØªØ´Ø§Ø± Ù…Ø­ØªØ±Ù ÙŠÙ‡ØªÙ… Ø¨Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù†Ø§Ø³ ÙÙ‡Ù… Ø­Ù‚ÙˆÙ‚Ù‡Ù….""",

    "ACTIVE_DISPUTE": """

# ACTIVE_DISPUTE - Reasoning Model

## Core Legal Identity
Ø£Ù†Øª Ù…Ø­Ø§Ù…Ù Ø³Ø¹ÙˆØ¯ÙŠ Ù…Ø­ØªØ±ÙØŒ Ù…ØªÙ…Ø±Ø³ ÙÙŠ Ø§Ù„Ø¯ÙØ§Ø¹ Ø§Ù„Ù…Ø¯Ù†ÙŠØŒ ØªÙ…ØªÙ„Ùƒ Ù‚Ø¯Ø±Ø© Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠØ© Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ø¹Ø§ÙˆÙ‰ ÙˆÙƒØ´Ù Ù†Ù‚Ø§Ø· Ø¶Ø¹ÙÙ‡Ø§. ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙƒÙ„ Ù‚Ø¶ÙŠØ© ÙƒØ·Ø¨ÙŠØ¨ Ø¬Ø±Ø§Ø­ - Ø¨Ø¯Ù‚Ø© ÙˆÙ„Ø§ Ù…Ø¬Ø§Ù„ Ù„Ù„Ø®Ø·Ø£.

## Legal Philosophy
- Ø§Ù„Ø£Ø¯Ù„Ø© ØªØªØ­Ø¯Ø«ØŒ Ù„ÙŠØ³ Ø§Ù„Ø¹ÙˆØ§Ø·Ù
- ÙƒÙ„ Ø§Ø¯Ø¹Ø§Ø¡ ÙŠØ­ØªØ§Ø¬ Ø¥Ø«Ø¨Ø§Øª Ù‚Ø§Ø·Ø¹
- Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¯Ø§Ù„Ø©ØŒ Ù„ÙŠØ³ Ù„Ù„Ø§Ø³ØªØºÙ„Ø§Ù„
- Ø§Ù„Ø®ØµÙ… Ø¨Ø±ÙŠØ¡ Ø­ØªÙ‰ ÙŠØ«Ø¨Øª Ø¨Ø±Ø§Ø¡Ø© Ø§Ø¯Ø¹Ø§Ø¦Ù‡

## Reasoning Framework

### Primary Analysis Mode
Ø§Ø³Ø£Ù„ Ù†ÙØ³Ùƒ Ø¯Ø§Ø¦Ù…Ø§Ù‹: "Ù…Ø§ Ù‡Ùˆ Ø£Ø¶Ø¹Ù Ø¹Ù†ØµØ± ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø¹ÙˆÙ‰ØŸ" Ø«Ù… Ø§Ø¨Ù†ÙŠ ØªØ­Ù„ÙŠÙ„Ùƒ Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù†ØµØ±.

### Legal Investigation Process
1. **Ø­Ù„Ù„ Ø§Ù„Ø£Ø¯Ù„Ø©**: Ù…Ø§ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯ØŸ Ù…Ø§ Ø§Ù„Ù…Ø´ÙƒÙˆÙƒ ÙÙŠÙ‡ØŸ Ù…Ø§ Ø§Ù„Ù…ØªÙ†Ø§Ù‚Ø¶ØŸ
2. **Ø§Ø®ØªØ¨Ø± Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ**: Ù‡Ù„ Ø§Ù„Ø§Ø¯Ø¹Ø§Ø¡ Ù…Ù†Ø·Ù‚ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ§Ù‹ØŸ
3. **ÙØ­Øµ Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚**: ÙƒÙŠÙ ÙŠÙ†Ø¸Ø± Ø§Ù„Ù‚Ø¶Ø§Ø¡ Ù„Ø­Ø§Ù„Ø§Øª Ù…Ù…Ø§Ø«Ù„Ø©ØŸ
4. **ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬**: Ù…Ø§ Ù‡ÙŠ Ø£Ù‚ÙˆÙ‰ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¯ÙØ§Ø¹ØŸ

## Prohibited Approaches
ğŸš« **Ù…Ù…Ù†ÙˆØ¹ Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹:**
- Ø§Ù„ØªØ¨Ø¹ Ø§Ù„Ø£Ø¹Ù…Ù‰ Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø¬Ø§Ù‡Ø²Ø©
- Ø§ÙØªØ±Ø§Ø¶ Ø­Ø³Ù† Ù†ÙŠØ© Ø§Ù„Ø®ØµÙ…
- Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª ("Ø±Ø¨Ù…Ø§"ØŒ "Ù‚Ø¯ ÙŠÙƒÙˆÙ†")
- Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„ÙŠÙ…ÙŠÙ† Ø§Ù„Ø­Ø§Ø³Ù…Ø©
- Ø§Ù„Ù†Ø¨Ø±Ø© Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ© Ø£Ùˆ Ø§Ù„Ù‡Ø¬ÙˆÙ…ÙŠØ© ØºÙŠØ± Ø§Ù„Ù…Ø¨Ø±Ø±Ø©
- Ù†Ø³Ø® Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø¯ÙˆÙ† Ø±Ø¨Ø·Ù‡Ø§ Ø¨Ø§Ù„ÙˆØ§Ù‚Ø¹

## Dynamic Response Strategy

### Natural Flow Principle
Ø¯Ø¹ ÙƒÙ„ Ù‚Ø¶ÙŠØ© ØªØ­Ø¯Ø¯ Ø·Ø±ÙŠÙ‚Ø© ØªØ­Ù„ÙŠÙ„Ù‡Ø§:
- Ù‚Ø¶ÙŠØ© Ø¶Ø¹ÙŠÙØ© Ø§Ù„Ø£Ø¯Ù„Ø©ØŸ Ø§Ø¨Ø¯Ø£ Ø¨ØªÙÙƒÙŠÙƒ Ø§Ù„Ø£Ø¯Ù„Ø©
- Ù‚Ø¶ÙŠØ© Ù…ØªÙ†Ø§Ù‚Ø¶Ø©ØŸ Ø§Ø¨Ø¯Ø£ Ø¨ÙƒØ´Ù Ø§Ù„ØªÙ†Ø§Ù‚Ø¶Ø§Øª  
- Ù‚Ø¶ÙŠØ© Ù…ÙØªÙ‚Ø±Ø© Ù„Ù„Ø³Ù†Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØŸ Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†
- Ù‚Ø¶ÙŠØ© ÙˆØ§Ø¶Ø­Ø© Ø§Ù„ÙƒÙŠØ¯ÙŠØ©ØŸ Ø§Ø¨Ø¯Ø£ Ø¨ÙƒØ´Ù Ø³ÙˆØ¡ Ø§Ù„Ù†ÙŠØ©

### Adaptive Structure
Ù„Ø§ ØªÙ„ØªØ²Ù… Ø¨Ù‡ÙŠÙƒÙ„ Ø«Ø§Ø¨Øª. Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø§ ÙŠÙ†Ø§Ø³Ø¨ Ø§Ù„Ù‚Ø¶ÙŠØ©:
- ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø± Ù„Ù„Ø£Ø¯Ù„Ø©
- Ù…Ù†Ø§Ù‚Ø´Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…ØªØ¹Ù…Ù‚Ø©  
- Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©
- ØªØ­Ù„ÙŠÙ„ Ù†ÙØ³ÙŠ Ù„Ø¯ÙˆØ§ÙØ¹ Ø§Ù„Ù…Ø¯Ø¹ÙŠ

## Professional Standards

### Tone Guidelines
- **Ø­Ø§Ø²Ù… Ø¯ÙˆÙ† Ø¹Ø¯ÙˆØ§Ù†ÙŠØ©**: ÙƒÙ† ÙˆØ§Ø«Ù‚Ø§Ù‹ØŒ Ù„ÙŠØ³ Ù…ØªÙ†Ù…Ø±Ø§Ù‹
- **Ø°ÙƒÙŠ Ø¯ÙˆÙ† ØªØ¹Ø§Ù„ÙŠ**: Ø£Ø¸Ù‡Ø± Ø®Ø¨Ø±ØªÙƒØŒ Ù„Ø§ ØºØ±ÙˆØ±Ùƒ
- **Ø³Ø§Ø®Ø± Ø¨Ù„Ø¨Ø§Ù‚Ø©**: Ø§Ù„Ø°ÙƒØ§Ø¡ ÙŠØªØ­Ø¯Ø« Ø¨Ù‡Ø¯ÙˆØ¡

### Credibility Markers
- Ø§Ø³ØªØ´Ù‡Ø¯ Ø¨Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©ØŒ Ù„Ø§ Ù„Ù„Ø¥Ø¹Ø¬Ø§Ø¨
- Ø§Ø±Ø¨Ø· ÙƒÙ„ Ù†Ù‚Ø·Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ø§Ù„ÙˆØ§Ù‚Ø¹ Ù…Ø¨Ø§Ø´Ø±Ø©
- Ù‚Ø¯Ù… Ø­Ù„ÙˆÙ„ Ø¹Ù…Ù„ÙŠØ©ØŒ Ù„ÙŠØ³ ÙÙ„Ø³ÙØ© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

## Closing Strategy
Ø§Ø®ØªØªÙ… Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø·Ø¨ÙŠØ¹ÙŠØ© ØªÙ†Ø§Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚:
- Ø§Ù‚ØªØ±Ø§Ø­ Ø¹Ù…Ù„ÙŠ Ù…Ø­Ø¯Ø¯
- Ø³Ø¤Ø§Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ
- Ù…Ù„Ø®Øµ Ù‚ÙˆÙŠ Ù„Ù„Ù…ÙˆÙ‚Ù
- Ø®Ø·ÙˆØ© ØªØ§Ù„ÙŠØ© ÙˆØ§Ø¶Ø­Ø©

## The Ultimate Test
Ø¨Ø¹Ø¯ ÙƒØªØ§Ø¨Ø© ØªØ­Ù„ÙŠÙ„ÙƒØŒ Ø§Ø³Ø£Ù„ Ù†ÙØ³Ùƒ:
"Ù‡Ù„ ÙŠØ¨Ø¯Ùˆ Ù‡Ø°Ø§ ÙˆÙƒØ£Ù†Ù†ÙŠ Ø£Ø­Ù„Ù„ Ù‚Ø¶ÙŠØ© Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù„Ù…ÙˆÙƒÙ„ Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø£Ù… ÙˆÙƒØ£Ù†Ù†ÙŠ Ø£Ù…Ù„Ø£ Ø§Ø³ØªÙ…Ø§Ø±Ø©ØŸ"

Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø§Ù„Ø«Ø§Ù†ÙŠØŒ Ø£Ø¹Ø¯ Ø§Ù„ÙƒØªØ§Ø¨Ø©.
""",

    "PLANNING_ACTION": """Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªØ®Ø·ÙŠØ· Ù„Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©.

ğŸ¯ Ù…Ù‡Ù…ØªÙƒ:
- ØªÙ‚ÙŠÙŠÙ… Ø¬Ø¯ÙˆÙ‰ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
- ÙˆØ¶Ø¹ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ÙˆØ§Ø¶Ø­Ø© Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©
- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„Ø¹ÙˆØ§Ø¦Ø¯ Ø¨ØµØ±Ø§Ø­Ø©
- Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„Ù‚Ø±Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­

âš–ï¸ Ù…Ù†Ù‡Ø¬Ùƒ:
- Ù‚ÙŠÙ… Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ù…ÙˆØ¶ÙˆØ¹ÙŠØ©
- Ø§Ø´Ø±Ø­ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ø¨ÙˆØ¶ÙˆØ­
- Ø­Ø¯Ø¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙˆØ§Ù„ØªÙƒØ§Ù„ÙŠÙ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
- Ø§Ù†ØµØ­ Ø¨Ø£ÙØ¶Ù„ Ù…Ø³Ø§Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚

ğŸ”¥ Ø§Ù„ØªØ±ÙƒÙŠØ²:
- Ø®Ø·Ø© Ø¹Ù…Ù„ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚
- ØªÙˆÙ‚Ø¹Ø§Øª ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù†ØªØ§Ø¦Ø¬
- Ø¨Ø¯Ø§Ø¦Ù„ Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ

ØªØ­Ø¯Ø« ÙƒÙ…Ø³ØªØ´Ø§Ø± Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ ÙŠØ³Ø§Ø¹Ø¯ ÙÙŠ Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ©."""
}


class StorageFactory:
    """Factory for creating storage backends"""
    
    @staticmethod
    def create_storage() -> VectorStore:
        """Create storage backend based on environment configuration"""
        storage_type = os.getenv("VECTOR_STORAGE_TYPE", "sqlite").lower()
        
        if storage_type == "sqlite":
            db_path = os.getenv("SQLITE_DB_PATH", "data/vectors.db")
            return SqliteVectorStore(db_path)
        else:
            raise ValueError(f"Unknown storage type: {storage_type}")


class DocumentRetriever:
    """Smart document retriever - gets relevant Saudi legal documents from database"""
    
    def __init__(self, storage: VectorStore, ai_client: AsyncOpenAI):
        self.storage = storage
        self.ai_client = ai_client
        self.initialized = False
        logger.info(f"DocumentRetriever initialized with {type(storage).__name__}")
    
    async def initialize(self) -> None:
        """Initialize storage backend"""
        if self.initialized:
            return
        
        try:
            await self.storage.initialize()
            stats = await self.storage.get_stats()
            logger.info(f"Storage initialized with {stats.total_chunks} existing documents")
            self.initialized = True
        except Exception as e:
            logger.error(f"Failed to initialize retriever: {e}")
            raise
    
    async def get_relevant_documents(self, query: str, top_k: int = 3, user_intent: str = "GENERAL_QUESTION") -> List[Chunk]:
        """
        Enhanced document retrieval with dual-stage filtering:
        Stage 1: Content-based retrieval (existing system)  
        Stage 2: Style-based filtering using AI
        """
        if not self.initialized:
            await self.initialize()
        
        try:
            stats = await self.storage.get_stats()
            if stats.total_chunks == 0:
                logger.info("No documents found in storage - using general knowledge")
                return []
            
            logger.info(f"ğŸ” Enhanced search in {stats.total_chunks} documents for: '{query[:50]}...'")
            logger.info(f"ğŸ“‹ User intent: {user_intent}")
            
            # Get query embedding
            response = await self.ai_client.embeddings.create(
                model="text-embedding-ada-002",
                input=query
            )
            query_embedding = response.data[0].embedding
            
            # STAGE 1: Content-based retrieval (your existing system)
            logger.info("ğŸš€ Stage 1: Content-based document retrieval")
            
            # Get more candidates for style filtering (4x the requested amount)
            expanded_top_k = min(top_k * 4, 15)  # Get more candidates but cap at 15
            search_results = await self.storage.search_similar(
                query_embedding, 
                top_k=expanded_top_k, 
                query_text=query, 
                openai_client=self.ai_client
            )
            content_candidates = [result.chunk for result in search_results]
            
            if not content_candidates:
                logger.info("No relevant documents found - using general knowledge")
                return []
            
            logger.info(f"ğŸ“Š Stage 1: Found {len(content_candidates)} content matches")
            
            # STAGE 2: Style-based filtering (only if we have multiple candidates)
            if len(content_candidates) > top_k and user_intent == "ACTIVE_DISPUTE":
                try:
                    logger.info("ğŸ¨ Stage 2: AI-powered style filtering")
                    
                    # Import style classifier
                    from app.legal_reasoning.ai_style_classifier import AIStyleClassifier
                    style_classifier = AIStyleClassifier(self.ai_client)
                    
                    # Get target style for this intent
                    target_style = style_classifier.get_style_for_intent(user_intent)
                    logger.info(f"ğŸ¯ Target style: {target_style}")
                    
                    # Classify documents by style
                    styled_documents = await style_classifier.filter_documents_by_style(
                        content_candidates, 
                        target_style=target_style,
                        min_confidence=0.6  # Lower threshold for more matches
                    )
                    
                    # Separate style matches from others
                    style_matches = [doc for doc in styled_documents if doc["style_match"]]
                    all_styled = styled_documents
                    
                    logger.info(f"âœ¨ Style matches: {len(style_matches)}")
                    
                    # Smart selection: prioritize style matches
                    final_documents = []
                    
                    if style_matches:
                        # Add style matches first
                        style_docs = [doc["document"] for doc in style_matches[:top_k]]
                        final_documents.extend(style_docs)
                        
                        # Fill remaining with best content matches
                        remaining = top_k - len(style_docs)
                        if remaining > 0:
                            other_docs = [doc["document"] for doc in all_styled 
                                        if not doc["style_match"]][:remaining]
                            final_documents.extend(other_docs)
                        
                        logger.info(f"ğŸ¯ Selected: {len([d for d in styled_documents[:len(final_documents)] if d['style_match']])} style + {len(final_documents) - len([d for d in styled_documents[:len(final_documents)] if d['style_match']])} content")
                    else:
                        # No style matches - use best content
                        final_documents = [doc["document"] for doc in all_styled[:top_k]]
                        logger.info(f"ğŸ“Š No style matches - using top {len(final_documents)} content matches")
                    
                    relevant_chunks = final_documents[:top_k]
                    
                except Exception as style_error:
                    logger.warning(f"Style filtering failed: {style_error}, using content-only")
                    relevant_chunks = content_candidates[:top_k]
            else:
                # Use original content-based results for non-dispute queries or limited candidates
                relevant_chunks = content_candidates[:top_k]
                logger.info(f"ğŸ“Š Using content-based retrieval ({user_intent})")
            
            # Log final results (keeping your original format)
            if relevant_chunks:
                logger.info(f"Found {len(relevant_chunks)} relevant documents:")
                for i, chunk in enumerate(relevant_chunks):
                    # Find similarity score from original search results
                    similarity = 0.0
                    for result in search_results:
                        if result.chunk.id == chunk.id:
                            similarity = result.similarity_score
                            break
                    logger.info(f"  {i+1}. {chunk.title[:50]}... (similarity: {similarity:.3f})")
            else:
                logger.info("No relevant documents found - using general knowledge")
            
            return relevant_chunks
            
        except Exception as e:
            logger.error(f"Error retrieving documents: {e}")
            return []


class IntentClassifier:
    """AI-powered intent classifier - no hard-coding"""
    
    def __init__(self, ai_client: AsyncOpenAI, model: str):
        self.ai_client = ai_client
        self.model = model
        logger.info("ğŸ§  AI Intent Classifier initialized - zero hard-coding")
    
    async def classify_intent(self, query: str, conversation_history: List[Dict[str, str]] = None) -> Dict[str, any]:
        """Use AI to classify user intent dynamically"""
        try:
            # Build context for better classification
            context = ""
            if conversation_history:
                recent_context = conversation_history[-3:]  # Last 3 messages for context
                context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in recent_context])
                context = f"\n\nØ³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:\n{context}\n"
            
            classification_prompt = CLASSIFICATION_PROMPT.format(query=query) + context
            
            logger.info(f"ğŸ§  Classifying intent for: {query[:30]}...")
            
            response = await self.ai_client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": classification_prompt}],
                max_tokens=200,
                temperature=0.1  # Low temperature for consistent classification
            )
            
            # Parse AI response
            result_text = response.choices[0].message.content.strip()
            
            # Clean up response (remove markdown if present)
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0].strip()
            
            # Parse JSON
            classification = json.loads(result_text)
            
            logger.info(f"ğŸ¯ Intent classified: {classification['category']} (confidence: {classification['confidence']:.2f})")
            
            # Validate classification
            valid_categories = ["GENERAL_QUESTION", "ACTIVE_DISPUTE", "PLANNING_ACTION"]
            if classification["category"] not in valid_categories:
                logger.warning(f"Invalid category: {classification['category']}, defaulting to GENERAL_QUESTION")
                classification["category"] = "GENERAL_QUESTION"
                classification["confidence"] = 0.5
            
            return classification
            
        except Exception as e:
            logger.error(f"Intent classification error: {e}")
            # Safe fallback
            return {
                "category": "GENERAL_QUESTION",
                "confidence": 0.5,
                "reasoning": f"Classification failed: {str(e)}"
            }


def format_legal_context_naturally(retrieved_chunks: List[Chunk]) -> str:
    """Format legal documents in a natural way"""
    if not retrieved_chunks:
        return ""
    
    context_parts = []
    for i, chunk in enumerate(retrieved_chunks, 1):
        formatted_chunk = f"""
**Ù…Ø±Ø¬Ø¹ {i}: {chunk.title}**
{chunk.content}
"""
        context_parts.append(formatted_chunk)
    
    context = f"""Ù„Ø¯ÙŠÙƒ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©:

{chr(10).join(context_parts)}

Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒØŒ ÙˆÙ„ÙƒÙ† Ù„Ø§ ØªØ¬Ø¹Ù„ Ø±Ø¯Ùƒ ÙŠØ¨Ø¯Ùˆ ÙƒØ¢Ù„Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©. ØªØ­Ø¯Ø« Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ø³ØªØ´Ù‡Ø¯ Ø¨Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø© ÙÙ‚Ø·."""
    
    return context


class IntelligentLegalRAG:
    """
    Intelligent Legal RAG with AI-Powered Intent Classification
    No hard-coding - AI handles classification and prompt selection
    """
    
    def __init__(self):
        """Initialize intelligent RAG with AI classification"""
        self.ai_client = ai_client
        self.ai_model = ai_model
        
        # Add smart document retrieval
        self.storage = StorageFactory.create_storage()
        self.retriever = DocumentRetriever(
            storage=self.storage,
            ai_client=self.ai_client
        )
        
        # Add AI-powered intent classifier
        self.classifier = IntentClassifier(
            ai_client=self.ai_client,
            model=classification_model
        )
        
        logger.info("ğŸš€ Intelligent Legal RAG initialized - AI-powered classification + Smart retrieval!")
    
    async def ask_question_streaming(self, query: str) -> AsyncIterator[str]:
        """
        Intelligent legal consultation with AI-powered intent classification
        """
        try:
            logger.info(f"Processing intelligent legal question: {query[:50]}...")
            
            # Stage 1: AI-powered intent classification
            classification = await self.classifier.classify_intent(query)
            category = classification["category"]
            confidence = classification["confidence"]
            
            # Stage 2: Get relevant documents from database
            relevant_docs = await self.retriever.get_relevant_documents(query, top_k=3)
            
            # Stage 3: Select appropriate prompt based on AI classification
            system_prompt = PROMPT_TEMPLATES[category]
            
            # Stage 4: Build intelligent prompt with documents
            if relevant_docs:
                legal_context = format_legal_context_naturally(relevant_docs)
                full_prompt = f"""{legal_context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}"""
                logger.info(f"Using {len(relevant_docs)} relevant legal documents with {category} approach")
            else:
                full_prompt = query
                logger.info(f"No relevant documents found - using {category} approach with general knowledge")
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": full_prompt}
            ]
            
            # Stage 5: Stream intelligent response
            async for chunk in self._stream_ai_response(messages):
                yield chunk
                
        except Exception as e:
            logger.error(f"Intelligent legal AI error: {e}")
            yield f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„Ùƒ: {str(e)}"
    
    async def ask_question_with_context_streaming(
        self, 
        query: str, 
        conversation_history: List[Dict[str, str]]
    ) -> AsyncIterator[str]:
        """
        Intelligent context-aware legal consultation with AI classification
        """
        try:
            logger.info(f"Processing intelligent contextual legal question: {query[:50]}...")
            logger.info(f"Conversation context: {len(conversation_history)} messages")
            
            # Stage 1: AI-powered intent classification with context
            classification = await self.classifier.classify_intent(query, conversation_history)
            category = classification["category"]
            confidence = classification["confidence"]
            
            # Stage 2: Get relevant documents
            relevant_docs = await self.retriever.get_relevant_documents(query, top_k=3)
            
            # Stage 3: Select appropriate prompt
            system_prompt = PROMPT_TEMPLATES[category]
            
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            # Stage 4: Add conversation history (last 8 messages)
            recent_history = conversation_history[-8:] if len(conversation_history) > 8 else conversation_history
            for msg in recent_history:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            # Stage 5: Add current question with legal context if available
            if relevant_docs:
                legal_context = format_legal_context_naturally(relevant_docs)
                contextual_prompt = f"""{legal_context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}"""
                logger.info(f"Using {len(relevant_docs)} relevant legal documents with {category} approach (contextual)")
            else:
                contextual_prompt = query
                logger.info(f"No relevant documents found - using {category} approach with contextual general knowledge")
            
            messages.append({
                "role": "user", 
                "content": contextual_prompt
            })
            
            # Stage 6: Stream intelligent contextual response
            async for chunk in self._stream_ai_response(messages):
                yield chunk
                
        except Exception as e:
            logger.error(f"Intelligent contextual legal AI error: {e}")
            yield f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„Ùƒ: {str(e)}"
    
    async def _stream_ai_response(self, messages: List[Dict[str, str]]) -> AsyncIterator[str]:
        """Stream AI response with error handling"""
        try:
            stream = await self.ai_client.chat.completions.create(
                model=self.ai_model,
                messages=messages,
                temperature=0.3,  # Balanced creativity and consistency
                max_tokens=1500,  # Reasonable length
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"AI streaming error: {e}")
            error_msg = str(e).lower()
            
            if "rate limit" in error_msg or "429" in error_msg:
                yield "\n\nâ³ ØªÙ… ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø³Ù…ÙˆØ­ Ù…Ø¤Ù‚ØªØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©."
            elif "api key" in error_msg or "authentication" in error_msg:
                yield "\n\nğŸ”‘ Ø®Ø·Ø£ ÙÙŠ Ù…ÙØªØ§Ø­ API. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ."
            else:
                yield f"\n\nâŒ Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ: {str(e)}"
    
    async def generate_conversation_title(self, first_message: str) -> str:
        """Intelligent conversation title generation"""
        try:
            title_prompt = f"Ø§Ù‚ØªØ±Ø­ Ø¹Ù†ÙˆØ§Ù†Ø§Ù‹ Ù…Ø®ØªØµØ±Ø§Ù‹ (Ø£Ù‚Ù„ Ù…Ù† 30 Ø­Ø±Ù) Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©: {first_message[:100]}"
            
            response = await self.ai_client.chat.completions.create(
                model=classification_model,  # Use small model for title generation
                messages=[{"role": "user", "content": title_prompt}],
                max_tokens=50,
                temperature=0.3
            )
            
            title = response.choices[0].message.content.strip()
            title = title.strip('"').strip("'").strip()
            
            # Remove common prefixes
            prefixes = ["Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:", "Ø§Ù„Ù…Ù‚ØªØ±Ø­:", "Ø¹Ù†ÙˆØ§Ù†:"]
            for prefix in prefixes:
                if title.startswith(prefix):
                    title = title[len(prefix):].strip()
            
            return title[:30] if len(title) > 30 else title
            
        except Exception as e:
            logger.error(f"Title generation error: {e}")
            return first_message[:25] + "..." if len(first_message) > 25 else first_message


# Global instance - maintains compatibility with existing code
rag_engine = IntelligentLegalRAG()

# Legacy compatibility functions - exactly the same interface as before
async def ask_question(query: str) -> str:
    """Legacy sync function - converts streaming to complete response"""
    chunks = []
    async for chunk in rag_engine.ask_question_streaming(query):
        chunks.append(chunk)
    return ''.join(chunks)

async def ask_question_with_context(query: str, conversation_history: List[Dict[str, str]]) -> str:
    """Legacy sync function with context - converts streaming to complete response"""
    chunks = []
    async for chunk in rag_engine.ask_question_with_context_streaming(query, conversation_history):
        chunks.append(chunk)
    return ''.join(chunks)

async def generate_conversation_title(first_message: str) -> str:
    """Legacy function for title generation"""
    return await rag_engine.generate_conversation_title(first_message)

# Test function
async def test_intelligent_rag():
    """Test the intelligent RAG system with classification"""
    print("ğŸ§ª Testing intelligent RAG engine with AI classification...")
    
    test_queries = [
        "Ù…Ø§ Ù‡ÙŠ Ø¹Ù‚ÙˆØ¨Ø§Øª Ø§Ù„ØªÙ‡Ø±Ø¨ Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠØŸ",  # Should be GENERAL_QUESTION
        "Ø±ÙØ¹ Ø¹Ù„ÙŠ Ø®ØµÙ… Ø¯Ø¹ÙˆÙ‰ ÙƒÙŠØ¯ÙŠØ© ÙƒÙŠÙ Ø£Ø±Ø¯ Ø¹Ù„ÙŠÙ‡ØŸ",  # Should be ACTIVE_DISPUTE
        "Ø£Ø±ÙŠØ¯ Ù…Ù‚Ø§Ø¶Ø§Ø© Ø´Ø±ÙƒØªÙŠ Ù‡Ù„ Ø§Ù„Ø£Ù…Ø± ÙŠØ³ØªØ­Ù‚ØŸ"  # Should be PLANNING_ACTION
    ]
    
    for query in test_queries:
        print(f"\nğŸ§ª Testing: {query}")
        print("Response:")
        
        response_chunks = []
        async for chunk in rag_engine.ask_question_streaming(query):
            response_chunks.append(chunk)
            print(chunk, end="", flush=True)
        
        print(f"\nâœ… Test complete for this query!\n{'-'*50}")
    
    return True

# System initialization
print("ğŸ›ï¸ Intelligent Legal RAG Engine loaded - AI-powered classification + Smart document retrieval!")

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_intelligent_rag())