"""
Legal Reasoning RAG Engine - Production Ready
Zero tech debt, clean architecture, smart legal reasoning
Built for Saudi legal AI with proper issue analysis and contextual prompting
"""
from app.legal_reasoning.document_type_analyzer import LegalDocumentTypeAnalyzer, DocumentType
from app.legal_reasoning.document_generator import LegalDocumentGenerator
import os
from datetime import datetime
from dotenv import load_dotenv
from openai import AsyncOpenAI
import markdown
from typing import List, Dict, Optional, Any, AsyncIterator
import re
import logging

# Import legal reasoning components
from app.legal_reasoning.issue_analyzer import LegalIssueAnalyzer, LegalIssue

# Import clean architecture components
from app.storage.vector_store import VectorStore, Chunk
from app.storage.sqlite_store import SqliteVectorStore

# Load environment variables
try:
    load_dotenv(".env")
except:
    pass

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# API key configuration
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Initialize AI clients - prioritize OpenAI, fallback to DeepSeek
if OPENAI_API_KEY:
    ai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    ai_model = "gpt-4o"
    print("âœ… Using OpenAI for async AI and embeddings")
elif DEEPSEEK_API_KEY:
    ai_client = AsyncOpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com/v1")
    ai_model = "deepseek-chat"
    print("âœ… Using DeepSeek for async AI and embeddings")
else:
    raise ValueError("âŒ Either OPENAI_API_KEY or DEEPSEEK_API_KEY must be provided in environment")


class StorageFactory:
    """Factory for creating storage backends based on configuration"""
    
    @staticmethod
    def create_storage() -> VectorStore:
        """Create storage backend based on environment configuration"""
        storage_type = os.getenv("VECTOR_STORAGE_TYPE", "sqlite").lower()
        
        if storage_type == "sqlite":
            db_path = os.getenv("SQLITE_DB_PATH", "data/vectors.db")
            return SqliteVectorStore(db_path)
        elif storage_type == "qdrant":
            # Future: QdrantVectorStore implementation
            raise NotImplementedError("Qdrant storage not yet implemented")
        else:
            raise ValueError(f"Unknown storage type: {storage_type}")


class DocumentRetriever:
    """
    Pure database-driven document retriever
    No hardcoded documents - everything comes from storage
    """
    
    def __init__(self, storage: VectorStore, ai_client: AsyncOpenAI):
        """
        Initialize retriever with storage backend
        
        Args:
            storage: Vector storage implementation
            ai_client: AI client for query embeddings
        """
        self.storage = storage
        self.ai_client = ai_client
        self.initialized = False
        
        logger.info(f"DocumentRetriever initialized with {type(storage).__name__}")
    
    async def initialize(self) -> None:
        """Initialize storage backend (no document loading)"""
        if self.initialized:
            return
        
        try:
            # Initialize storage backend
            await self.storage.initialize()
            
            # Check current document count
            stats = await self.storage.get_stats()
            logger.info(f"Storage initialized with {stats.total_chunks} existing documents")
            
            self.initialized = True
            
        except Exception as e:
            logger.error(f"Failed to initialize retriever: {e}")
            raise
    
    async def retrieve_relevant_chunks(self, query: str, legal_issue: LegalIssue, top_k: int = 2) -> List[Chunk]:
        """
        Retrieve relevant chunks from database with legal context
        
        Args:
            query: Search query
            legal_issue: Analyzed legal issue for contextual retrieval
            top_k: Number of results to return
            
        Returns:
            List of relevant Chunk objects from database
        """
        # Ensure initialization
        if not self.initialized:
            await self.initialize()
        
        try:
            # Check if we have any documents in storage
            stats = await self.storage.get_stats()
            if stats.total_chunks == 0:
                logger.warning("No documents found in storage")
                return []
            
            logger.info(f"Searching {stats.total_chunks} documents for: '{query[:50]}...'")
            logger.info(f"Legal context: {legal_issue.legal_domain} | {legal_issue.issue_type}")
            
            # Use hybrid search for better legal document retrieval
            if hasattr(self.storage, 'search_hybrid'):
                search_results = await self.storage.search_hybrid(query, top_k=top_k)
            else:
                # Fallback to basic similarity search
                response = await self.ai_client.embeddings.create(
                    model="text-embedding-ada-002",
                    input=query
                )
                query_embedding = response.data[0].embedding
                search_results = await self.storage.search_similar(query_embedding, top_k=top_k)
            
            # Extract chunks from search results
            relevant_chunks = [result.chunk for result in search_results]
            
            if relevant_chunks:
                logger.info(f"Found {len(relevant_chunks)} relevant documents:")
                for i, chunk in enumerate(relevant_chunks):
                    similarity = search_results[i].similarity_score
                    logger.info(f"  {i+1}. {chunk.title[:50]}... (similarity: {similarity:.3f})")
            else:
                logger.info("No relevant documents found")
            
            return relevant_chunks
            
        except Exception as e:
            logger.error(f"Failed to retrieve relevant chunks: {e}")
            return []
    
    async def get_document_count(self) -> int:
        """Get total number of documents in storage"""
        try:
            stats = await self.storage.get_stats()
            return stats.total_chunks
        except Exception as e:
            logger.error(f"Error getting document count: {e}")
            return 0


class LegalPromptBuilder:
    """Advanced legal prompt builder with issue-aware contextualization"""
    
    LEGAL_SYSTEM_PROMPT = """Ø£Ù†Øª Ù…Ø­Ø§Ù…ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ Ø®Ø¨ÙŠØ± ÙˆÙ…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªÙ…Ø±Ø³ Ù…Ø¹ 20 Ø¹Ø§Ù…Ø§Ù‹ Ù…Ù† Ø§Ù„Ø®Ø¨Ø±Ø© ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ.

ØªØ®ØµØµØ§ØªÙƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
- Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠ ÙˆØ§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¬Ø²Ø§Ø¦ÙŠØ©
- Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø¯Ù†ÙŠ ÙˆØ§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª Ø§Ù„Ø´Ø±Ø¹ÙŠØ©
- Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¹Ù…Ù„ ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø¹Ù…Ø§Ù„ÙŠØ©
- Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ØªØ¬Ø§Ø±ÙŠ ÙˆØ§Ù„Ø´Ø±ÙƒØ§Øª
- Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ
- Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ©

Ù…Ù†Ù‡Ø¬ÙŠØ© Ø¹Ù…Ù„Ùƒ:
- ØªÙ‚Ø¯ÙŠÙ… Ù…Ø´ÙˆØ±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¹Ù…Ù„ÙŠØ© ÙˆÙ‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚
- Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù„ÙˆÙ„ ÙˆØ§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ÙÙˆØ±ÙŠØ©
- Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„ØºØ© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ø¹Ù…Ù„Ø§Ø¡
- Ø§Ù„Ø§Ø³ØªÙ†Ø§Ø¯ Ù„Ù„Ø£Ù†Ø¸Ù…Ø© ÙˆØ§Ù„Ù„ÙˆØ§Ø¦Ø­ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©
- ØªÙ‚Ø¯ÙŠÙ… Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© ÙˆØ¹Ù…Ù„ÙŠØ©"""
    
    @classmethod
    def get_system_prompt(cls) -> str:
        """Get legal system prompt"""
        return cls.LEGAL_SYSTEM_PROMPT
    
    @classmethod
    def build_legal_prompt(cls, query: str, retrieved_chunks: List[Chunk], legal_issue: LegalIssue) -> str:
        """Build contextual legal prompt based on issue analysis"""
        
        # Determine prompt strategy based on legal issue
        if legal_issue.user_position == 'defendant' and legal_issue.advice_type == 'defense_strategy':
            return cls._build_defense_strategy_prompt(query, retrieved_chunks, legal_issue)
        
        elif legal_issue.advice_type == 'procedural_guide':
            return cls._build_procedural_guide_prompt(query, retrieved_chunks, legal_issue)
        
        elif legal_issue.advice_type == 'rights_explanation':
            return cls._build_rights_explanation_prompt(query, retrieved_chunks, legal_issue)
        
        elif legal_issue.user_position == 'plaintiff':
            return cls._build_action_strategy_prompt(query, retrieved_chunks, legal_issue)
        
        else:
            return cls._build_general_advice_prompt(query, retrieved_chunks, legal_issue)
    
    @classmethod
    def _build_defense_strategy_prompt(cls, query: str, retrieved_chunks: List[Chunk], legal_issue: LegalIssue) -> str:
        """Build defense strategy prompt for defendants"""
        
        legal_context = cls._format_legal_context(retrieved_chunks)
        
        return f"""Ø£Ù†Øª Ù…Ø­Ø§Ù…ÙŠ Ø¯ÙØ§Ø¹ Ø³Ø¹ÙˆØ¯ÙŠ Ø®Ø¨ÙŠØ±. Ù…ÙˆÙƒÙ„Ùƒ ÙŠÙˆØ§Ø¬Ù‡ Ù‚Ø¶ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ {legal_issue.legal_domain} ÙˆÙŠØ­ØªØ§Ø¬ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¯ÙØ§Ø¹ Ù‚ÙˆÙŠØ© ÙˆØ¹Ù…Ù„ÙŠØ©.

ğŸ“š **Ø§Ù„Ø£Ù†Ø¸Ù…Ø© ÙˆØ§Ù„Ù„ÙˆØ§Ø¦Ø­ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©:**
{legal_context}

âš–ï¸ **Ù…ÙˆÙ‚Ù Ø§Ù„Ø¯ÙØ§Ø¹:**
{query}

**Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ ÙƒÙ…Ø­Ø§Ù…ÙŠ Ø¯ÙØ§Ø¹ Ù…ØªÙ…Ø±Ø³:**

ğŸ¯ **Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¯ÙØ§Ø¹ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:**
- Ø­Ø¯Ø¯ Ø£Ù‚ÙˆÙ‰ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¯ÙØ§Ø¹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ù…Ø±ÙÙ‚Ø©
- Ø§Ù‚ØªØ±Ø­ Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø¯ÙØ§Ø¹ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± ÙØ¹Ø§Ù„ÙŠØ© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø¶ÙŠØ©
- Ø±ØªØ¨ Ø§Ù„Ø¯ÙÙˆØ¹ Ø­Ø³Ø¨ Ù‚ÙˆØ© Ø§Ù„ØªØ£Ø«ÙŠØ± ÙˆØ§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©

ğŸ›¡ï¸ **Ø§Ù„Ø¯ÙÙˆØ¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©:**
- Ø§Ø°ÙƒØ± Ø§Ù„Ø¯ÙÙˆØ¹ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø© ØªÙØµÙŠÙ„ÙŠØ§Ù‹
- Ø±Ø¨Ø· ÙƒÙ„ Ø¯ÙØ¹ Ø¨Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
- Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ØªØ·Ø¨ÙŠÙ‚ ÙƒÙ„ Ø¯ÙØ¹ Ø¹Ù…Ù„ÙŠØ§Ù‹

ğŸ“‹ **Ø®Ø·Ø© Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„ÙÙˆØ±ÙŠØ©:**
- Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ÙˆØ§Ø¬Ø¨ Ø§ØªØ®Ø§Ø°Ù‡Ø§ ÙÙˆØ±Ø§Ù‹ (Ø®Ù„Ø§Ù„ 24-48 Ø³Ø§Ø¹Ø©)
- Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø¬Ù…Ø¹Ù‡Ø§ Ø¨Ø§Ù„ØªÙØµÙŠÙ„
- Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ÙˆØ§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

ğŸ’¡ **Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©:**
- Ù†ØµØ§Ø¦Ø­ ØªÙƒØªÙŠÙƒÙŠØ© Ù„ØªÙ‚ÙˆÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ
- Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
- Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø­Ø§Ù„Ø© ÙØ´Ù„ Ø§Ù„Ø¯ÙØ¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ

ØªØ­Ø¯Ø« ÙƒÙ…Ø­Ø§Ù…ÙŠ Ø¯ÙØ§Ø¹ Ù…Ø­ØªØ±Ù ÙŠØ¹Ø·ÙŠ Ù†ØµØ§Ø¦Ø­ Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙ‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙˆØ±Ø§Ù‹."""

    @classmethod
    def _build_procedural_guide_prompt(cls, query: str, retrieved_chunks: List[Chunk], legal_issue: LegalIssue) -> str:
        """Build procedural guide prompt"""
        
        legal_context = cls._format_legal_context(retrieved_chunks)
        
        return f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¥Ø¬Ø±Ø§Ø¦ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ {legal_issue.legal_domain}. 

ğŸ“š **Ø§Ù„Ø£Ù†Ø¸Ù…Ø© ÙˆØ§Ù„Ù„ÙˆØ§Ø¦Ø­ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©:**
{legal_context}

â“ **Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠ:**
{query}

**Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ ÙƒØ®Ø¨ÙŠØ± Ø¥Ø¬Ø±Ø§Ø¦ÙŠ:**

ğŸ“‹ **Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ:**
- Ø§Ø´Ø±Ø­ ÙƒÙ„ Ø®Ø·ÙˆØ© Ù…Ø·Ù„ÙˆØ¨Ø© Ø¨Ø§Ù„ØªÙØµÙŠÙ„ ÙˆØ§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØµØ­ÙŠØ­
- Ø­Ø¯Ø¯ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ ÙˆØ§Ù„Ù…Ù‡Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ø¯Ù‚Ø©
- Ø§Ø°ÙƒØ± Ø§Ù„Ø±Ø³ÙˆÙ… ÙˆØ§Ù„ØªÙƒØ§Ù„ÙŠÙ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¥Ù† ÙˆØ¬Ø¯Øª
- ÙˆØ¶Ø­ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø© ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø·ÙˆØ§Ø±Ø¦

ğŸ“„ **Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©:**
- Ø­Ø¯Ø¯ ÙƒÙ„ Ù…Ø³ØªÙ†Ø¯ Ù…Ø·Ù„ÙˆØ¨ Ø¨Ø¯Ù‚Ø© Ù…Ø¹ Ø§Ù„ÙˆØµÙ
- Ø§Ø´Ø±Ø­ ÙƒÙŠÙÙŠØ© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒÙ„ Ù…Ø³ØªÙ†Ø¯
- Ø§Ø°ÙƒØ± Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØ§Ù„Ø´Ø±ÙˆØ· Ù„ÙƒÙ„ Ù…Ø³ØªÙ†Ø¯
- Ø­Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠØ©

âš ï¸ **Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ© Ø§Ù„Ø­Ø±Ø¬Ø©:**
- Ø§Ù†Ø¨Ù‡ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ© Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªØ¤Ø¯ÙŠ Ù„Ø±ÙØ¶ Ø§Ù„Ø·Ù„Ø¨
- Ø§Ø°ÙƒØ± Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙˆÙƒÙŠÙÙŠØ© ØªØ¬Ù†Ø¨Ù‡Ø§
- Ø­Ø¯Ø¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…

ğŸ• **Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ ÙˆØ§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯:**
- Ø¶Ø¹ Ø¬Ø¯ÙˆÙ„Ø§Ù‹ Ø²Ù…Ù†ÙŠØ§Ù‹ ÙˆØ§Ø¶Ø­Ø§Ù‹ Ù„ÙƒÙ„ Ø¥Ø¬Ø±Ø§Ø¡
- Ø­Ø¯Ø¯ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø­Ø±Ø¬Ø© Ø§Ù„ØªÙŠ Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ£Ø¬ÙŠÙ„Ù‡Ø§
- Ø§Ù‚ØªØ±Ø­ Ù‡Ø§Ù…Ø´ Ø£Ù…Ø§Ù† Ø²Ù…Ù†ÙŠ Ù„ÙƒÙ„ Ø®Ø·ÙˆØ©

Ù‚Ø¯Ù… Ø¯Ù„ÙŠÙ„Ø§Ù‹ Ø¹Ù…Ù„ÙŠØ§Ù‹ Ø´Ø§Ù…Ù„Ø§Ù‹ ÙŠÙ…ÙƒÙ† Ø§ØªØ¨Ø§Ø¹Ù‡ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© Ø¨Ø¯ÙˆÙ† Ø£Ø®Ø·Ø§Ø¡."""

    @classmethod
    def _build_rights_explanation_prompt(cls, query: str, retrieved_chunks: List[Chunk], legal_issue: LegalIssue) -> str:
        """Build rights explanation prompt"""
        
        legal_context = cls._format_legal_context(retrieved_chunks)
        
        return f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø´Ø±Ø­ Ø§Ù„Ø­Ù‚ÙˆÙ‚ ÙˆØ§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª ÙÙŠ {legal_issue.legal_domain}.

ğŸ“š **Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**
{legal_context}

â“ **Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ø§Ù„Ø­Ù‚ÙˆÙ‚:**
{query}

**Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ ÙƒØ®Ø¨ÙŠØ± Ø­Ù‚ÙˆÙ‚ÙŠ:**

âš–ï¸ **Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:**
- Ø§Ø´Ø±Ø­ ÙƒÙ„ Ø­Ù‚ Ø¨ÙˆØ¶ÙˆØ­ Ù…Ø¹ Ø§Ù„Ø§Ø³ØªÙ†Ø§Ø¯ Ù„Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- Ø­Ø¯Ø¯ Ù†Ø·Ø§Ù‚ ÙƒÙ„ Ø­Ù‚ ÙˆØ­Ø¯ÙˆØ¯Ù‡ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
- ÙˆØ¶Ø­ ÙƒÙŠÙÙŠØ© Ù…Ù…Ø§Ø±Ø³Ø© ÙƒÙ„ Ø­Ù‚ Ø¹Ù…Ù„ÙŠØ§Ù‹
- Ø§Ø°ÙƒØ± Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ø·Ù„Ù‚Ø© ÙˆØ§Ù„Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ·Ø©

ğŸ“œ **Ø§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©:**
- Ø­Ø¯Ø¯ Ø§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªÙ‚Ø§Ø¨Ù„ ÙƒÙ„ Ø­Ù‚
- Ø§Ø´Ø±Ø­ Ø¹ÙˆØ§Ù‚Ø¨ Ø¹Ø¯Ù… Ø§Ù„ÙˆÙØ§Ø¡ Ø¨Ø§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª
- ÙˆØ¶Ø­ Ø§Ù„ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø­Ù‚ÙˆÙ‚ ÙˆØ§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª

ğŸ›¡ï¸ **Ø¢Ù„ÙŠØ§Øª Ø§Ù„Ø­Ù…Ø§ÙŠØ© ÙˆØ§Ù„Ø¥Ù†ÙØ§Ø°:**
- ÙƒÙŠÙÙŠØ© Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ø¨Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ§Ù‹
- Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ© Ø¨Ø­Ù…Ø§ÙŠØ© ÙƒÙ„ Ø­Ù‚
- Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù†ØªÙ‡Ø§Ùƒ Ø§Ù„Ø­Ù‚ÙˆÙ‚
- Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø© Ù„Ø­Ù„ Ø§Ù„Ù†Ø²Ø§Ø¹Ø§Øª

ğŸ’¡ **Ø§Ù„Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©:**
- ÙƒÙŠÙÙŠØ© ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙˆÙ‚ ÙˆØ­Ù…Ø§ÙŠØªÙ‡Ø§
- Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª Ù…Ù† Ø§Ù„ØªÙ†Ø§Ø²Ù„ ØºÙŠØ± Ø§Ù„Ù…Ù‚ØµÙˆØ¯ Ø¹Ù† Ø§Ù„Ø­Ù‚ÙˆÙ‚
- Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù‚ÙˆÙ‚ ÙƒØ§Ù…Ù„Ø©

Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø© Ù…Ø¹ Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ© Ù…Ù† Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ."""

    @classmethod
    def _build_action_strategy_prompt(cls, query: str, retrieved_chunks: List[Chunk], legal_issue: LegalIssue) -> str:
        """Build action strategy prompt for plaintiffs"""
        
        legal_context = cls._format_legal_context(retrieved_chunks)
        
        return f"""Ø£Ù†Øª Ù…Ø­Ø§Ù…ÙŠ ÙˆÙ…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„ØªÙ‚Ø§Ø¶ÙŠ ÙˆØ§Ù„Ù…Ø·Ø§Ù„Ø¨Ø§Øª ÙÙŠ {legal_issue.legal_domain}.

ğŸ“š **Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:**
{legal_context}

âš–ï¸ **Ù…ÙˆÙ‚Ù Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©:**
{query}

**Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ ÙƒÙ…Ø­Ø§Ù…ÙŠ ØªÙ‚Ø§Ø¶ÙŠ Ù…ØªÙ…Ø±Ø³:**

ğŸ¯ **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©:**
- Ø­Ø¯Ø¯ Ø£Ù‚ÙˆÙ‰ Ø§Ù„Ø£Ø³Ø³ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù„Ù„Ù…Ø·Ø§Ù„Ø¨Ø©
- Ø§Ù‚ØªØ±Ø­ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± ÙØ¹Ø§Ù„ÙŠØ© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
- Ø±ØªØ¨ Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø­Ø³Ø¨ Ù‚ÙˆØ© Ø§Ù„ØªØ£Ø«ÙŠØ±

ğŸ“‹ **Ø®Ø·Ø© Ø§Ù„ØªÙ‚Ø§Ø¶ÙŠ:**
- Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ø±ÙØ¹ Ø§Ù„Ø¯Ø¹ÙˆÙ‰ Ø£Ùˆ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©
- Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„ÙˆØ§Ø¬Ø¨ Ø¬Ù…Ø¹Ù‡Ø§
- Ø£ÙØ¶Ù„ ØªÙˆÙ‚ÙŠØª Ù„Ø§ØªØ®Ø§Ø° Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©

ğŸ’ª **ØªÙ‚ÙˆÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:**
- ÙƒÙŠÙÙŠØ© ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø£Ø¯Ù„Ø© ÙˆØ§Ù„Ø­Ø¬Ø¬
- Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·Ø§Øª Ø§Ù„ÙˆØ§Ø¬Ø¨ Ø§ØªØ®Ø§Ø°Ù‡Ø§ Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø­Ù‚ÙˆÙ‚
- Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªÙØ§ÙˆØ¶ Ù‚Ø¨Ù„ Ø§Ù„ØªÙ‚Ø§Ø¶ÙŠ

âš ï¸ **ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø±:**
- Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„Ù†Ø¬Ø§Ø­ ÙˆØ¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ£Ø«ÙŠØ±
- Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØ§Ù„Ø¹Ø§Ø¦Ø¯ Ø§Ù„Ù…Ø­ØªÙ…Ù„
- Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… Ù†Ø¬Ø§Ø­ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

Ù‚Ø¯Ù… Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø´Ø§Ù…Ù„Ø© ÙˆØ¹Ù…Ù„ÙŠØ© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬."""

    @classmethod
    def _build_general_advice_prompt(cls, query: str, retrieved_chunks: List[Chunk], legal_issue: LegalIssue) -> str:
        """Build general legal advice prompt"""
        
        legal_context = cls._format_legal_context(retrieved_chunks)
        
        if not legal_context:
            return f"""Ù‚Ø¯Ù… Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø³Ø¹ÙˆØ¯ÙŠØ© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ:

{query}

**Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø©:**
- Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙˆØ§Ø¶Ø­Ø© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©
- ØªÙˆØ¶ÙŠØ­ Ø¹Ù…Ù„ÙŠ Ù„Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ
- Ù†ØµØ§Ø¦Ø­ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© ÙˆÙ‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚
- ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¥Ù† Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±"""
        
        return f"""Ù‚Ø¯Ù… Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø³Ø¹ÙˆØ¯ÙŠØ© Ù…ØªØ®ØµØµØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©:

ğŸ“š **Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø±Ø³Ù…ÙŠØ©:**
{legal_context}

â“ **Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:**
{query}

**Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ ÙƒÙ…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ:**

ğŸ” **Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:**
- ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ¶Ø¹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…Ø±ÙÙ‚Ø©
- ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
- ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ù‚ÙˆÙ‚ ÙˆØ§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª

ğŸ’¡ **Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠ:**
- Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙˆØ§Ø¬Ø¨ Ø§ØªØ®Ø§Ø°Ù‡Ø§
- Ø§Ù„Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
- Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª ÙˆØ§Ù„Ø§Ø­ØªÙŠØ§Ø·Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©

ğŸ“‹ **Ø§Ù„ØªÙˆØµÙŠØ§Øª:**
- Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©
- Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ Ø¹Ù‚Ø¨Ø§Øª
- Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ ÙˆØ§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„ÙŠÙ‡Ø§

Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ù…Ù‡Ù†ÙŠØ© ÙˆØ§Ø¶Ø­Ø© Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©."""

    @classmethod
    def _format_legal_context(cls, retrieved_chunks: List[Chunk]) -> str:
        """Format retrieved legal documents for context"""
        if not retrieved_chunks:
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ ÙˆØ«Ø§Ø¦Ù‚ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© Ù…ØªØ§Ø­Ø© - Ø³ÙŠØªÙ… Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø©."
        
        formatted_context = []
        for i, chunk in enumerate(retrieved_chunks, 1):
            formatted_context.append(f"ğŸ“„ **Ø§Ù„Ù…Ø±Ø¬Ø¹ {i}: {chunk.title}**\n{chunk.content}")
        
        return "\n\n".join(formatted_context)


class LegalReasoningRAGEngine:
    """
    Advanced Legal Reasoning RAG Engine
    Combines document retrieval with intelligent legal issue analysis
    """
    
    def __init__(self):
        """Initialize Legal RAG engine with reasoning capabilities"""
        self.ai_client = ai_client
        self.ai_model = ai_model
        
        # Create storage backend via factory
        self.storage = StorageFactory.create_storage()
        
        # Create components
        self.retriever = DocumentRetriever(
            storage=self.storage,
            ai_client=self.ai_client
        )
        
        self.issue_analyzer = LegalIssueAnalyzer()
        self.document_type_analyzer = LegalDocumentTypeAnalyzer()
        self.document_generator = LegalDocumentGenerator()
        self.prompt_builder = LegalPromptBuilder()
        
        logger.info(f"LegalReasoningRAGEngine initialized with {type(self.storage).__name__} storage")
    
    async def ask_question_streaming(self, query: str) -> AsyncIterator[str]:
        """
        Stream legal consultation with intelligent reasoning
        
        Args:
            query: User's legal question
            
        Yields:
            Streaming response chunks
        """
        try:
            logger.info(f"Processing legal question: {query[:50]}...")
            
            # Stage 1: Analyze legal issue
            legal_issue = await self.issue_analyzer.analyze_issue(query)
            logger.info(f"Legal analysis: {legal_issue.issue_type} | {legal_issue.legal_domain} | {legal_issue.user_position}")
            
            # Stage 2: Retrieve relevant legal documents
            # Stage 2.5: Analyze document type
            document_type = self.document_type_analyzer.analyze_document_type(query)
            logger.info(f"Contextual document type: {document_type.specific_type} | Category: {document_type.document_category}")
            relevant_chunks = await self.retriever.retrieve_relevant_chunks(
                query=query, 
                legal_issue=legal_issue,
                top_k=2
            )
            
            # Stage 3: Build contextual legal prompt
            # Stage 2.5: Analyze document type
            document_type = self.document_type_analyzer.analyze_document_type(query)
            logger.info(f"Document type: {document_type.specific_type} | Category: {document_type.document_category}")
            legal_prompt = self.document_generator.generate_document_prompt(query, relevant_chunks, legal_issue, document_type)
            
            if relevant_chunks:
                logger.info(f"Using legal reasoning with {len(relevant_chunks)} relevant documents")
            else:
                logger.info("Using general legal knowledge (no specific documents found)")
            
            # Stage 4: Generate legal advice
            messages = [
                {"role": "system", "content": self.prompt_builder.get_system_prompt()},
                {"role": "user", "content": legal_prompt}
            ]
            
            # Stream legal advice
            yield "âš–ï¸ **Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**\n\n"
            
            async for chunk in self._stream_legal_response(messages):
                yield chunk
                
        except Exception as e:
            logger.error(f"Legal reasoning error: {e}")
            yield f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©: {str(e)}"
    
    async def ask_question_with_context_streaming(
        self, 
        query: str, 
        conversation_history: List[Dict[str, str]]
    ) -> AsyncIterator[str]:
        """
        Stream legal consultation with conversation context
        
        Args:
            query: User's legal question
            conversation_history: Previous conversation messages
            
        Yields:
            Streaming response chunks
        """
        try:
            logger.info(f"Processing contextual legal question: {query[:50]}...")
            logger.info(f"Conversation context: {len(conversation_history)} messages")
            
            # Stage 1: Analyze legal issue
            legal_issue = await self.issue_analyzer.analyze_issue(query)
            logger.info(f"Legal analysis: {legal_issue.issue_type} | {legal_issue.legal_domain} | {legal_issue.user_position}")
            
            # Stage 2: Retrieve relevant legal documents
            # Stage 2.5: Analyze document type
            document_type = self.document_type_analyzer.analyze_document_type(query)
            logger.info(f"Contextual document type: {document_type.specific_type} | Category: {document_type.document_category}")
            relevant_chunks = await self.retriever.retrieve_relevant_chunks(
                query=query, 
                legal_issue=legal_issue,
                top_k=2
            )
            
            # Stage 3: Build contextual messages
            messages = [
                {"role": "system", "content": self.prompt_builder.get_system_prompt()}
            ]
            
            # Add conversation history (limit to last 8 messages)
            recent_history = conversation_history[-8:] if len(conversation_history) > 8 else conversation_history
            for msg in recent_history:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            # Stage 4: Add current query with legal context
            contextual_prompt = f"Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©ØŒ {self.document_generator.generate_document_prompt(query, relevant_chunks, legal_issue, document_type)}"
            
            messages.append({
                "role": "user", 
                "content": contextual_prompt
            })
            
            if relevant_chunks:
                logger.info(f"Using contextual legal reasoning with {len(relevant_chunks)} documents")
            else:
                logger.info("Using contextual general legal knowledge")
            
            # Stream legal advice
            yield "âš–ï¸ **Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**\n\n"
            
            async for chunk in self._stream_legal_response(messages):
                yield chunk
                
        except Exception as e:
            logger.error(f"Contextual legal reasoning error: {e}")
            yield f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©: {str(e)}"
    
    async def _stream_legal_response(self, messages: List[Dict[str, str]]) -> AsyncIterator[str]:
        """Stream legal response from AI"""
        try:
            stream = await self.ai_client.chat.completions.create(
                model=self.ai_model,
                messages=messages,
                temperature=0.15,  # Low temperature for consistent legal advice
                max_tokens=6000,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"AI streaming error: {e}")
            yield f"\n\nØ¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {str(e)}"
    
    async def generate_conversation_title(self, first_message: str) -> str:
        """Generate conversation title from first message"""
        try:
            prompt = f"Ø§Ù‚ØªØ±Ø­ Ø¹Ù†ÙˆØ§Ù†Ø§Ù‹ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ§Ù‹ Ù…Ø®ØªØµØ±Ø§Ù‹ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© (Ø£Ù‚Ù„ Ù…Ù† 40 Ø­Ø±Ù): {first_message[:150]}"
            
            response = await self.ai_client.chat.completions.create(
                model=self.ai_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=50,
                temperature=0.3
            )
            
            title = response.choices[0].message.content.strip()
            title = title.strip('"').strip("'").strip()
            
            # Remove common prefixes
            prefixes = ["Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:", "Ø§Ù„Ù…Ù‚ØªØ±Ø­:", "Ø¹Ù†ÙˆØ§Ù†:", "Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø©:"]
            for prefix in prefixes:
                if title.startswith(prefix):
                    title = title[len(prefix):].strip()
            
            return title if len(title) <= 40 else title[:37] + "..."
            
        except Exception as e:
            logger.error(f"Error generating title: {e}")
            return first_message[:25] + "..." if len(first_message) > 25 else first_message
    
    async def get_system_stats(self) -> Dict[str, Any]:
        """Get comprehensive system statistics"""
        try:
            stats = await self.storage.get_stats()
            health_status = await self.storage.health_check()
            
            return {
                "total_documents": stats.total_chunks,
                "storage_size_mb": stats.storage_size_mb,
                "last_updated": stats.last_updated.isoformat(),
                "health": "healthy" if health_status else "unhealthy",
                "storage_type": type(self.storage).__name__,
                "ai_model": self.ai_model,
                "legal_reasoning": "enabled",
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Error getting system stats: {e}")
            return {"error": str(e)}


# Global legal reasoning RAG engine instance
rag_engine = LegalReasoningRAGEngine()

# Legacy sync functions for backward compatibility
async def ask_question(query: str) -> str:
    """Legacy sync function - converts streaming to complete response"""
    chunks = []
    async for chunk in rag_engine.ask_question_streaming(query):
        chunks.append(chunk)
    return ''.join(chunks)

async def ask_question_with_context(query: str, conversation_history: List[Dict[str, str]]) -> str:
    """Legacy sync function with context - converts streaming to complete response"""
    chunks = []
    async for chunk in rag_engine.ask_question_with_context_streaming(query, conversation_history):
        chunks.append(chunk)
    return ''.join(chunks)

async def generate_conversation_title(first_message: str) -> str:
    """Legacy function for title generation"""
    return await rag_engine.generate_conversation_title(first_message)

# System initialization message
print("ğŸ›ï¸ Legal Reasoning RAG Engine loaded - Production ready with zero tech debt!")