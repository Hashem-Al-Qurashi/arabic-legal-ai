import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from smart_legal_chunker import SmartLegalChunker

async def test_real_massive():
    pw = await async_playwright().start()
    browser = await pw.chromium.launch(headless=True)
    page = await browser.new_page()
    
    # Test with the real massive document we found
    url = "https://laws.moj.gov.sa/ar/legislation/3PpcH7Pox63dH0Jw82s-UA"
    title = "Ø§Ù„Ù„ÙˆØ§Ø¦Ø­ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª Ø§Ù„Ø´Ø±Ø¹ÙŠØ©"
    
    print(f"ğŸ” Testing massive document: {title}")
    print(f"ğŸŒ URL: {url}")
    
    await page.goto(url, wait_until="networkidle")
    await page.wait_for_timeout(5000)
    
    html = await page.content()
    soup = BeautifulSoup(html, 'html.parser')
    
    # Remove unwanted elements
    for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
        element.decompose()
    
    # Get main content
    main = soup.select_one('main')
    content = main.get_text(strip=True)
    
    print(f"ğŸ“ Content: {len(content):,} characters")
    print(f"ğŸ”¢ Estimated tokens: {len(content) // 2:,}")
    print(f"ğŸ“– Preview: {content[:200]}...")
    print()
    
    # Test the smart chunker
    print("ğŸ§ª Testing Smart Legal Chunker...")
    chunker = SmartLegalChunker(max_tokens_per_chunk=6000)
    chunks = chunker.chunk_legal_document(content, title)
    
    print(f"âœ‚ï¸ Created {len(chunks)} chunks:")
    print()
    
    total_chars = 0
    for i, chunk in enumerate(chunks):
        total_chars += len(chunk.content)
        tokens = chunker.estimate_tokens(chunk.content)
        status = "âœ…" if tokens <= 6000 else "âŒ"
        
        print(f"{status} Chunk {i+1}: {chunk.title}")
        print(f"   ğŸ“ {len(chunk.content):,} chars (~{tokens:,} tokens)")
        print(f"   ğŸ“ Level: {chunk.hierarchy_level}")
        print(f"   ğŸ”— Metadata: {chunk.metadata}")
        print(f"   ğŸ“– Preview: {chunk.content[:100].replace(chr(10), ' ')}...")
        print()
    
    # Summary
    oversized = [c for c in chunks if chunker.estimate_tokens(c.content) > 6000]
    print("ğŸ“Š SUMMARY:")
    print(f"   ğŸ“„ Original: {len(content):,} chars (~{len(content)//2:,} tokens)")
    print(f"   âœ‚ï¸ Chunks: {len(chunks)}")
    print(f"   ğŸ“ Total chunked: {total_chars:,} chars")
    print(f"   âœ… Under limit: {len(chunks) - len(oversized)}")
    print(f"   âŒ Still too big: {len(oversized)}")
    
    if len(oversized) == 0:
        print(f"ğŸ‰ SUCCESS! All chunks are under 6000 token limit!")
        print(f"ğŸš€ Ready to store in database!")
    else:
        print(f"âš ï¸ Need to improve chunking for {len(oversized)} chunks")
    
    await browser.close()
    await pw.stop()

asyncio.run(test_real_massive())
