"""
ENSEMBLE LEGAL AI SYSTEM - Full Implementation
Combines 4 AI models + 3 judges for superior legal responses

Architecture:
1. Query Processing & Context Retrieval (existing RAG)
2. Multi-Model Response Generation (4 models in parallel)  
3. Component Extraction (3 judge models)
4. Consensus Building (voting system)
5. Response Assembly (coherent final response)
6. Quality Verification (automated checks)
7. Data Collection (training data pipeline)
"""

import asyncio
import json
import logging
import time
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any, AsyncIterator
from dataclasses import dataclass
from enum import Enum

from openai import AsyncOpenAI
import httpx
import os
from dotenv import load_dotenv

# Import existing RAG system
from rag_engine import get_rag_engine
from app.storage.vector_store import Chunk

load_dotenv()

logger = logging.getLogger(__name__)

# ==================== CONFIGURATION ====================

@dataclass
class EnsembleConfig:
    """Ensemble system configuration"""
    # API Keys
    openai_api_key: str = os.getenv("OPENAI_API_KEY")
    deepseek_api_key: str = os.getenv("DEEPSEEK_API_KEY")  
    grok_api_key: str = os.getenv("GROK_API_KEY")
    gemini_api_key: str = os.getenv("GEMINI_API_KEY")
    claude_api_key: str = os.getenv("ANTHROPIC_API_KEY")
    
    # Model settings
    temperature: float = 0.7
    max_tokens: int = 2000
    context_chunks: int = 20  # 15-25 chunks as specified
    
    # Cost tracking
    track_costs: bool = True
    max_cost_per_query: float = 0.50  # Safety limit
    
    # Data collection
    save_training_data: bool = True
    training_data_path: str = "data/ensemble_training_data.jsonl"

config = EnsembleConfig()

class ComponentType(Enum):
    """7 components to extract from responses"""
    DIRECT_ANSWER = "direct_answer"
    LEGAL_FOUNDATION = "legal_foundation"  
    ARTICLE_CITATIONS = "article_citations"
    NUMERICAL_EXAMPLES = "numerical_examples"
    STEP_PROCEDURES = "step_procedures"
    EDGE_CASES = "edge_cases"
    PRACTICAL_ADVICE = "practical_advice"

# ==================== MODEL CLIENTS ====================

class ModelClients:
    """Initialize all 4 generator models + 3 judge models"""
    
    def __init__(self):
        # Generator models (4)
        self.chatgpt = AsyncOpenAI(
            api_key=config.openai_api_key,
            http_client=httpx.AsyncClient()
        ) if config.openai_api_key else None
        
        self.deepseek = AsyncOpenAI(
            api_key=config.deepseek_api_key,
            base_url="https://api.deepseek.com/v1",
            http_client=httpx.AsyncClient()
        ) if config.deepseek_api_key else None
        
        # Note: Using OpenAI client format for all - adjust base_url for each provider
        self.grok = AsyncOpenAI(
            api_key=config.grok_api_key,
            base_url="https://api.x.ai/v1",  # Grok API endpoint
            http_client=httpx.AsyncClient()
        ) if config.grok_api_key else None
        
        # Note: Gemini requires different client setup - disabled for now
        # Will add proper Gemini integration in future iteration
        self.gemini = None  # Temporarily disabled - needs proper Google AI SDK
        
        # Judge models (3)
        self.judge_claude = AsyncOpenAI(
            api_key=config.claude_api_key,
            base_url="https://api.anthropic.com/v1",
            http_client=httpx.AsyncClient()
        ) if config.claude_api_key else None
        
        self.judge_gpt = self.chatgpt  # Same as generator ChatGPT
        self.judge_gemini = None  # Disabled until proper Gemini integration
        
        logger.info(f"ü§ñ Model clients initialized:")
        logger.info(f"  Generators: ChatGPT={bool(self.chatgpt)}, DeepSeek={bool(self.deepseek)}")
        logger.info(f"              Grok={bool(self.grok)}, Gemini={bool(self.gemini)}")
        logger.info(f"  Judges: Claude={bool(self.judge_claude)}, GPT={bool(self.judge_gpt)}, Gemini={bool(self.judge_gemini)}")
    
    def get_available_generators(self) -> Dict[str, AsyncOpenAI]:
        """Get available generator models"""
        generators = {}
        if self.chatgpt:
            generators["chatgpt"] = self.chatgpt
        if self.deepseek:
            generators["deepseek"] = self.deepseek
        if self.grok:
            generators["grok"] = self.grok
        if self.gemini:
            generators["gemini"] = self.gemini
        return generators
    
    def get_available_judges(self) -> Dict[str, AsyncOpenAI]:
        """Get available judge models"""
        judges = {}
        if self.judge_claude:
            judges["claude"] = self.judge_claude
        if self.judge_gpt:
            judges["gpt4o"] = self.judge_gpt
        if self.judge_gemini:
            judges["gemini"] = self.judge_gemini
        return judges

# ==================== LAYER 1: CONTEXT RETRIEVAL ====================

class ContextRetriever:
    """Use existing RAG system to get 15-25 legal chunks"""
    
    def __init__(self):
        self.rag_engine = get_rag_engine()
        logger.info("üìö Context retriever initialized with existing RAG system")
    
    async def get_legal_context(self, query: str) -> Dict[str, Any]:
        """
        Get legal context using existing RAG system
        
        Returns:
            Dict with query, context chunks, and intent classification
        """
        try:
            # Get intent classification (from existing system)
            classification = await self.rag_engine.classifier.classify_intent(query)
            intent = classification.get("category", "GENERAL_QUESTION")
            
            # Get relevant legal chunks (15-25 as specified)
            chunks = await self.rag_engine.retriever.get_relevant_documents(
                query=query,
                top_k=config.context_chunks,
                user_intent=intent
            )
            
            # Format context for models
            formatted_context = self._format_context_for_models(chunks)
            
            logger.info(f"üìÑ Retrieved {len(chunks)} legal chunks for intent: {intent}")
            
            return {
                "query": query,
                "intent": intent,
                "chunks": chunks,
                "formatted_context": formatted_context,
                "chunk_count": len(chunks)
            }
            
        except Exception as e:
            logger.error(f"Context retrieval failed: {e}")
            return {
                "query": query,
                "intent": "GENERAL_QUESTION", 
                "chunks": [],
                "formatted_context": "",
                "chunk_count": 0
            }
    
    def _format_context_for_models(self, chunks: List[Chunk]) -> str:
        """Format legal chunks for model consumption"""
        if not chunks:
            return ""
        
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            context_parts.append(f"""
üìÑ ŸÖÿ±ÿ¨ÿπ ŸÇÿßŸÜŸàŸÜŸä {i}: {chunk.title}
{chunk.content}
""")
        
        return "üìö ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ© ÿßŸÑŸÖÿ™ÿßÿ≠ÿ©:\n" + "\n".join(context_parts)

# ==================== LAYER 2: MULTI-MODEL GENERATION ====================

class MultiModelGenerator:
    """Generate responses from 4 different models in parallel"""
    
    def __init__(self, clients: ModelClients):
        self.clients = clients
        self.generators = clients.get_available_generators()
        
        # Model-specific configurations
        self.model_configs = {
            "chatgpt": {"model": "gpt-4o", "description": "Best overall reasoning"},
            "deepseek": {"model": "deepseek-chat", "description": "Cost-effective, strong on structured tasks"},
            "grok": {"model": "grok-2", "description": "Alternative perspective"},  
            "gemini": {"model": "gemini-1.5-pro", "description": "Comprehensive responses"}
        }
        
        logger.info(f"üé≠ Multi-model generator initialized with {len(self.generators)} models")
    
    async def generate_parallel_responses(self, context_data: Dict[str, Any]) -> Dict[str, str]:
        """
        Send IDENTICAL input to all 4 models in parallel
        
        Critical requirement: All models get same context and prompt
        """
        query = context_data["query"]
        formatted_context = context_data["formatted_context"]
        
        # Create identical system prompt for all models
        system_prompt = """ÿ£ŸÜÿ™ ÿÆÿ®Ÿäÿ± ŸÇÿßŸÜŸàŸÜŸä ÿ≥ÿπŸàÿØŸä ŸÖÿ™ÿÆÿµÿµ. ÿ£ÿ¨ÿ® ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ© ÿßŸÑŸÖŸÇÿØŸÖÿ© ŸÅŸÇÿ∑.

üéØ ŸÖŸáŸÖÿ™ŸÉ:
- ŸÇÿØŸÖ ÿßÿ≥ÿ™ÿ¥ÿßÿ±ÿ© ŸÇÿßŸÜŸàŸÜŸäÿ© ÿØŸÇŸäŸÇÿ© ŸàŸÖŸÅÿµŸÑÿ©
- ÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÅŸÇÿ∑ ÿßŸÑŸÖÿ±ÿßÿ¨ÿπ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ© ÿßŸÑŸÖŸÇÿØŸÖÿ©
- ÿßÿ∞ŸÉÿ± ÿ£ÿ±ŸÇÿßŸÖ ÿßŸÑŸÖŸàÿßÿØ ŸàÿßŸÑŸÇŸàÿßŸÜŸäŸÜ ÿπŸÜÿØ ÿßŸÑÿßÿ≥ÿ™ÿ¥ŸáÿßÿØ
- ŸÇÿØŸÖ ÿ£ŸÖÿ´ŸÑÿ© ÿ±ŸÇŸÖŸäÿ© ÿπŸÜÿØ ÿßŸÑÿ•ŸÖŸÉÿßŸÜ
- ÿßÿ¥ÿ±ÿ≠ ÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿÆÿ∑Ÿàÿ© ÿ®ÿÆÿ∑Ÿàÿ©
- ÿ™ÿ∑ÿ±ŸÇ ŸÑŸÑÿ≠ÿßŸÑÿßÿ™ ÿßŸÑÿßÿ≥ÿ™ÿ´ŸÜÿßÿ¶Ÿäÿ©
- ŸÇÿØŸÖ ŸÜÿµÿßÿ¶ÿ≠ ÿπŸÖŸÑŸäÿ©

‚öñÔ∏è ŸÇŸàÿßÿπÿØ ÿßŸÑÿßÿ≥ÿ™ÿ¥ŸáÿßÿØ:
- ÿ£ÿ¥ÿ± ŸÑŸÑŸÖÿßÿØÿ© ÿßŸÑŸÖÿ≠ÿØÿØÿ©: "ŸàŸÅŸÇÿßŸã ŸÑŸÑŸÖÿßÿØÿ© X ŸÖŸÜ ŸÜÿ∏ÿßŸÖ..."
- ŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÖŸÜ ÿÆÿßÿ±ÿ¨ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÖŸÇÿØŸÖÿ©
- ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ÿ¨ÿØ ÿßŸÑŸÖÿπŸÑŸàŸÖÿ© ŸÅŸä ÿßŸÑŸÜÿµŸàÿµÿå ŸÇŸÑ ÿ∞ŸÑŸÉ ÿ®Ÿàÿ∂Ÿàÿ≠"""

        # Create identical user prompt for all models  
        user_prompt = f"""{formatted_context}

‚ùì ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿßŸÑŸÇÿßŸÜŸàŸÜŸä:
{query}

ÿ£ÿ¨ÿ® ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ© ÿßŸÑŸÖŸÇÿØŸÖÿ© ÿ£ÿπŸÑÿßŸá ŸÅŸÇÿ∑."""

        # Generate responses in parallel
        tasks = []
        for model_name, client in self.generators.items():
            task = self._generate_single_response(
                model_name=model_name,
                client=client,
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            tasks.append((model_name, task))
        
        # Wait for all responses
        logger.info(f"üöÄ Generating responses from {len(tasks)} models in parallel...")
        start_time = time.time()
        
        responses = {}
        results = await asyncio.gather(*[task for _, task in tasks], return_exceptions=True)
        
        for (model_name, _), result in zip(tasks, results):
            if isinstance(result, Exception):
                logger.error(f"Model {model_name} failed: {result}")
                responses[model_name] = f"ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨: {str(result)}"
            else:
                responses[model_name] = result
                logger.info(f"‚úÖ {model_name}: {len(result)} characters")
        
        generation_time = time.time() - start_time
        logger.info(f"‚è±Ô∏è All models completed in {generation_time:.2f} seconds")
        
        return responses
    
    async def _generate_single_response(self, model_name: str, client: AsyncOpenAI, 
                                      system_prompt: str, user_prompt: str) -> str:
        """Generate response from a single model"""
        try:
            model_config = self.model_configs.get(model_name, {})
            model_id = model_config.get("model", "gpt-4o")  # Fallback
            
            response = await client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=config.temperature,
                max_tokens=config.max_tokens
            )
            
            content = response.choices[0].message.content
            logger.debug(f"üìù {model_name} generated {len(content)} characters")
            
            return content
            
        except Exception as e:
            logger.error(f"Model {model_name} generation failed: {e}")
            raise

# ==================== LAYER 3: COMPONENT EXTRACTION JUDGES ====================

@dataclass
class ComponentEvaluation:
    """Single component evaluation by one judge"""
    component: ComponentType
    evaluations: Dict[str, Dict[str, Any]]  # model_name -> {score, reasoning}
    winner: str  # winning model name
    extracted_text: str  # exact text from winning response
    winner_score: float
    reasoning: str

class ComponentJudge:
    """Single judge that evaluates all 7 components"""
    
    def __init__(self, judge_name: str, client: AsyncOpenAI):
        self.judge_name = judge_name
        self.client = client
        self.components = list(ComponentType)
        
        logger.info(f"‚öñÔ∏è Judge {judge_name} initialized for {len(self.components)} components")
    
    async def evaluate_all_components(self, query: str, responses: Dict[str, str]) -> List[ComponentEvaluation]:
        """Evaluate all 7 components across all 4 responses"""
        
        logger.info(f"‚öñÔ∏è Judge {self.judge_name} evaluating {len(self.components)} components...")
        
        evaluations = []
        for component in self.components:
            try:
                evaluation = await self._evaluate_single_component(component, query, responses)
                evaluations.append(evaluation)
                logger.debug(f"  ‚úÖ {component.value}: Winner = {evaluation.winner} (score: {evaluation.winner_score:.1f})")
            except Exception as e:
                logger.error(f"Component {component.value} evaluation failed: {e}")
                # Create fallback evaluation
                fallback = ComponentEvaluation(
                    component=component,
                    evaluations={},
                    winner="chatgpt",  # Safe fallback
                    extracted_text="[ÿ™ÿπÿ∞ÿ± ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ Ÿáÿ∞ÿß ÿßŸÑŸÖŸÉŸàŸÜ]",
                    winner_score=5.0,
                    reasoning=f"ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑÿ™ŸÇŸäŸäŸÖ: {str(e)}"
                )
                evaluations.append(fallback)
        
        logger.info(f"‚öñÔ∏è Judge {self.judge_name} completed all component evaluations")
        return evaluations
    
    async def _evaluate_single_component(self, component: ComponentType, query: str, 
                                       responses: Dict[str, str]) -> ComponentEvaluation:
        """Evaluate a single component across all responses"""
        
        # Create component-specific evaluation prompt
        evaluation_prompt = self._create_component_prompt(component, query, responses)
        
        # Get judge's evaluation
        response = await self.client.chat.completions.create(
            model="gpt-4o",  # Use consistent model for judges
            messages=[{"role": "user", "content": evaluation_prompt}],
            temperature=0.1,  # Low temperature for consistent evaluation
            max_tokens=1500
        )
        
        evaluation_text = response.choices[0].message.content
        
        # Parse evaluation result
        return self._parse_evaluation_result(component, evaluation_text, responses)
    
    def _create_component_prompt(self, component: ComponentType, query: str, 
                               responses: Dict[str, str]) -> str:
        """Create evaluation prompt for specific component"""
        
        component_descriptions = {
            ComponentType.DIRECT_ANSWER: {
                "name": "ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÖÿ®ÿßÿ¥ÿ±ÿ©",
                "description": "ÿßŸÑÿ¨Ÿàÿßÿ® ÿßŸÑŸàÿßÿ∂ÿ≠ ŸàÿßŸÑŸÖÿ®ÿßÿ¥ÿ± ÿπŸÑŸâ ÿßŸÑÿ≥ÿ§ÿßŸÑ ŸÅŸä 2-3 ÿ¨ŸÖŸÑ",
                "example": "ŸÖÿØÿ© ÿßŸÑÿ•ÿ¨ÿßÿ≤ÿ© ÿßŸÑÿ≥ŸÜŸàŸäÿ© 21 ŸäŸàŸÖÿßŸãÿå ÿ™ÿ≤ŸäÿØ ÿ•ŸÑŸâ 30 ÿ®ÿπÿØ 5 ÿ≥ŸÜŸàÿßÿ™ ÿÆÿØŸÖÿ©"
            },
            ComponentType.LEGAL_FOUNDATION: {
                "name": "ÿßŸÑÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÇÿßŸÜŸàŸÜŸä", 
                "description": "ÿ¥ÿ±ÿ≠ ÿßŸÑŸÇŸàÿßŸÜŸäŸÜ ŸàÿßŸÑŸÖŸàÿßÿØ ÿßŸÑŸÖÿ∑ÿ®ŸÇÿ© ŸàÿßŸÑÿ•ÿ∑ÿßÿ± ÿßŸÑŸÇÿßŸÜŸàŸÜŸä",
                "example": "ŸàŸÅŸÇÿßŸã ŸÑŸÑŸÖÿßÿØÿ© 84 ŸÖŸÜ ŸÜÿ∏ÿßŸÖ ÿßŸÑÿπŸÖŸÑ ÿßŸÑÿ≥ÿπŸàÿØŸä..."
            },
            ComponentType.ARTICLE_CITATIONS: {
                "name": "ÿßŸÑÿßÿ≥ÿ™ÿ¥ŸáÿßÿØÿßÿ™ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ©",
                "description": "ÿ¨ŸÖŸäÿπ ÿ£ÿ±ŸÇÿßŸÖ ÿßŸÑŸÖŸàÿßÿØ ŸàÿßŸÑŸÇŸàÿßŸÜŸäŸÜ ÿßŸÑŸÖÿ∞ŸÉŸàÿ±ÿ©",
                "example": "ÿßŸÑŸÖŸàÿßÿØ 84ÿå 85ÿå 109 ŸÖŸÜ ŸÜÿ∏ÿßŸÖ ÿßŸÑÿπŸÖŸÑ"
            },
            ComponentType.NUMERICAL_EXAMPLES: {
                "name": "ÿßŸÑÿ£ŸÖÿ´ŸÑÿ© ÿßŸÑÿ±ŸÇŸÖŸäÿ©",
                "description": "ÿ≠ÿ≥ÿßÿ®ÿßÿ™ ÿ®ÿ£ÿ±ŸÇÿßŸÖ ÿ≠ŸÇŸäŸÇŸäÿ© ŸàÿµŸäÿ∫ ÿ±Ÿäÿßÿ∂Ÿäÿ©",
                "example": "ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿ±ÿßÿ™ÿ® 6000 ÿ±ŸäÿßŸÑ: ÿßŸÑŸÖŸÉÿßŸÅÿ£ÿ© = 0.5 √ó 6000 √ó 5 = 15,000 ÿ±ŸäÿßŸÑ"
            },
            ComponentType.STEP_PROCEDURES: {
                "name": "ÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑŸÖÿ™ÿ≥ŸÑÿ≥ŸÑÿ©",
                "description": "ÿÆÿ∑Ÿàÿßÿ™ Ÿàÿßÿ∂ÿ≠ÿ© ŸÇÿßÿ®ŸÑÿ© ŸÑŸÑÿ™ŸÜŸÅŸäÿ∞",
                "example": "ÿßŸÑÿÆÿ∑Ÿàÿ© 1: ÿ™ŸÇÿØŸäŸÖ ÿ¥ŸÉŸàŸâ ŸÑŸÖŸÉÿ™ÿ® ÿßŸÑÿπŸÖŸÑ..."
            },
            ComponentType.EDGE_CASES: {
                "name": "ÿßŸÑÿ≠ÿßŸÑÿßÿ™ ÿßŸÑÿßÿ≥ÿ™ÿ´ŸÜÿßÿ¶Ÿäÿ©",
                "description": "ÿ∏ÿ±ŸàŸÅ ÿÆÿßÿµÿ©ÿå ÿ™ÿπŸÇŸäÿØÿßÿ™ÿå ÿßÿ≥ÿ™ÿ´ŸÜÿßÿ°ÿßÿ™",
                "example": "ŸÅŸä ÿ≠ÿßŸÑÿ© ÿßŸÑÿßÿ≥ÿ™ŸÇÿßŸÑÿ© ŸÑŸÑÿ≤Ÿàÿßÿ¨ÿå ÿ™ÿ≥ÿ™ÿ≠ŸÇ ÿßŸÑŸÖŸÉÿßŸÅÿ£ÿ© ŸÉÿßŸÖŸÑÿ©"
            },
            ComponentType.PRACTICAL_ADVICE: {
                "name": "ÿßŸÑŸÜÿµÿßÿ¶ÿ≠ ÿßŸÑÿπŸÖŸÑŸäÿ©", 
                "description": "ÿ™ŸàÿµŸäÿßÿ™ ÿπŸÖŸÑŸäÿ©ÿå ÿ™ÿ≠ÿ∞Ÿäÿ±ÿßÿ™ÿå ŸÜÿµÿßÿ¶ÿ≠ ŸàÿßŸÇÿπŸäÿ©",
                "example": "ŸäŸèŸÅÿ∂ŸÑ ÿßŸÑÿßÿ≠ÿ™ŸÅÿßÿ∏ ÿ®ŸÜÿ≥ÿÆ ŸÖŸÜ ÿ¨ŸÖŸäÿπ ÿßŸÑŸÖÿ±ÿßÿ≥ŸÑÿßÿ™"
            }
        }
        
        comp_info = component_descriptions[component]
        
        # Build responses section
        responses_text = ""
        for model_name, response in responses.items():
            responses_text += f"\n--- ÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿ© {model_name.upper()} ---\n{response}\n"
        
        prompt = f"""ÿ£ŸÜÿ™ ÿÆÿ®Ÿäÿ± ÿ™ŸÇŸäŸäŸÖ ŸÇÿßŸÜŸàŸÜŸä. ŸÇŸäŸÖ ÿ¨ŸàÿØÿ© ŸÖŸÉŸàŸÜ ŸÖÿ≠ÿØÿØ ŸÅŸä 4 ÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿßÿ™ ŸÖÿÆÿ™ŸÑŸÅÿ©.

ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿßŸÑÿ£ÿµŸÑŸä: {query}

ÿßŸÑŸÖŸÉŸàŸÜ ÿßŸÑŸÖÿ∑ŸÑŸàÿ® ÿ™ŸÇŸäŸäŸÖŸá: {comp_info['name']}
ÿßŸÑŸàÿµŸÅ: {comp_info['description']}
ŸÖÿ´ÿßŸÑ: {comp_info['example']}

ÿßŸÑÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿßÿ™ ÿßŸÑÿ£ÿ±ÿ®ÿπÿ©:{responses_text}

üìã ŸÖŸáŸÖÿ™ŸÉ:
1. ŸÇŸäŸÖ ÿ¨ŸàÿØÿ© "{comp_info['name']}" ŸÅŸä ŸÉŸÑ ÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿ© (ÿØÿ±ÿ¨ÿ© ŸÖŸÜ 0-10)
2. ÿßÿÆÿ™ÿ± ÿßŸÑÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿ© ÿßŸÑÿ£ŸÅÿ∂ŸÑ
3. ÿßÿ≥ÿ™ÿÆÿ±ÿ¨ ÿßŸÑŸÜÿµ ÿßŸÑÿØŸÇŸäŸÇ ŸÑŸÑŸÖŸÉŸàŸÜ ŸÖŸÜ ÿßŸÑÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÅÿßÿ¶ÿ≤ÿ©
4. ÿßÿ¥ÿ±ÿ≠ ÿ≥ÿ®ÿ® ÿßÿÆÿ™Ÿäÿßÿ±ŸÉ

üìä ÿµŸäÿ∫ÿ© ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© (JSON ŸÅŸÇÿ∑):
{{
  "evaluations": {{
    "chatgpt": {{"score": 8.5, "reasoning": "ÿ≥ÿ®ÿ® ÿßŸÑÿ™ŸÇŸäŸäŸÖ"}},
    "deepseek": {{"score": 6.0, "reasoning": "ÿ≥ÿ®ÿ® ÿßŸÑÿ™ŸÇŸäŸäŸÖ"}},
    "grok": {{"score": 9.0, "reasoning": "ÿ≥ÿ®ÿ® ÿßŸÑÿ™ŸÇŸäŸäŸÖ"}}, 
    "gemini": {{"score": 7.5, "reasoning": "ÿ≥ÿ®ÿ® ÿßŸÑÿ™ŸÇŸäŸäŸÖ"}}
  }},
  "winner": "grok",
  "extracted_text": "ÿßŸÑŸÜÿµ ÿßŸÑÿØŸÇŸäŸÇ ŸÑŸÑŸÖŸÉŸàŸÜ ŸÖŸÜ ÿßŸÑÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÅÿßÿ¶ÿ≤ÿ©",
  "reasoning": "ÿ≥ÿ®ÿ® ÿßÿÆÿ™Ÿäÿßÿ± Ÿáÿ∞Ÿá ÿßŸÑÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿ© ŸÉÿ£ŸÅÿ∂ŸÑ"
}}

ÿ£ÿ¨ÿ® ÿ®ŸÄ JSON ŸÅŸÇÿ∑ÿå ŸÑÿß ŸÜÿµ ÿ•ÿ∂ÿßŸÅŸä."""

        return prompt
    
    def _parse_evaluation_result(self, component: ComponentType, evaluation_text: str, 
                               responses: Dict[str, str]) -> ComponentEvaluation:
        """Parse judge's evaluation result"""
        
        try:
            # Clean and parse JSON
            clean_text = evaluation_text.strip()
            if "```json" in clean_text:
                clean_text = clean_text.split("```json")[1].split("```")[0].strip()
            elif "```" in clean_text:
                clean_text = clean_text.split("```")[1].split("```")[0].strip()
            
            result = json.loads(clean_text)
            
            evaluations = result.get("evaluations", {})
            winner = result.get("winner", "chatgpt")
            extracted_text = result.get("extracted_text", "[ŸÑŸÖ Ÿäÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿßŸÑŸÜÿµ]")
            reasoning = result.get("reasoning", "ŸÑŸÖ Ÿäÿ™ŸÖ ÿ™ŸÇÿØŸäŸÖ ÿ™ÿ®ÿ±Ÿäÿ±")
            
            # Get winner score
            winner_score = 5.0  # Default
            if winner in evaluations:
                winner_score = float(evaluations[winner].get("score", 5.0))
            
            return ComponentEvaluation(
                component=component,
                evaluations=evaluations,
                winner=winner,
                extracted_text=extracted_text,
                winner_score=winner_score,
                reasoning=reasoning
            )
            
        except Exception as e:
            logger.error(f"Failed to parse evaluation for {component.value}: {e}")
            logger.error(f"Raw evaluation text: {evaluation_text[:300]}...")
            
            # Fallback evaluation
            return ComponentEvaluation(
                component=component,
                evaluations={},
                winner="chatgpt",
                extracted_text="[ÿÆÿ∑ÿ£ ŸÅŸä ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©]",
                winner_score=5.0,
                reasoning=f"ÿÆÿ∑ÿ£ ŸÅŸä ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©: {str(e)}"
            )

class ThreeJudgeSystem:
    """Coordinate 3 judges to evaluate all components"""
    
    def __init__(self, clients: ModelClients):
        self.judges = {}
        available_judges = clients.get_available_judges()
        
        for judge_name, client in available_judges.items():
            self.judges[judge_name] = ComponentJudge(judge_name, client)
        
        logger.info(f"‚öñÔ∏è Three-judge system initialized with {len(self.judges)} judges: {list(self.judges.keys())}")
    
    async def evaluate_responses(self, query: str, responses: Dict[str, str]) -> Dict[str, List[ComponentEvaluation]]:
        """All 3 judges evaluate all responses"""
        
        logger.info("‚öñÔ∏è Starting 3-judge evaluation process...")
        start_time = time.time()
        
        # Run all judges in parallel
        judge_tasks = []
        for judge_name, judge in self.judges.items():
            task = judge.evaluate_all_components(query, responses)
            judge_tasks.append((judge_name, task))
        
        # Wait for all judges
        judge_results = {}
        results = await asyncio.gather(*[task for _, task in judge_tasks], return_exceptions=True)
        
        for (judge_name, _), result in zip(judge_tasks, results):
            if isinstance(result, Exception):
                logger.error(f"Judge {judge_name} failed: {result}")
                judge_results[judge_name] = []
            else:
                judge_results[judge_name] = result
                logger.info(f"‚öñÔ∏è Judge {judge_name}: {len(result)} component evaluations")
        
        evaluation_time = time.time() - start_time
        logger.info(f"‚öñÔ∏è All judges completed in {evaluation_time:.2f} seconds")
        
        return judge_results

# ==================== LAYER 4: CONSENSUS BUILDING ====================

@dataclass  
class ConsensusResult:
    """Final consensus for one component"""
    component: ComponentType
    final_text: str
    winning_model: str
    consensus_type: str  # "strong_consensus", "highest_score", "missing"
    judge_votes: List[str]  # Which models each judge chose
    final_score: float

class ConsensusBuilder:
    """Build consensus from 3 judge evaluations using voting rules"""
    
    def __init__(self):
        logger.info("üó≥Ô∏è Consensus builder initialized")
    
    def build_consensus(self, judge_evaluations: Dict[str, List[ComponentEvaluation]]) -> List[ConsensusResult]:
        """Apply voting rules to select best components"""
        
        if not judge_evaluations:
            logger.warning("No judge evaluations provided")
            return []
        
        # Get all components to process
        components = list(ComponentType)
        consensus_results = []
        
        for component in components:
            try:
                consensus = self._build_component_consensus(component, judge_evaluations)
                consensus_results.append(consensus)
                
                logger.info(f"üó≥Ô∏è {component.value}: {consensus.consensus_type} -> {consensus.winning_model} (score: {consensus.final_score:.1f})")
                
            except Exception as e:
                logger.error(f"Consensus building failed for {component.value}: {e}")
                # Create fallback consensus
                fallback = ConsensusResult(
                    component=component,
                    final_text="[ÿ™ÿπÿ∞ÿ± ÿ®ŸÜÿßÿ° ÿßŸÑÿ•ÿ¨ŸÖÿßÿπ ŸÑŸáÿ∞ÿß ÿßŸÑŸÖŸÉŸàŸÜ]",
                    winning_model="chatgpt",
                    consensus_type="error",
                    judge_votes=[],
                    final_score=0.0
                )
                consensus_results.append(fallback)
        
        logger.info(f"üó≥Ô∏è Consensus building completed for {len(consensus_results)} components")
        return consensus_results
    
    def _build_component_consensus(self, component: ComponentType, 
                                 judge_evaluations: Dict[str, List[ComponentEvaluation]]) -> ConsensusResult:
        """Build consensus for a single component"""
        
        # Extract evaluations for this component from all judges
        component_evaluations = []
        for judge_name, evaluations in judge_evaluations.items():
            for eval_item in evaluations:
                if eval_item.component == component:
                    component_evaluations.append((judge_name, eval_item))
                    break
        
        if not component_evaluations:
            # Component missing from all judges
            return ConsensusResult(
                component=component,
                final_text="",
                winning_model="none",
                consensus_type="missing",
                judge_votes=[],
                final_score=0.0
            )
        
        # Extract votes and scores
        judge_votes = [eval_item.winner for _, eval_item in component_evaluations]
        judge_scores = [eval_item.winner_score for _, eval_item in component_evaluations]
        
        # Apply voting rules
        
        # Rule 1: Strong Consensus (2+ judges agree)
        vote_counts = {}
        for vote in judge_votes:
            vote_counts[vote] = vote_counts.get(vote, 0) + 1
        
        # Find if any model has 2+ votes
        consensus_winner = None
        for model, count in vote_counts.items():
            if count >= 2:
                consensus_winner = model
                break
        
        if consensus_winner:
            # Strong consensus found
            winning_evaluation = None
            for _, eval_item in component_evaluations:
                if eval_item.winner == consensus_winner:
                    winning_evaluation = eval_item
                    break
            
            return ConsensusResult(
                component=component,
                final_text=winning_evaluation.extracted_text if winning_evaluation else "",
                winning_model=consensus_winner,
                consensus_type="strong_consensus",
                judge_votes=judge_votes,
                final_score=winning_evaluation.winner_score if winning_evaluation else 0.0
            )
        
        # Rule 2: No Consensus - Use Highest Individual Score
        best_score = max(judge_scores)
        best_judge_idx = judge_scores.index(best_score)
        best_winner = judge_votes[best_judge_idx]
        best_evaluation = component_evaluations[best_judge_idx][1]
        
        return ConsensusResult(
            component=component,
            final_text=best_evaluation.extracted_text,
            winning_model=best_winner,
            consensus_type="highest_score",
            judge_votes=judge_votes,
            final_score=best_score
        )
    
    def _handle_special_citations(self, judge_evaluations: Dict[str, List[ComponentEvaluation]]) -> str:
        """Rule 4: Special handling for citations - take union of all"""
        
        all_citations = set()
        
        for judge_name, evaluations in judge_evaluations.items():
            for eval_item in evaluations:
                if eval_item.component == ComponentType.ARTICLE_CITATIONS:
                    # Extract article numbers from the text
                    citations = self._extract_article_numbers(eval_item.extracted_text)
                    all_citations.update(citations)
        
        if all_citations:
            sorted_citations = sorted(all_citations, key=lambda x: int(x) if x.isdigit() else 999)
            return f"ÿßŸÑŸÖŸàÿßÿØ: {', '.join(sorted_citations)}"
        
        return ""
    
    def _extract_article_numbers(self, text: str) -> List[str]:
        """Extract article numbers from text"""
        import re
        
        # Pattern to match Arabic and English article numbers
        patterns = [
            r'ÿßŸÑŸÖÿßÿØÿ©\s+(\d+)',
            r'ŸÖÿßÿØÿ©\s+(\d+)', 
            r'Article\s+(\d+)',
            r'(?:^|[^\d])(\d{1,3})(?:[^\d]|$)'  # Standalone numbers 1-999
        ]
        
        found_articles = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            found_articles.extend(matches)
        
        # Remove duplicates and return
        return list(set(found_articles))

# ==================== LAYER 5: RESPONSE ASSEMBLY ====================

class ResponseAssembler:
    """Assemble final response from consensus components"""
    
    def __init__(self, clients: ModelClients):
        # Use ChatGPT for assembly (reliable for text editing)
        self.assembler_client = clients.chatgpt
        logger.info("üîß Response assembler initialized")
    
    async def assemble_final_response(self, consensus_results: List[ConsensusResult], 
                                    query: str, use_smooth_transitions: bool = True) -> str:
        """Assemble final response from winning components"""
        
        if use_smooth_transitions:
            return await self._assemble_with_transitions(consensus_results, query)
        else:
            return self._assemble_simple_concatenation(consensus_results)
    
    def _assemble_simple_concatenation(self, consensus_results: List[ConsensusResult]) -> str:
        """Option A: Simple concatenation with section breaks"""
        
        # Assembly order as specified
        order = [
            ComponentType.DIRECT_ANSWER,
            ComponentType.LEGAL_FOUNDATION, 
            ComponentType.ARTICLE_CITATIONS,
            ComponentType.NUMERICAL_EXAMPLES,
            ComponentType.STEP_PROCEDURES,
            ComponentType.EDGE_CASES,
            ComponentType.PRACTICAL_ADVICE
        ]
        
        sections = []
        
        for component_type in order:
            # Find consensus result for this component
            component_result = None
            for result in consensus_results:
                if result.component == component_type:
                    component_result = result
                    break
            
            if component_result and component_result.final_text.strip():
                # Add section header
                section_headers = {
                    ComponentType.DIRECT_ANSWER: "## ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÖÿ®ÿßÿ¥ÿ±ÿ©",
                    ComponentType.LEGAL_FOUNDATION: "## ÿßŸÑÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÇÿßŸÜŸàŸÜŸä",
                    ComponentType.ARTICLE_CITATIONS: "## ÿßŸÑÿßÿ≥ÿ™ÿ¥ŸáÿßÿØÿßÿ™ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ©",
                    ComponentType.NUMERICAL_EXAMPLES: "## ÿßŸÑÿ£ŸÖÿ´ŸÑÿ© ÿßŸÑÿ±ŸÇŸÖŸäÿ©", 
                    ComponentType.STEP_PROCEDURES: "## ÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©",
                    ComponentType.EDGE_CASES: "## ÿßŸÑÿ≠ÿßŸÑÿßÿ™ ÿßŸÑÿßÿ≥ÿ™ÿ´ŸÜÿßÿ¶Ÿäÿ©",
                    ComponentType.PRACTICAL_ADVICE: "## ÿßŸÑŸÜÿµÿßÿ¶ÿ≠ ÿßŸÑÿπŸÖŸÑŸäÿ©"
                }
                
                header = section_headers.get(component_type, f"## {component_type.value}")
                sections.append(f"{header}\n\n{component_result.final_text}")
        
        final_response = "\n\n---\n\n".join(sections)
        
        if not final_response.strip():
            final_response = "ÿπÿ∞ÿ±ÿßŸãÿå ŸÑŸÖ ÿ£ÿ™ŸÖŸÉŸÜ ŸÖŸÜ ÿ™ÿ¨ŸÖŸäÿπ ÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿ© ŸÖŸÜÿßÿ≥ÿ®ÿ© ŸÖŸÜ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑŸÖÿ™ÿßÿ≠ÿ©."
        
        logger.info(f"üîß Simple assembly completed: {len(sections)} sections, {len(final_response)} characters")
        return final_response
    
    async def _assemble_with_transitions(self, consensus_results: List[ConsensusResult], query: str) -> str:
        """Option B: Smooth transitions with AI assembly"""
        
        if not self.assembler_client:
            logger.warning("No assembler client available, falling back to simple concatenation")
            return self._assemble_simple_concatenation(consensus_results)
        
        try:
            # Collect all components
            component_texts = {}
            
            for result in consensus_results:
                if result.final_text.strip():
                    component_texts[result.component.value] = result.final_text
            
            if not component_texts:
                return "ÿπÿ∞ÿ±ÿßŸãÿå ŸÑŸÖ ÿ£ÿ™ŸÖŸÉŸÜ ŸÖŸÜ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿ£Ÿä ŸÖŸÉŸàŸÜÿßÿ™ ŸÖŸÜÿßÿ≥ÿ®ÿ© ŸÖŸÜ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨."
            
            # Create assembly prompt
            assembly_prompt = self._create_assembly_prompt(component_texts, query)
            
            # Generate assembled response
            response = await self.assembler_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": assembly_prompt}],
                temperature=0.3,  # Low temperature for consistent assembly
                max_tokens=3000
            )
            
            assembled_response = response.choices[0].message.content.strip()
            
            logger.info(f"üîß Smooth assembly completed: {len(component_texts)} components, {len(assembled_response)} characters")
            
            return assembled_response
            
        except Exception as e:
            logger.error(f"Smooth assembly failed: {e}, falling back to simple concatenation")
            return self._assemble_simple_concatenation(consensus_results)
    
    def _create_assembly_prompt(self, component_texts: Dict[str, str], query: str) -> str:
        """Create prompt for smooth assembly"""
        
        components_section = ""
        for component_name, text in component_texts.items():
            components_section += f"\n[{component_name}]\n{text}\n"
        
        return f"""ÿ£ŸÜÿ™ ŸÖÿ≠ÿ±ÿ± ŸÇÿßŸÜŸàŸÜŸä ŸÖÿ™ÿÆÿµÿµ ŸÅŸä ÿ™ÿ¨ŸÖŸäÿπ ÿßŸÑÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿßÿ™ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ©.

ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿßŸÑÿ£ÿµŸÑŸä: {query}

ÿßŸÑŸÖŸÉŸàŸÜÿßÿ™ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿ±ÿ¨ÿ© ŸÖŸÜ ÿ£ŸÅÿ∂ŸÑ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨:{components_section}

üéØ ŸÖŸáŸÖÿ™ŸÉ:
- ÿßÿ¨ŸÖÿπ Ÿáÿ∞Ÿá ÿßŸÑŸÖŸÉŸàŸÜÿßÿ™ ŸÅŸä ÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿ© Ÿàÿßÿ≠ÿØÿ© ŸÖÿ™ŸÖÿßÿ≥ŸÉÿ©
- ÿ£ÿ∂ŸÅ ÿ¨ŸÖŸÑ ÿ±ÿ®ÿ∑ ÿ®ÿ≥Ÿäÿ∑ÿ© ÿ®ŸäŸÜ ÿßŸÑÿ£ŸÇÿ≥ÿßŸÖ ÿ≠ÿ≥ÿ® ÿßŸÑÿ≠ÿßÿ¨ÿ©
- ÿßÿ≠ÿ™ŸÅÿ∏ ÿ®ÿßŸÑŸÜÿµ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿ±ÿ¨ ÿ®ÿßŸÑÿ∂ÿ®ÿ∑ ŸÉŸÖÿß ŸáŸà (ŸÑÿß ÿ™ÿπÿØŸÑ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ)
- ÿ±ÿ™ÿ® ÿßŸÑŸÖŸÉŸàŸÜÿßÿ™ ŸÖŸÜÿ∑ŸÇŸäÿßŸã (ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÖÿ®ÿßÿ¥ÿ±ÿ© ÿ£ŸàŸÑÿßŸãÿå ÿßŸÑŸÜÿµÿßÿ¶ÿ≠ ÿ£ÿÆŸäÿ±ÿßŸã)
- ÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿßŸÑÿ™ÿØŸÅŸÇ ÿßŸÑÿ∑ÿ®ŸäÿπŸä ŸÑŸÑŸÜÿµ

‚ö†Ô∏è ŸÇŸàÿßÿπÿØ ÿ≠ÿßÿ≥ŸÖÿ©:
- ŸÑÿß ÿ™ÿ∫Ÿäÿ± ÿ£Ÿà ÿ™ÿπŸäÿØ ÿµŸäÿßÿ∫ÿ© ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿ±ÿ¨ÿ©
- ÿ£ÿ∂ŸÅ ŸÅŸÇÿ∑ ÿ¨ŸÖŸÑ ÿ±ÿ®ÿ∑ ŸÇÿµŸäÿ±ÿ© ÿπŸÜÿØ ÿßŸÑÿ∂ÿ±Ÿàÿ±ÿ©
- ŸÑÿß ÿ™ÿ∂ŸÅ ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÇÿßŸÜŸàŸÜŸäÿ© ÿ¨ÿØŸäÿØÿ©
- ÿßŸÑŸáÿØŸÅ: ÿ™ÿØŸÅŸÇ ÿ∑ÿ®ŸäÿπŸä ŸÑŸÑŸÖŸÉŸàŸÜÿßÿ™ ÿßŸÑŸÖŸàÿ¨ŸàÿØÿ©

ÿ£ÿ¨ÿ® ÿ®ÿßŸÑÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ© ÿßŸÑŸÖÿ¨ŸÖÿπÿ© ŸÅŸÇÿ∑."""

# ==================== LAYER 6: QUALITY VERIFICATION ====================

@dataclass
class QualityReport:
    """Quality verification report"""
    passed: bool
    length_check: bool
    citation_check: bool  
    completeness_check: bool
    coherence_check: bool
    duplication_check: bool
    issues: List[str]
    metrics: Dict[str, Any]

class QualityVerifier:
    """Verify quality of assembled response before saving"""
    
    def __init__(self):
        logger.info("üîç Quality verifier initialized")
    
    def verify_quality(self, final_response: str, original_responses: Dict[str, str],
                      consensus_results: List[ConsensusResult], context_chunks: List[Chunk]) -> QualityReport:
        """Run all quality checks"""
        
        issues = []
        
        # Check 1: Length Check
        length_check = self._check_length(final_response, original_responses)
        if not length_check:
            issues.append("Final response is shorter than longest individual response")
        
        # Check 2: Citation Verification  
        citation_check = self._check_citations(final_response, context_chunks)
        if not citation_check:
            issues.append("Some citations not found in provided context")
        
        # Check 3: Completeness Check
        completeness_check = self._check_completeness(final_response, consensus_results)
        if not completeness_check:
            issues.append("Some extracted components missing from final response")
        
        # Check 4: Coherence Check
        coherence_check = self._check_coherence(final_response)
        if not coherence_check:
            issues.append("Response appears disjointed or incoherent")
        
        # Check 5: No Duplication
        duplication_check = self._check_duplication(final_response)
        if not duplication_check:
            issues.append("Response contains repeated information")
        
        passed = all([length_check, citation_check, completeness_check, coherence_check, duplication_check])
        
        # Calculate metrics
        metrics = {
            "final_length": len(final_response),
            "max_original_length": max(len(resp) for resp in original_responses.values()) if original_responses else 0,
            "component_count": len([r for r in consensus_results if r.final_text.strip()]),
            "citation_count": self._count_citations(final_response),
            "coherence_score": self._calculate_coherence_score(final_response)
        }
        
        report = QualityReport(
            passed=passed,
            length_check=length_check,
            citation_check=citation_check,
            completeness_check=completeness_check, 
            coherence_check=coherence_check,
            duplication_check=duplication_check,
            issues=issues,
            metrics=metrics
        )
        
        if passed:
            logger.info("‚úÖ Quality verification passed")
        else:
            logger.warning(f"‚ö†Ô∏è Quality issues found: {', '.join(issues)}")
        
        return report
    
    def _check_length(self, final_response: str, original_responses: Dict[str, str]) -> bool:
        """Check if final response is comprehensive enough"""
        if not original_responses:
            return True
        
        final_length = len(final_response)
        max_original = max(len(resp) for resp in original_responses.values())
        
        # Final should be at least 80% of longest original (allowing for some compression)
        return final_length >= max_original * 0.8
    
    def _check_citations(self, final_response: str, context_chunks: List[Chunk]) -> bool:
        """Check if all cited articles exist in provided context"""
        import re
        
        # Extract article numbers from final response
        cited_articles = set()
        patterns = [r'ÿßŸÑŸÖÿßÿØÿ©\s+(\d+)', r'ŸÖÿßÿØÿ©\s+(\d+)']
        
        for pattern in patterns:
            matches = re.findall(pattern, final_response)
            cited_articles.update(matches)
        
        if not cited_articles:
            return True  # No citations to verify
        
        # Extract available articles from context
        available_articles = set()
        for chunk in context_chunks:
            for pattern in patterns:
                matches = re.findall(pattern, chunk.content)
                available_articles.update(matches)
        
        # Check if all cited articles are available
        missing_articles = cited_articles - available_articles
        
        if missing_articles:
            logger.warning(f"Missing article citations: {missing_articles}")
            return False
        
        return True
    
    def _check_completeness(self, final_response: str, consensus_results: List[ConsensusResult]) -> bool:
        """Check if extracted components made it to final response"""
        
        missing_components = 0
        total_components = 0
        
        for result in consensus_results:
            if result.final_text.strip():
                total_components += 1
                
                # Check if component text appears in final response (allowing for minor edits)
                component_words = set(result.final_text.split()[:10])  # First 10 words
                final_words = set(final_response.split())
                
                overlap = len(component_words.intersection(final_words))
                if overlap < len(component_words) * 0.5:  # At least 50% word overlap
                    missing_components += 1
        
        if total_components == 0:
            return True
        
        completion_rate = (total_components - missing_components) / total_components
        return completion_rate >= 0.8  # At least 80% of components present
    
    def _check_coherence(self, final_response: str) -> bool:
        """Basic coherence check"""
        
        # Check for signs of poor assembly
        incoherence_indicators = [
            "## ##",  # Double headers
            "\n\n\n\n\n",  # Too many line breaks
            "[ÿ™ÿπÿ∞ÿ±",  # Error messages
            "ÿÆÿ∑ÿ£ ŸÅŸä",  # Error indicators
        ]
        
        for indicator in incoherence_indicators:
            if indicator in final_response:
                return False
        
        # Check minimum length for coherent response
        if len(final_response) < 100:
            return False
        
        return True
    
    def _check_duplication(self, final_response: str) -> bool:
        """Check for repeated information"""
        
        sentences = final_response.split('.')
        sentence_counts = {}
        
        for sentence in sentences:
            clean_sentence = sentence.strip()
            if len(clean_sentence) > 20:  # Only check meaningful sentences
                # Simple similarity check
                for existing in sentence_counts:
                    if len(set(clean_sentence.split()).intersection(set(existing.split()))) > len(clean_sentence.split()) * 0.7:
                        return False  # High similarity = likely duplication
                
                sentence_counts[clean_sentence] = sentence_counts.get(clean_sentence, 0) + 1
        
        # Check for exact duplicates
        for count in sentence_counts.values():
            if count > 1:
                return False
        
        return True
    
    def _count_citations(self, text: str) -> int:
        """Count legal citations in text"""
        import re
        patterns = [r'ÿßŸÑŸÖÿßÿØÿ©\s+\d+', r'ŸÖÿßÿØÿ©\s+\d+', r'ÿßŸÑŸÇÿßŸÜŸàŸÜ', r'ÿßŸÑŸÜÿ∏ÿßŸÖ']
        
        total_citations = 0
        for pattern in patterns:
            matches = re.findall(pattern, text)
            total_citations += len(matches)
        
        return total_citations
    
    def _calculate_coherence_score(self, text: str) -> float:
        """Calculate basic coherence score"""
        
        # Simple heuristics for coherence
        score = 1.0
        
        # Penalty for too short
        if len(text) < 200:
            score -= 0.3
        
        # Penalty for too many line breaks
        line_break_ratio = text.count('\n\n') / max(len(text) / 100, 1)
        if line_break_ratio > 5:
            score -= 0.2
        
        # Bonus for proper Arabic structure
        if 'ÿ£ŸàŸÑÿßŸã' in text and 'ÿ´ÿßŸÜŸäÿßŸã' in text:
            score += 0.1
        
        if any(phrase in text for phrase in ['ŸàŸÅŸÇÿßŸã ŸÑ', 'ÿßÿ≥ÿ™ŸÜÿßÿØÿßŸã ÿ•ŸÑŸâ', 'ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ']):
            score += 0.1
        
        return max(0.0, min(1.0, score))

# ==================== LAYER 7: DATA COLLECTION ====================

@dataclass
class EnsembleTrainingData:
    """Training data format for fine-tuning"""
    query: str
    retrieved_context: List[str]
    generator_responses: Dict[str, str]
    judge_evaluations: Dict[str, List[ComponentEvaluation]]
    component_winners: Dict[str, str]
    final_response: str
    quality_scores: Dict[str, Any]
    timestamp: str
    processing_time_ms: int
    cost_estimate: float

class DataCollector:
    """Collect ensemble data for fine-tuning"""
    
    def __init__(self, config: EnsembleConfig):
        self.config = config
        self.data_file = config.training_data_path
        logger.info(f"üíæ Data collector initialized: {self.data_file}")
    
    def collect_training_example(self, query: str, context_data: Dict[str, Any],
                               responses: Dict[str, str], judge_evaluations: Dict[str, List[ComponentEvaluation]],
                               consensus_results: List[ConsensusResult], final_response: str,
                               quality_report: QualityReport, processing_time_ms: int,
                               cost_estimate: float) -> EnsembleTrainingData:
        """Create training data example"""
        
        # Convert context chunks to strings
        context_strings = []
        if "chunks" in context_data:
            for chunk in context_data["chunks"]:
                context_strings.append(f"{chunk.title}: {chunk.content}")
        
        # Extract component winners
        component_winners = {}
        for result in consensus_results:
            component_winners[result.component.value] = result.winning_model
        
        training_example = EnsembleTrainingData(
            query=query,
            retrieved_context=context_strings,
            generator_responses=responses,
            judge_evaluations=judge_evaluations,
            component_winners=component_winners,
            final_response=final_response,
            quality_scores=quality_report.metrics,
            timestamp=datetime.now().isoformat(),
            processing_time_ms=processing_time_ms,
            cost_estimate=cost_estimate
        )
        
        if self.config.save_training_data:
            self._save_training_example(training_example)
        
        return training_example
    
    def _save_training_example(self, example: EnsembleTrainingData):
        """Save training example to JSONL file"""
        
        try:
            # Convert to dict for JSON serialization
            data_dict = {
                "query": example.query,
                "retrieved_context": example.retrieved_context,
                "generator_responses": example.generator_responses,
                "judge_evaluations": self._serialize_judge_evaluations(example.judge_evaluations),
                "component_winners": example.component_winners,
                "final_response": example.final_response,
                "quality_scores": example.quality_scores,
                "timestamp": example.timestamp,
                "processing_time_ms": example.processing_time_ms,
                "cost_estimate": example.cost_estimate
            }
            
            # Append to JSONL file
            import os
            os.makedirs(os.path.dirname(self.data_file), exist_ok=True)
            
            with open(self.data_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(data_dict, ensure_ascii=False) + '\n')
            
            logger.debug(f"üíæ Training example saved to {self.data_file}")
            
        except Exception as e:
            logger.error(f"Failed to save training example: {e}")
    
    def _serialize_judge_evaluations(self, judge_evaluations: Dict[str, List[ComponentEvaluation]]) -> Dict:
        """Convert ComponentEvaluation objects to dict for JSON"""
        
        serialized = {}
        for judge_name, evaluations in judge_evaluations.items():
            serialized[judge_name] = []
            for eval_item in evaluations:
                serialized[judge_name].append({
                    "component": eval_item.component.value,
                    "evaluations": eval_item.evaluations,
                    "winner": eval_item.winner,
                    "extracted_text": eval_item.extracted_text,
                    "winner_score": eval_item.winner_score,
                    "reasoning": eval_item.reasoning
                })
        
        return serialized
    
    def get_training_stats(self) -> Dict[str, Any]:
        """Get statistics about collected training data"""
        
        if not os.path.exists(self.data_file):
            return {"total_examples": 0}
        
        try:
            line_count = 0
            total_cost = 0.0
            
            with open(self.data_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        line_count += 1
                        try:
                            data = json.loads(line)
                            total_cost += data.get("cost_estimate", 0.0)
                        except:
                            continue
            
            return {
                "total_examples": line_count,
                "total_cost_estimate": total_cost,
                "data_file": self.data_file
            }
            
        except Exception as e:
            logger.error(f"Failed to get training stats: {e}")
            return {"total_examples": 0, "error": str(e)}

# ==================== MAIN ENSEMBLE ENGINE ====================

class EnsembleLegalAI:
    """
    Main Ensemble Legal AI System
    Orchestrates all 7 layers of the ensemble pipeline
    """
    
    def __init__(self):
        """Initialize all ensemble components"""
        
        logger.info("üöÄ Initializing Ensemble Legal AI System...")
        
        # Initialize all components
        self.config = config
        self.clients = ModelClients()
        
        # Layer 1: Context Retrieval (existing RAG)
        self.context_retriever = ContextRetriever()
        
        # Layer 2: Multi-Model Generation  
        self.multi_generator = MultiModelGenerator(self.clients)
        
        # Layer 3: 3-Judge System
        self.judge_system = ThreeJudgeSystem(self.clients)
        
        # Layer 4: Consensus Building
        self.consensus_builder = ConsensusBuilder()
        
        # Layer 5: Response Assembly
        self.assembler = ResponseAssembler(self.clients)
        
        # Layer 6: Quality Verification
        self.quality_verifier = QualityVerifier()
        
        # Layer 7: Data Collection
        self.data_collector = DataCollector(config)
        
        logger.info("‚úÖ Ensemble Legal AI System initialized successfully")
        logger.info(f"ü§ñ Available generators: {len(self.clients.get_available_generators())}")
        logger.info(f"‚öñÔ∏è Available judges: {len(self.clients.get_available_judges())}")
    
    async def process_legal_query(self, query: str) -> Dict[str, Any]:
        """
        Main ensemble processing pipeline
        
        Returns complete ensemble result with all intermediate data
        """
        
        start_time = time.time()
        request_id = str(uuid.uuid4())
        
        logger.info(f"üöÄ Processing ensemble query [{request_id}]: {query[:50]}...")
        
        try:
            # Layer 1: Get legal context from existing RAG
            logger.info("üìö Layer 1: Context Retrieval")
            context_data = await self.context_retriever.get_legal_context(query)
            
            if not context_data["chunks"]:
                logger.warning("No legal context found, proceeding with general knowledge")
            
            # Layer 2: Generate responses from 4 models in parallel
            logger.info("üé≠ Layer 2: Multi-Model Generation")
            responses = await self.multi_generator.generate_parallel_responses(context_data)
            
            if not responses:
                raise Exception("No responses generated from any model")
            
            # Layer 3: 3-Judge Component Extraction
            logger.info("‚öñÔ∏è Layer 3: Component Extraction (3 Judges)")
            judge_evaluations = await self.judge_system.evaluate_responses(query, responses)
            
            # Layer 4: Consensus Building
            logger.info("üó≥Ô∏è Layer 4: Consensus Building")
            consensus_results = self.consensus_builder.build_consensus(judge_evaluations)
            
            # Layer 5: Response Assembly
            logger.info("üîß Layer 5: Response Assembly")
            final_response = await self.assembler.assemble_final_response(consensus_results, query)
            
            # Layer 6: Quality Verification
            logger.info("üîç Layer 6: Quality Verification")
            quality_report = self.quality_verifier.verify_quality(
                final_response, responses, consensus_results, context_data["chunks"]
            )
            
            # Calculate processing metrics
            processing_time_ms = int((time.time() - start_time) * 1000)
            cost_estimate = self._estimate_cost(responses, judge_evaluations)
            
            # Layer 7: Data Collection
            logger.info("üíæ Layer 7: Data Collection")
            training_data = self.data_collector.collect_training_example(
                query=query,
                context_data=context_data,
                responses=responses,
                judge_evaluations=judge_evaluations,
                consensus_results=consensus_results,
                final_response=final_response,
                quality_report=quality_report,
                processing_time_ms=processing_time_ms,
                cost_estimate=cost_estimate
            )
            
            # Compile final result
            result = {
                "request_id": request_id,
                "query": query,
                "final_response": final_response,
                "processing_time_ms": processing_time_ms,
                "cost_estimate": cost_estimate,
                "quality_report": {
                    "passed": quality_report.passed,
                    "issues": quality_report.issues,
                    "metrics": quality_report.metrics
                },
                "ensemble_data": {
                    "context_chunks": len(context_data["chunks"]),
                    "generator_responses": len(responses),
                    "judge_evaluations": len(judge_evaluations),
                    "consensus_components": len([r for r in consensus_results if r.final_text.strip()]),
                    "component_winners": {r.component.value: r.winning_model for r in consensus_results}
                },
                "intermediate_data": {
                    "responses": responses,
                    "consensus_results": consensus_results,
                    "judge_evaluations": judge_evaluations
                } if self.config.save_training_data else None
            }
            
            logger.info(f"‚úÖ Ensemble processing completed [{request_id}]: {processing_time_ms}ms, ${cost_estimate:.3f}")
            
            return result
            
        except Exception as e:
            processing_time_ms = int((time.time() - start_time) * 1000)
            logger.error(f"‚ùå Ensemble processing failed [{request_id}]: {e}")
            
            return {
                "request_id": request_id,
                "query": query,
                "final_response": f"ÿπÿ∞ÿ±ÿßŸãÿå ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿßÿ≥ÿ™ŸÅÿ≥ÿßÿ±ŸÉ: {str(e)}",
                "processing_time_ms": processing_time_ms,
                "cost_estimate": 0.0,
                "quality_report": {"passed": False, "issues": [str(e)], "metrics": {}},
                "ensemble_data": {},
                "error": str(e)
            }
    
    def _estimate_cost(self, responses: Dict[str, str], judge_evaluations: Dict[str, List[ComponentEvaluation]]) -> float:
        """Estimate API costs for this query"""
        
        # Generation costs (rough estimates based on token usage)
        generation_cost = 0.0
        
        cost_per_model = {
            "chatgpt": 0.06,    # GPT-4o
            "deepseek": 0.01,   # DeepSeek  
            "grok": 0.05,       # Grok-2
            "gemini": 0.04      # Gemini 1.5 Pro
        }
        
        for model_name in responses.keys():
            generation_cost += cost_per_model.get(model_name, 0.03)  # Default
        
        # Judging costs
        judging_cost = len(judge_evaluations) * 0.04  # ~$0.04 per judge
        
        # Assembly cost
        assembly_cost = 0.02
        
        total_cost = generation_cost + judging_cost + assembly_cost
        
        return round(total_cost, 3)
    
    async def process_streaming(self, query: str) -> AsyncIterator[Dict[str, Any]]:
        """
        Streaming version that yields progress updates
        """
        
        request_id = str(uuid.uuid4())
        start_time = time.time()
        
        try:
            yield {"type": "start", "request_id": request_id, "query": query}
            
            # Layer 1
            yield {"type": "progress", "layer": 1, "status": "Context Retrieval"}
            context_data = await self.context_retriever.get_legal_context(query)
            yield {"type": "progress", "layer": 1, "status": "Complete", "chunks": len(context_data["chunks"])}
            
            # Layer 2  
            yield {"type": "progress", "layer": 2, "status": "Multi-Model Generation"}
            responses = await self.multi_generator.generate_parallel_responses(context_data)
            yield {"type": "progress", "layer": 2, "status": "Complete", "models": len(responses)}
            
            # Layer 3
            yield {"type": "progress", "layer": 3, "status": "Judge Evaluation"}
            judge_evaluations = await self.judge_system.evaluate_responses(query, responses)
            yield {"type": "progress", "layer": 3, "status": "Complete", "judges": len(judge_evaluations)}
            
            # Layer 4
            yield {"type": "progress", "layer": 4, "status": "Consensus Building"}
            consensus_results = self.consensus_builder.build_consensus(judge_evaluations)
            yield {"type": "progress", "layer": 4, "status": "Complete", "components": len(consensus_results)}
            
            # Layer 5
            yield {"type": "progress", "layer": 5, "status": "Response Assembly"}
            final_response = await self.assembler.assemble_final_response(consensus_results, query)
            yield {"type": "progress", "layer": 5, "status": "Complete"}
            
            # Layer 6
            yield {"type": "progress", "layer": 6, "status": "Quality Verification"}
            quality_report = self.quality_verifier.verify_quality(
                final_response, responses, consensus_results, context_data["chunks"]
            )
            yield {"type": "progress", "layer": 6, "status": "Complete", "passed": quality_report.passed}
            
            # Layer 7
            yield {"type": "progress", "layer": 7, "status": "Data Collection"}
            processing_time_ms = int((time.time() - start_time) * 1000)
            cost_estimate = self._estimate_cost(responses, judge_evaluations)
            
            training_data = self.data_collector.collect_training_example(
                query=query, context_data=context_data, responses=responses,
                judge_evaluations=judge_evaluations, consensus_results=consensus_results,
                final_response=final_response, quality_report=quality_report,
                processing_time_ms=processing_time_ms, cost_estimate=cost_estimate
            )
            yield {"type": "progress", "layer": 7, "status": "Complete"}
            
            # Final result
            yield {
                "type": "complete",
                "request_id": request_id,
                "final_response": final_response,
                "processing_time_ms": processing_time_ms,
                "cost_estimate": cost_estimate,
                "quality_passed": quality_report.passed,
                "ensemble_stats": {
                    "context_chunks": len(context_data["chunks"]),
                    "models_used": len(responses),
                    "judges_used": len(judge_evaluations),
                    "components_extracted": len([r for r in consensus_results if r.final_text.strip()])
                }
            }
            
        except Exception as e:
            yield {
                "type": "error",
                "request_id": request_id,
                "error": str(e),
                "processing_time_ms": int((time.time() - start_time) * 1000)
            }

# ==================== GLOBAL INSTANCE ====================

# Initialize ensemble system
ensemble_engine = EnsembleLegalAI()

# Export functions for integration
async def process_ensemble_query(query: str) -> Dict[str, Any]:
    """Main ensemble processing function"""
    return await ensemble_engine.process_legal_query(query)

async def process_ensemble_streaming(query: str) -> AsyncIterator[Dict[str, Any]]:
    """Streaming ensemble processing"""
    async for update in ensemble_engine.process_streaming(query):
        yield update

# Utility functions
def get_ensemble_stats() -> Dict[str, Any]:
    """Get ensemble system statistics"""
    training_stats = ensemble_engine.data_collector.get_training_stats()
    
    return {
        "system_status": "active",
        "available_generators": len(ensemble_engine.clients.get_available_generators()),
        "available_judges": len(ensemble_engine.clients.get_available_judges()),
        "training_data": training_stats,
        "config": {
            "temperature": ensemble_engine.config.temperature,
            "max_tokens": ensemble_engine.config.max_tokens,
            "context_chunks": ensemble_engine.config.context_chunks
        }
    }

def get_ensemble_engine():
    """Get ensemble engine instance"""
    return ensemble_engine

logger.info("üöÄ Ensemble Legal AI System loaded and ready!")

# ==================== TESTING FRAMEWORK ====================

async def test_ensemble_system():
    """Test ensemble system with sample queries"""
    
    test_queries = [
        "ŸÖÿß ŸáŸä ŸÖŸÉÿßŸÅÿ£ÿ© ŸÜŸáÿßŸäÿ© ÿßŸÑÿÆÿØŸÖÿ© ŸÑŸÑŸÖŸàÿ∏ŸÅ ÿßŸÑÿ∞Ÿä ÿπŸÖŸÑ 5 ÿ≥ŸÜŸàÿßÿ™ ÿ®ÿ±ÿßÿ™ÿ® 6000 ÿ±ŸäÿßŸÑÿü",
        "ŸÖŸàÿ∏ŸÅ ÿ±ŸÅÿπ ÿπŸÑŸäŸá ÿØÿπŸàŸâ ŸÖŸÜ ÿ¥ÿ±ŸÉÿ™Ÿá ÿ®ÿ≤ÿπŸÖ ÿ•ŸÅÿ¥ÿßÿ° ÿ£ÿ≥ÿ±ÿßÿ±ÿå ŸÉŸäŸÅ Ÿäÿ±ÿØ ÿπŸÑŸâ Ÿáÿ∞Ÿá ÿßŸÑÿØÿπŸàŸâÿü", 
        "ÿ£ÿ±ŸäÿØ ŸÅÿµŸÑ ŸÖŸàÿ∏ŸÅ ŸÑÿ≥Ÿàÿ° ÿßŸÑÿ≥ŸÑŸàŸÉÿå ŸÖÿß ŸáŸä ÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©ÿü"
    ]
    
    logger.info("üß™ Starting ensemble system tests...")
    
    for i, query in enumerate(test_queries, 1):
        logger.info(f"\nüß™ Test {i}: {query}")
        
        try:
            result = await ensemble_engine.process_legal_query(query)
            
            logger.info(f"‚úÖ Test {i} completed:")
            logger.info(f"  Processing time: {result['processing_time_ms']}ms")
            logger.info(f"  Cost estimate: ${result['cost_estimate']:.3f}")
            logger.info(f"  Quality passed: {result['quality_report']['passed']}")
            logger.info(f"  Response length: {len(result['final_response'])} chars")
            
            if result['ensemble_data']:
                logger.info(f"  Models used: {result['ensemble_data']['generator_responses']}")
                logger.info(f"  Components: {result['ensemble_data']['consensus_components']}")
            
        except Exception as e:
            logger.error(f"‚ùå Test {i} failed: {e}")
    
    logger.info("üß™ Ensemble testing completed")

if __name__ == "__main__":
    asyncio.run(test_ensemble_system())