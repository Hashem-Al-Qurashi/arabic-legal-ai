import asyncio
import os
from dotenv import load_dotenv  # Add this import
from openai import AsyncOpenAI
from app.storage.sqlite_store import SqliteVectorStore
from app.services.document_service import DocumentService

async def test_current_document_service():
    """Test current DocumentService with small and large documents"""
    
    print("ğŸ§ª Testing Current DocumentService")
    print("=" * 50)
    
    # Load environment variables from .env file
    load_dotenv()
    print("ğŸ“ Loaded .env file")
    
    try:
        # Initialize your real DocumentService (same as crawler uses)
        print("ğŸ”§ Initializing DocumentService...")
        storage = SqliteVectorStore("data/vectors.db")
        await storage.initialize()
        
        # Get OpenAI key from environment
        openai_key = os.getenv("OPENAI_API_KEY")
        if not openai_key:
            print("âŒ OPENAI_API_KEY not found in environment")
            print(f"ğŸ” Available env vars: {list(os.environ.keys())[:5]}...")
            return
        
        print(f"ğŸ”‘ OpenAI key found: {openai_key[:10]}...")
        
        ai_client = AsyncOpenAI(api_key=openai_key)
        doc_service = DocumentService(storage, ai_client)
        
        print("âœ… DocumentService initialized successfully")
        print()
        
        # Test 1: Small document (should work perfectly)
        print("ğŸ§ª TEST 1: Small Document (Should Work)")
        print("-" * 30)
        
        small_doc = [{
            "title": "Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ«ÙŠÙ‚Ø© ØµØºÙŠØ±Ø©",
            "content": "Ù‡Ø°Ø§ Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ· Ù„Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„ØµØºÙŠØ±Ø©. ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Øµ Ù‚ØµÙŠØ± Ù„Ø§ ÙŠØªØ¬Ø§ÙˆØ² Ø­Ø¯ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡.",
            "metadata": {"test_type": "small", "source": "test"}
        }]
        
        print(f"ğŸ“„ Document: {small_doc[0]['title']}")
        print(f"ğŸ“ Content length: {len(small_doc[0]['content'])} chars")
        print(f"ğŸ”¢ Estimated tokens: {len(small_doc[0]['content']) // 2}")
        
        result = await doc_service.add_documents_batch(small_doc)
        print(f"âœ… Result: {result}")
        print()
        
        # Test 2: Large document (should fail with token error)  
        print("ğŸ§ª TEST 2: Large Document (Should Fail)")
        print("-" * 30)
        
        # Create a realistically large document (simulate Saudi legal doc)
        large_content = """
        Ø§Ù„Ø¨Ø§Ø¨ Ø§Ù„Ø£ÙˆÙ„: Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø¹Ø§Ù…Ø©
        """ + """
        Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: ØªØ¹Ø±ÙŠÙØ§Øª ÙˆØ£Ø­ÙƒØ§Ù… Ø¹Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ ØªØ´Ù…Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ© ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠØ©.
        Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ù†Ø·Ø§Ù‚ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ§Ù„Ø§Ø®ØªØµØ§Øµ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠ ÙÙŠ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©.
        """ * 2000  # Repeat to make it large
        
        large_doc = [{
            "title": "Ù†Ø¸Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± ÙƒØ¨ÙŠØ±",
            "content": large_content,
            "metadata": {"test_type": "large", "source": "test"}
        }]
        
        print(f"ğŸ“„ Document: {large_doc[0]['title']}")
        print(f"ğŸ“ Content length: {len(large_doc[0]['content']):,} chars")
        print(f"ğŸ”¢ Estimated tokens: {len(large_doc[0]['content']) // 2:,}")
        
        result = await doc_service.add_documents_batch(large_doc)
        print(f"ğŸ“Š Result: {result}")
        print()
        
        # Test 3: Check storage status
        print("ğŸ§ª TEST 3: Storage Health Check")
        print("-" * 30)
        
        health = await doc_service.get_storage_health()
        print(f"ğŸ¥ Storage Health: {health}")
        
        docs_list = await doc_service.list_documents()
        print(f"ğŸ“‹ Documents List: {docs_list}")
        
    except Exception as e:
        print(f"âŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_current_document_service())
