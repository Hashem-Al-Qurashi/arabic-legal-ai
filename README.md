# ğŸ›ï¸ Legal AI Vanilla Ensemble System

A sophisticated ensemble system that combines 4 AI models with 3 judges to provide superior legal responses based on Saudi law, using pure vanilla generation without RAG or external context.

## ğŸ¯ System Architecture

### Step 1: Multi-Model Vanilla Generation
- **4 Models**: GPT-4o, DeepSeek, Grok-2, Gemini-1.5-Flash
- Each model responds with pure vanilla legal knowledge
- No external context or RAG - just like using ChatGPT directly

### Step 2: Component Extraction by 3 Judges
- **3 Judges**: Claude-3.5-Sonnet, GPT-4o, Gemini-1.5-Flash
- Extract the BEST version of 7 components from all responses:
  - Direct Answer
  - Legal Foundation  
  - Article Citations
  - Numerical Examples
  - Step-by-Step Procedures
  - Edge Cases & Exceptions
  - Practical Advice

### Step 3: Consensus Voting
- Majority voting (2/3 judges) for each component
- Special rule: Take union of all cited articles
- Highest individual score used when no consensus

### Step 4: Response Assembly
- Combine winning components into coherent response
- AI adds smooth transitions between components
- Keep extracted text exactly as-is

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Configure API Keys
Edit `.env` file with your API keys:
```env
OPENAI_API_KEY=your_openai_key
DEEPSEEK_API_KEY=your_deepseek_key  
GEMINI_API_KEY=your_gemini_key
```

### 3. Run the Server
```bash
python app.py
```

### 4. Open the UI
Navigate to: http://localhost:8000

## ğŸ“Š System Features

### Comprehensive Logging
- **Real-time logs** for every step of processing
- **File logging** with timestamps for audit trails
- **Console output** for immediate feedback
- **Error tracking** with detailed error messages

### Monitoring Dashboard
- **Health checks** - API status and system readiness
- **Live logs** - Recent system activity in real-time
- **Statistics** - System performance and usage metrics
- **Cost tracking** - Per-request cost estimation

### Performance Metrics
- **Processing Time**: Typically 30-60 seconds
- **Cost per Query**: ~$0.29 (4 models + 3 judges + assembly)
- **Success Rate**: Tracked for each model and judge
- **Response Quality**: Consensus scoring system

## ğŸ§ª Test Questions

Try these sample questions:
- `Ù…Ø§ Ù‡ÙŠ Ù…Ø¯Ø© Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ø³Ù†ÙˆÙŠØ©ØŸ` (What is the annual leave duration?)
- `ÙƒÙŠÙ Ø£Ø­Ø³Ø¨ Ù…ÙƒØ§ÙØ£Ø© Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø®Ø¯Ù…Ø©ØŸ` (How to calculate end-of-service gratuity?)
- `Ù…Ø§ Ø¹Ù‚ÙˆØ¨Ø§Øª Ø§Ù„ØªØ£Ø®ÙŠØ± Ø¹Ù† Ø§Ù„Ø¹Ù…Ù„ØŸ` (What are the penalties for work delays?)

## ğŸ“¡ API Endpoints

### POST /ask
**Request:**
```json
{
  "question": "Ù…Ø§ Ù‡ÙŠ Ù…Ø¯Ø© Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ø³Ù†ÙˆÙŠØ©ØŸ"
}
```

**Response:**
```json
{
  "final_response": "assembled response from best components",
  "processing_time_ms": 45000,
  "cost_estimate": 0.25,
  "models_used": 4,
  "judges_used": 3,
  "components_extracted": 7,
  "consensus_score": 8.5
}
```

### GET /health
Check system health and API key status

### GET /logs  
Get recent system logs for debugging

### GET /stats
Get system statistics and performance metrics

## ğŸ”§ System Configuration

### Model Configuration
- **Generation Models**: 4 models for diverse perspectives
- **Judge Models**: 3 models for robust evaluation
- **Timeout**: 30 seconds per API call
- **Retries**: 3 attempts for failed calls

### Cost Optimization
- **Parallel Processing**: All API calls run simultaneously
- **Smart Fallbacks**: Graceful degradation when models fail
- **Cost Tracking**: Real-time cost estimation per request

### Error Handling
- **Robust Retries**: Automatic retry on temporary failures
- **Graceful Degradation**: Continue with available models
- **Detailed Logging**: Full error context for debugging

## ğŸ“ Project Structure

```
legal_ai_ensemble/
â”œâ”€â”€ vanilla_ensemble.py    # Core ensemble engine
â”œâ”€â”€ app.py                # FastAPI server
â”œâ”€â”€ test.html            # Web UI for testing
â”œâ”€â”€ .env                 # Environment variables
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ ensemble_*.log      # System logs
â””â”€â”€ app_*.log          # Server logs
```

## ğŸ” Logging Details

The system provides comprehensive logging at every step:

### Generation Phase
```
ğŸ¤– Calling OpenAI model: gpt-4o
âœ… gpt-4o responded in 3.45s, cost: $0.0423
```

### Judging Phase  
```
âš–ï¸ Judge evaluation with gpt-4o
âœ… gpt-4o: Score 8.5, $0.0234, 2.1s
```

### Consensus Voting
```
ğŸ—³ï¸ Computing consensus from 3 judges
ğŸ“Š Voting for component: direct_answer
âœ… direct_answer: Winner selected (score: 8.5)
```

### Final Assembly
```
ğŸ”§ Starting Step 4: Response Assembly
âœ… Step 4 Complete: Response assembled in 1.23s, cost: $0.02
```

## ğŸ’¡ Key Benefits

1. **Superior Quality**: Ensemble approach beats any single model
2. **Robust Processing**: Continues even if some models fail
3. **Full Transparency**: Complete logging of every step
4. **Cost Effective**: ~$0.29 per comprehensive legal analysis
5. **Saudi Law Focus**: Specialized for Saudi legal framework
6. **No Dependencies**: Pure vanilla generation without external data

## ğŸ› ï¸ Troubleshooting

### Check API Keys
```bash
curl http://localhost:8000/health
```

### View Recent Logs
```bash
curl http://localhost:8000/logs
```

### Monitor Performance
```bash
curl http://localhost:8000/stats
```

### Common Issues
- **Slow responses**: Normal for ensemble processing (30-60s)
- **Model failures**: Check logs for specific API errors
- **Cost concerns**: Monitor `/stats` endpoint for usage tracking

---

**Note**: This system focuses on vanilla AI generation without RAG or external context, making it a pure test of ensemble methodology for legal AI applications.